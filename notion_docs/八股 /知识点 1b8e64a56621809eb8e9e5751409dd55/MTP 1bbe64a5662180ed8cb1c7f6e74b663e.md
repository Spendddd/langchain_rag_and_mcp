# MTP

State: 已完成

https://zhuanlan.zhihu.com/p/18056041194：**deepseek技术解读(2)-MTP（Multi-Token Prediction）的前世今生**

# **为什么要做MTP**

当前主流的大模型(LLMs)都是decoder-base的模型结构，也就是无论在模型训练还是在推理阶段，对于一个序列的生成过程，都是token-by-token的。**每次在生成一个token的时候，都要频繁跟访存交互，加载KV-Cache，再通过多层网络做完整的前向计算。**对于这样的**访存密集型**的任务，通常会**因为访存效率形成训练或推理的瓶颈**。

MTP核心思想：**通过解码阶段的优化，将1-token的生成，转变成multi-token的生成，从而提升训练和推理的性能。具体来说，在训练阶段，一次生成多个后续token，可以一次学习多个位置的label，进而有效提升样本的利用效率，提升训练速度；在推理阶段通过一次生成多个token，实现成倍的推理加速来提升推理性能。**

# **MTP 方法的一些探索**

### **1. Blockwise Parallel Decoding**

![image.png](MTP%201bbe64a5662180ed8cb1c7f6e74b663e/image.png)

### **2. Meta's MTP**

![image.png](MTP%201bbe64a5662180ed8cb1c7f6e74b663e/image%201.png)

我们**仔细对比下图2和图6，网络结构基本一致，有两个微小的不同：**

- 图2是2层FFN， 图6是一个Transformer
- 图6 除了可按图2方法一样可做并行推理，本文也重点考虑模型加速训练的优化，在模型训练时，多个头都会并行计算loss时，提升样本利用效率和加速模型收敛。

至此，我们讲完了两篇paper的主要工作，方法比较直观，接下来，我们再来看看DeepSeek 的 MTP

# **DeepSeek MTP**

首先我们还是从网络结构出发，看看DeepSeek的MTP的设计。如下图7所示，乍看上去也是多头，但结构略复杂。且论文中也强调，在实现上保留了序列推理的连接关系**（causal chain），**如图中，从一个Module链接到后继Module的箭头。

![图7、Deepseek MTP实现](MTP%201bbe64a5662180ed8cb1c7f6e74b663e/image%202.png)

图7、Deepseek MTP实现

## **MTP模块细节实现**

如上图7所示，用D个顺序的模块，预测D个tokens。每个MTP模块的具体结构(如图7红框内）：

![image.png](MTP%201bbe64a5662180ed8cb1c7f6e74b663e/image%203.png)

![image.png](MTP%201bbe64a5662180ed8cb1c7f6e74b663e/image%204.png)

![image.png](MTP%201bbe64a5662180ed8cb1c7f6e74b663e/image%205.png)

## **MTP模型训练**

通过**CrossEntropyLoss**计算每个MTP Module Head的损失，如公式(24)所示

![image.png](MTP%201bbe64a5662180ed8cb1c7f6e74b663e/image%206.png)

![image.png](MTP%201bbe64a5662180ed8cb1c7f6e74b663e/image%207.png)

![image.png](MTP%201bbe64a5662180ed8cb1c7f6e74b663e/image%208.png)

![image.png](MTP%201bbe64a5662180ed8cb1c7f6e74b663e/image%209.png)

建议对比图2、图6、图9对比下几种方法实现上的差异。**DeepSeek的实现相对于之前的方法增加了causal chain的连接关系，同时在embedding层增加了残差链接**。

![image.png](MTP%201bbe64a5662180ed8cb1c7f6e74b663e/image%2010.png)

## **MTP模型推理**

DeepSeek V3推理可以有两种方法：

**方法1**：直接把MTP Model头全部删掉，模型变成了一个Predict Next Token的 Main Model。然后部署模型做推理，这个就跟正常LLM模型推理一样。没有什么加速效果

**方法2：保留MTP Model 做self-speculative decoding**，这样充分使用多Head预测能力，提升推理加速性能。类似2.1中介绍的三阶段

![image.png](MTP%201bbe64a5662180ed8cb1c7f6e74b663e/image%2011.png)

这里要再注意一个细节，**阶段1：predict(预测)的的流程图，跟图9长得一样吗？**当然不一样。Teacher forcing 只能用于训练阶段。推理阶段要用上一个状态的预估值作为下一个状态的输入（free-running模式），我也画了下推理阶段的流程图，如图10所示 ：

![image.png](MTP%201bbe64a5662180ed8cb1c7f6e74b663e/image%2012.png)
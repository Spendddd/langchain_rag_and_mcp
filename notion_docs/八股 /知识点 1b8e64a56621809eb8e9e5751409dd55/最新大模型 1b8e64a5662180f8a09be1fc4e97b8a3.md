# 最新大模型

State: 学习中

## minimax

https://zhuanlan.zhihu.com/p/20429127929：**模型方法-MiniMax-01突破长上下文极限**

模型架构图

![image.png](%E6%9C%80%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%201b8e64a5662180f8a09be1fc4e97b8a3/image.png)

- **G是什么**
    
    图中的G代表的是一种门控机制（Gating Mechanism），通常用于控制信息的流动和加权。在神经网络中，门控机制可以帮助模型选择性地关注某些输入或特征，从而提高模型的性能和效率。
    
    具体来说，图中的G与SiLU（Sigmoid Linear Unit）激活函数结合使用。SiLU是一种平滑的非线性激活函数，它结合了Sigmoid函数和线性函数的优点，能够更好地捕捉非线性特征。通过将SiLU与门控机制结合，可以动态调整输入特征的权重，使得模型能够更灵活地适应不同的输入数据。
    
    在Lightning Attention模块中，G的计算过程涉及多个步骤，主要是通过一系列的数学运算和激活函数来实现的。具体来说，G的计算过程如下：
    
    1. **输入线性变换**：
    首先，输入的查询（Q）、键（K）和值（V）会经过一个线性变换层。这一步的目的是将输入特征映射到一个新的特征空间，以便后续的计算。
    2. **SiLU激活函数**：
    经过线性变换后的Q、K、V会分别通过SiLU（Sigmoid Linear Unit）激活函数。SiLU函数结合了线性函数和Sigmoid函数的优点，能够更好地捕捉非线性特征。
    3. **生成门控权重G**：
    经过SiLU激活函数处理后的Q、K、V会进一步通过一个Sigmoid函数生成门控权重G。具体来说，G的计算公式为：
    这里，G是一个介于0和1之间的权重值，用于控制信息的流动。
        
        ![image.png](%E6%9C%80%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%201b8e64a5662180f8a09be1fc4e97b8a3/image%201.png)
        
    4. **加权计算**：
    最后，门控权重G会用于加权Q、K、V之间的交互。具体来说，加权计算公式为：
        
        ![image.png](%E6%9C%80%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%201b8e64a5662180f8a09be1fc4e97b8a3/image%202.png)
        
        通过这种方式，G能够动态调整输入特征的权重，使得模型能够更灵活地适应不同的输入数据。
        
    
    总的来说，G的计算过程通过一系列的线性变换和非线性激活函数，实现了信息的动态加权和控制。这种门控机制能够帮助模型更有效地处理和理解输入数据，提高模型的性能和泛化能力。
    

## deepseek

https://zhuanlan.zhihu.com/p/23048347789：**DeepSeek关键技术详解**

**DeepSeek 关键技术一览**

| **技术创新** | **模型版本** | **发布时间** |
| --- | --- | --- |
| Deepseek MoE 架构 | [DeepSeek-MOE](https://zhida.zhihu.com/search?content_id=253631915&content_type=Article&match_order=1&q=DeepSeek-MOE&zhida_source=entity)[1] | 2024年1月 |
| Group Relative Policy Optimization（GRPO，群体相对策略优化） | DeepSeek-Math[2] | 2024年4月 |
| Multi-Head Latent Attention（MLA，多头隐式注意力） | DeepSeek-V2[3] | 2024年6月 |
| Multi-Token Prediction（MTP，多令牌预测） | DeepSeek-V3[4] | 2024年12月 |
| AI Infra相关（以训练加速为主，如FP8混合精度训练、[DualPipe](https://zhida.zhihu.com/search?content_id=253631915&content_type=Article&match_order=1&q=DualPipe&zhida_source=entity)等） | DeepSeek-V3[4] | 2024年12月 |
| 通过强化学习显著提升模型推理能力，R1-Zero在AIME 2024等推理基准测试中达到OpenAI-o1-0912的水平 | DeepSeek-R1-Zero[5] | 2025年1月 |
| 使用**冷启动(SFT)—>强化学习(reasoning oriented RL, GRPO, 注重长推理场景)—>SFT(混合No reasoning、non-reasoning(包含CoT prompting)数据)—>强化学习(GRPO, 全场景, 包括reasoning、偏好奖励、多样化prompt)**四阶段训练，R1模型达到OpenAI-o1-1217的水平 | DeepSeek-R1[5] | 2025年1月 |
| 将R1推理能力蒸馏到小的稠密模型 | DeepSeek-R1-Distill[5] | 2025年1月 |

### MoE技术（-MoE）

见[MoE与负载均衡](MoE%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%201b8e64a566218027ad4bf217e06d1e1b.md)

### GRPO（-Math）

见[GRPO](RLHF%EF%BC%9APPO%E3%80%81DPO%E3%80%81GRPO%201bbe64a5662180f1bab3e4aa07acfe6b.md)

### MLA（V2）

见[MLA](MLA%E3%80%81MHA%E3%80%81GQA%E3%80%81MQA%201bae64a5662180b4b8f7e6419d7f225a.md)

### MTP：**Multi-Token Prediction（V3）**

见[MTP](MTP%201bbe64a5662180ed8cb1c7f6e74b663e.md)

### FP8（V3）

DeepSeek-V3 模型使用了 FP8 训练，为了增强训练稳定性以及维持训练效果不至于下降太多，作者提出了一种精细的量化策略，另外为了进一步减少MoE训练中的内存和通信开销，作者**在FP8中缓存和分发激活值，同时以BF16格式存储低精度优化器状态**。在实验中，FP8训练模型与BF16基线相比，相对损失误差始终低于0.25%，在训练随机性范围内是可以接受的。

基于此，DeepSeek-V3 文中提出了一种 FP8 训练的混合精度框架。在这个框架中，大多数计算密集型操作在 FP8 中进行，而一些关键操作则保持其原始数据格式，以平衡训练效率和数值稳定性。**为了加速模型训练，主要的核心计算内核（如General Matrix Multiplication，GEMM操作）在 FP8 精度下实现，这些操作接受 FP8 张量作为输入，并生成 BF16 或 FP32 格式的输出**。**所有与线性操作相关的三个 GEMM（前向传播、激活反向传播和权重反向传播）都在 FP8 中执行**，这种设计理论上将计算速度提高了一倍。此外，FP8 权重反向传播 GEMM **允许激活值以 FP8 格式存储，以便在反向传播中使用**，从而显著减少了内存消耗。

训练框架**在以下组件中保持了原始精度（如BF16或FP32）：Embedding 模块、输出头、MoE门控模块、归一化算子和注意力算子等**。这些高精度的保留确保了DeepSeek-V3的稳定训练动态。为了进一步保证数值稳定性，作者将**模型的主权重、权重梯度和优化器状态**均存储在更高的精度中。该混合精度框架示意图可见图5。

![image.png](%E6%9C%80%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%201b8e64a5662180f8a09be1fc4e97b8a3/image%203.png)

### 冷启动（R1）

https://cloud.tencent.com/developer/article/2494654

https://blog.csdn.net/dongtuoc/article/details/145455551?spm=1001.2014.3001.5502

DeepSeek-R1-Zero：完全依靠 强化学习（RL） 进行训练，没有经过 监督微调（SFT）。这种方式让模型自己探索推理方法，但也带来了一些问题，比如容易生成 重复内容、可读性差、语言混杂。
**DeepSeek-R1：在强化学习之前，先加入了冷启动数据进行微调（SFT），让模型从一开始就具备基础的语言和推理能力**，之后再用强化学习优化推理能力。这样可以减少 R1-Zero 版本的缺点，提高回答质量和可读性。

### 拒绝采样（R1）

https://blog.csdn.net/qq_35812205/article/details/145463421

RFT的核心思想是利用已有的监督模型来生成新的数据样本，如果将其用于数学推理任务，那么可以通过选择正确的推理路径来增强模型的训练数据集。

1. **生成候选推理路径**：使用一个已经通过监督微调（SFT）训练好的模型来生成针对训练集中每个问题的多个候选推理路径。这些路径包括一系列计算步骤，旨在解决问题。
2. **筛选正确路径**：从生成的候选路径中筛选出那些能够正确推导出问题答案的推理路径。【这一步表现“拒绝”】
3. **去重和多样化：**进一步从筛选出的正确路径中选择具有不同计算过程或表达方式的路径，以增加数据集的多样性。这有助于模型学习不同的解决问题的方法。
4. **微调：使用这些经过筛选和去重的推理路径作为新的训练数据，对原始的监督模型进行进一步的微调。【用自己生成的数据中质量高的用于后续微调】**
5. 提高泛化能力：通过引入多样化的推理路径，RFT旨在提高模型在未见过的问题上的泛化能力。

将RFT用于数学推理任务，可以利用模型自身生成的数据来增强其推理能力，同时避免了昂贵的人工标注成本。这种方法特别适用于那些难以通过增加监督数据量来提升性能的场景，因为它允许模型从未充分利用的训练数据中学习新的推理策略。

和SFT相比较，RFT具有以下几点优势：

1. 数据增强的有效性：RFT通过拒绝采样的方式，使用监督模型生成并收集正确的推理路径作为额外的微调数据集。这种方法可以在不增加人工标注工作量的情况下，增加数据样本，从而提高模型性能。
2. 推理路径的多样性：RFT特别强调通过**增加不同的推理路径来提高LLMs的数学推理能力**。这意味着RFT能够提供多种解决问题的方法，有助于模型在面对新问题时有更好的泛化能力。
3. 对性能较差模型的提升效果：论文中提到，RFT对于性能较差的LLMs提升更为明显。这表明RFT可能是一种更为有效的改进手段，特别是对于那些需要显著提高推理能力的模型。
4. 组合多个模型的优势：RFT可以通过**组合来自多个模型的拒绝样本**来进一步提升性能。这种方法使得LLaMA-7B在GSM8K数据集上的准确率从SFT的35.9%显著提高到49.3%。
5. 计算资源的经济性：尽管RFT在生成样本时可能需要较多的计算资源，但在训练阶段相比从头开始预训练一个LLM来说，它是一种更为经济的方法。这使得RFT成为一种可行的、成本效益更高的改进模型性能的手段。
6. 减少过拟合：RFT通过引入多样化的推理路径，有助于减少模型在训练数据上的过拟合，特别是在大型模型中。

### 知识蒸馏（R1-distill）

https://cloud.tencent.com/developer/article/2459083

### **知识蒸馏的基本原理**

知识蒸馏的核心思想是在训练学生模型时，不仅仅依赖于传统的硬标签（Hard Labels），而是使用教师模型的软标签（Soft Labels）。这些软标签包含了教师模型对输入的概率分布信息，从而帮助学生模型更好地学习知识。

教师模型的输出通常是一个分类任务中的概率分布。例如，对于一个有3个类别的分类问题，教师模型的输出可能是 `[0.7, 0.2, 0.1]`，这代表教师模型对输入属于类别1、类别2和类别3的概率。这种分布通常比硬标签（例如 `[1, 0, 0]`）提供了更多的信息，尤其是对于模棱两可的样本。

通过引入**温度参数（Temperature Parameter，T）**，可以控制教师模型输出的软标签分布。温度越高，概率分布越平滑，从而提供更多的关于各个类别的相对信息。温度较低时，软标签分布更接近硬标签。

知识蒸馏中温度的作用：对于p和q进行平滑（softmax）

![image.png](%E6%9C%80%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%201b8e64a5662180f8a09be1fc4e97b8a3/image%204.png)

### **知识蒸馏的数学公式**

在知识蒸馏中，损失函数通常由两部分组成：

1. **标准交叉熵损失（Cross-Entropy Loss）**：学生模型直接拟合训练数据的硬标签，公式如下：
    
    ![image.png](%E6%9C%80%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%201b8e64a5662180f8a09be1fc4e97b8a3/image%205.png)
    
    其中，yi是第 i 个样本的真实标签，Pstudent(xi)是学生模型对该样本的预测概率。
    
2. **蒸馏损失（Distillation Loss）**：学生模型学习教师模型的软标签分布，公式如下：
    
    ![image.png](%E6%9C%80%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%201b8e64a5662180f8a09be1fc4e97b8a3/image%206.png)
    
    其中，T是温度参数，qteacher(xi,T)是教师模型在温度 T 下的输出概率分布，Pstudent(xi,T)是学生模型在相同温度下的预测。
    
3. 最后，总损失函数 L 是标准交叉熵损失和蒸馏损失的加权和：
    
    ![image.png](%E6%9C%80%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%201b8e64a5662180f8a09be1fc4e97b8a3/image%207.png)
    
    其中，α是用于调节两者权重的超参数。
    

### DeepSeek R1-distill

![将R1推理能力蒸馏到其他模型，此处R1严格来说是R1的临时模型（第二次sft之前），instruct model是原有的小模型](%E6%9C%80%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%201b8e64a5662180f8a09be1fc4e97b8a3/image%208.png)

将R1推理能力蒸馏到其他模型，此处R1严格来说是R1的临时模型（第二次sft之前），instruct model是原有的小模型

用 600k + 200k = 800k 条数据对 6 个不同参数量的开源模型进行了直接有监督微调。这种方式也就是直接的数据蒸馏。过这种方法就能够显著增强小参数规模模型的推理能力。同时也反映了 R1 模型的价值，它能够用于激发绝大多数模型的推理能力。

![image.png](%E6%9C%80%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%201b8e64a5662180f8a09be1fc4e97b8a3/image%209.png)

表10展示了基于1.5B-70B规模的开源模型，使用DeepSeek-R1 数据蒸馏后的推理能力评测，指标都具有较强的竞争力。这实在是一件很夸张的事，这相当于告诉了我们**一个简单直接的模型效果优化手段，就是从 R1 模型构造数据，然后SFT！**

### CoT与Long CoT（R1（zero））

CoT指的是一种推理过程，其中模型在生成最终答案之前，先逐步推导出一系列的中间步骤或子目标。这些中间步骤构成了一个“思维链”，最终引导模型得到正确的结果。它模仿人类的推理过程，即人们往往在解决问题时不是直接得出答案，而是通过一系列的思考、分析和推理步骤。

Long-CoT（长思考/慢思考）是CoT的一种扩展形式。 传统的CoT方法通过将复杂问题分解为一系列中间推理步骤来引导模型进行逐步推理。 而Long-CoT则进一步扩展了这种思路，**使得思考过程和输出的答案解耦，可以通过更长的上下文和更复杂的推理路径（在思考过程中通过加入问题复述、思考回顾、反思、知识回忆、公式化等思考节点）**来增强模型的推理能力。

DeepSeek 公开了他们 R1 的技术细节[5]，比如所采用的对话模版，如图6所示。DeepSeek 对社区的贡献还在于，他们提供的 API 展示了模型的思考过程，让从业人员以及使用者能够全方位地了解到 Long-CoT 的特点与作用。图7展示了我使用 DeepSeek 的深度思考以及联网搜索的一个示例，实实在在地感受到了模型的强大。

![image.png](%E6%9C%80%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%201b8e64a5662180f8a09be1fc4e97b8a3/image%2010.png)

- 可解释性

DeepSeek R1 和 R1 Zero 模型采用 Long CoT 方法，能够清晰地展示其推理过程（o1只能展示部分），提高了模型的可解释性。其中给出的思考过程也是一个非常有研究价值的内容，有助于大模型领域的发展。

### R1和R1-zero

### R1-zero

R1-Zero 的特别之处在于，它无需经过 SFT 训练集进行训练就能够在推理任务中表现出色。它的训练过程直接从一个预训练的基础模型（DeepSeek V3 Base）开始，通过强化学习训练完成。具体地：

- 采用群体相对策略优化（GRPO），节省RL的训练成本。
- 在RL训练过程中，采用Rule-based奖励，主要由两种奖励构成：a) Accuracy rewards：评估模型的输出是否正确；b) Format rewards：强制模型将其思考过程置于指定的和之间。
- 设计训练模版，指导基模型在训练过程中遵守设定的指令，即图7。

DeepSeek-R1-Zero 展示出了自我进化能力，随着强化学习训练进程的深入，模型的思考时间在增加，并自发出现了诸如反思，模型重新审视和重新评估其先前步骤以及探索解决问题的替代方法等更加复杂的行为。

![DeepSeek-R1-Zero平均回复长度随训练迭代步数的关系曲线](%E6%9C%80%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%201b8e64a5662180f8a09be1fc4e97b8a3/image%2011.png)

DeepSeek-R1-Zero平均回复长度随训练迭代步数的关系曲线

DeepSeek-R1-Zero 在训练过程中的平均回复长度，说明了随着训练进行，模型在解答推理类问题时，花了更多的时间去思考，以提高回答准确率。

### R1

尽管 DeepSeek-R1-Zero 展示了强大的推理能力，并能够自主发展出意想不到且强大的推理行为，但它也面临一些问题。例如，DeepSeek-R1-Zero 存在可读性差和语言混杂等问题。R1 旨在成为一个更易用的模型。因此，R1 并不像 R1-Zero 那样完全依赖于强化学习过程。训练过程分成四个阶段：

![DeepSeek-R1 训练流程](%E6%9C%80%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%201b8e64a5662180f8a09be1fc4e97b8a3/image%2012.png)

DeepSeek-R1 训练流程

https://blog.csdn.net/dongtuoc/article/details/145458418?spm=1001.2014.3001.5502

- **1.冷启动**：为了避免RL训练从基础模型开始的早期不稳定冷启动阶段，构建并收集少量长的 CoT 数据来微调 DeepSeek-V3-Base 作为 RL 的起点。
- **2.推理导向的强化学习**：在冷启动数据上微调 DeepSeek-V3-Base 后，应用与 DeepSeek-R1-Zero 中相同的 RL 方法训练。**本阶段侧重于增强模型的推理能力**，也运用**拒绝采样**，尤其是在编码、数学、科学和逻辑推理等推理密集型任务中，这些任务涉及具有明确解决方案的明确定义的问题。当 RL 提示涉及多种语言时，CoT 经常表现出语言混合现象。**为了减轻语言混合问题，在 RL 训练过程中引入了一种语言一致性奖励**。
- **3.拒绝抽样和监督微调**：当2中的RL过程趋于收敛时，**利用训练出的临时模型(R1-temp，第二次sft之前)**生产并用**拒绝采样**的方式收集用于下一轮训练的SFT数据（600K推理数据）。与1中的冷启动数据区别在于，此阶段既包含用于推理能力提升的600k数据，也包含200k推理无关的数据。使用这800k样本的精选数据集对DeepSeek-V3-Base进行了两个epoch的微调。
- **4.适用于全场景的强化学习**：在3中微调模型的基础上，使用全场景的强化学习数据提升模型回复的有用性和无害性。**对于推理数据，遵循 DeepSeek-R1-Zero 的方法，利用基于规则的奖励来指导数学、代码和逻辑推理领域的学习过程。**对于一般数据，采用基于模型的奖励来捕捉复杂和细微场景中的人类偏好。

### 从V3到R1-zero、R1和R1-distill

DeepSeek-R1-Zero，DeepSeek-R1 与 DeepSeek-R1-Distill 模型训练流程框图

![image.png](%E6%9C%80%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%201b8e64a5662180f8a09be1fc4e97b8a3/image%2013.png)

![image.png](%E6%9C%80%E6%96%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%201b8e64a5662180f8a09be1fc4e97b8a3/image%2014.png)

https://blog.csdn.net/weixin_65514978/article/details/145445780

第一阶段是冷启动，一开始要收集少量的Long-CoT数据来微调模型，目的是防止早期训练不稳定和可读性差问题。

与DeepSeek-R1-Zero不同，为了防止RL训练的早期阶段由于基础模型的不稳定而导致冷启动问题，因此需要**为DeepSeek-R1构建并收集少量长的思维链数据，作为对初始RL演员模型进行微调（SFT）的数据。**为了收集这些数据，探索了几种方法：使用少量提示并以长的思维链作为示例，直接提示模型生成详细的答案并进行反思和验证，收集DeepSeek-R1-Zero的输出并将其转化为可读格式，然后通过人工注释者进行后处理以优化结果。

冷启动数据的优势包括：

**可读性**：DeepSeek-R1-Zero的一个关键限制是其内容通常不适合阅读。回答可能会混杂多种语言，或者缺乏用于突出答案的markdown格式。相比之下，在为DeepSeek-R1创建冷启动数据时，设计一种可读的模式，在每个回答的末尾包括总结，并过滤掉不适合阅读的回答。定义的输出格式是 |special_token|<reasoning_process>|special_token|<summary>，其中reasoning_process是该查询的CoT，而summary用于总结推理结果。

**潜力**：通过精心设计冷启动数据的模式并结合人类先验，DeepSeek-R1在性能上优于DeepSeek-R1-Zero。迭代训练也许是推理模型更好的发展模式。

第二阶段是推理导向的强化学习，它以DeepSeek-V3为基础，针对推理密集型任务，用和R1-Zero相同的大规模RL来进行训练。同时它为了解决语言混杂问题，引入了语言一致性奖励。

第三阶段是拒绝抽样和监督微调，要真正训练R1了，所以它将第一阶段的模型加上一些抽样，结合其他领域的SFT数据，增强模型在写作、角色扮演和其他通用任务中的能力。

第四阶段是适用于所有场景的强化学习，数据准备好、进行微调之后，再以DeepSeek-V3为基础，先是SFT，然后进行所有场景的RL。对于推理任务就用基于规则的奖励来指导，对于一般任务就用RLHF（人类反馈强化学习）这种方式来进行。

## GPT4

## Qwen

## **DeepSeek 与 OpenAI 模型对比**

| **模型** | **发布时间** | **特点** | **训练方法** |
| --- | --- | --- | --- |
| GPT-4 | 2024.3 | 通用型语言模型，能够处理多种类型的任务。 | SFT + RL |
| GPT-4o | 2024.5 | 最大的特点是多模态能力，能够处理文本、图像、音频等多种输入，并生成相应的输出。能够快速处理请求，适合需要快速反馈的场景。 | SFT + RL |
| o1-preview O1 | 2024.9 2024.12 | 优势在于深度推理能力，采用长思维链（Long Cot）方法，能够在处理复杂问题时像人类思考一样将任务分解为多个简单步骤，更高效准确地解决问题。 | SFT + RL |
| R1-zero | 2025.1 | 同上 | RL为主 |
| R1 | 2025.1 | 同上 | SFT + RL |
| K1.5 | 2025.1 | 多模态思考模型，采用long CoT方法训练 | SFT + RL |
| o3 | 2025.2 | o1的改进版本 | ？ |
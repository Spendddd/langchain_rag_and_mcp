# transformer、LLaMA

State: 已完成

![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image.png)

# Transformer：encoder和decoder各6层

## FFN层

### **简要介绍FFN层**

Transformer模型通过多头注意力层和FFN层交替工作。FFN层存在于Transformer架构的编码器和解码器部分中。例如，下方的编码器块由多头注意力层和一个FFN层组成。

![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%201.png)

![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%202.png)

### **FFN的GLU变体**

![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%203.png)

![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%204.png)

由于这种方式使得FFN中的权重矩阵从2变为了3，为了使得模型的参数大体不变，因此中间层的向量维度需要削减为原始维度的三分之二。

在LLaMA2-7B中，FFN的原始输入维度为4096，一般而言中间层是输入维度的4倍等于16384。

由于SwiGLU的原因FFN从2个矩阵变成3个矩阵，为了使得模型的参数量大体保持不变，中间层维度做了缩减，缩减为原来的2/3即10922，进一步为了使得中间层是256的整数倍，有做了取模再还原的操作，最终中间层维度为11008。

![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%205.png)

## FFN的作用

https://mp.weixin.qq.com/s/dLTHD05wezA5pAM2c4j07Q：**Transformers 中 FFN 的作用**

### 1、模型设计的角度：对V进行非线性变换的唯一步骤

假设没有 FFN，就是 attention 的堆叠。那么有：

![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%206.png)

通过上面的公式可以看出，无论堆叠多少层，都是最开始输入 x 的一个线性变换。线性变换无法处理一些非线性的特征，恰如当年马文明斯基给神经网络判了死刑，只需要加个非线性变换的激活函数就能起死回生。Attention, FFN, ResNet 缺一不可但却可能是各司其职，我个人的观点（并不一定准确）是， **Attention 的功能是做信息的提取和聚合，Resnet 提供信息带宽，而真正学到的知识或者信息都存储在 FFN 中。**在图像领域中，也有一种说法，那就是 Attention 其实是 token mixer, **FNN 其实是 channel mixer.**

### 2、功能性角度

- **缓解各向异性**
    
    《 Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth》这篇论文，提出了 Transformers 架构存在 token uniformity 的归纳偏置(inductive bias，有时候也叫**归纳偏好**)问题。如果去掉 FFN 或者 Resnet，则问题更加严重。
    
    这里解释一下这两个名词，所谓归纳偏置，可以通俗的理解为模型的“个性”，就是满足训练集合的解法有无数种，但是不同的模型架构会让模型更偏向于某些解法。比如我们常用的一些正则化方法，其实就是让模型的归纳偏置倾向于选择一些简单的解法。任何模型都有归纳偏置，尤其是碰到未见过的样本的时候，模型的归纳偏置就更容易体现出来。**Transformers 的一个归纳偏执是什么呢？就是 token uniformity，有时候也叫 information diffusion，或者 anisotropic (各向异性)**，也就是说 训练完后的 token 会共享很多相似信息。
    
    - 各向异性是什么
        
        各向异性：https://www.zhihu.com/question/460991118/answer/2353153090：**Bert中的词向量各向异性具体什么意思啊？**
        
        **各向异性在向量空间上的含义就是分布与方向有关系，而各向同性就是各个方向都一样**，比如二维的空间，各向异性和各向同性对比如下(左图为各向异性，右图为各向同性)：
        
        ![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%207.png)
        
        学者们([Gao et al. 2019](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1907.12009) 和 [Wang et al. (2020)](https://link.zhihu.com/?target=https%3A//openreview.net/pdf%3Fid%3DByxY8CNtvr))发现Transformer学到的词向量在空间的分布是这个样子的：
        
        ![](https://picx.zhimg.com/80/v2-ac63b89f5ff6dab40b63bbd784d8aac1_1440w.webp?source=2c26e567)
        
        [Ethayarajh, 2019](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1909.00512) 发现类似的情况在BERT，GPT-2中同样存在。看上面的图就知道模型学到的向量分布是各向异性的。
        
        解决各向异性的方法有很多，比如
        
        **1. 映射为各向同性**
        
        BERT-flow的工作就是将原来的分布校准为[高斯分布](https://zhida.zhihu.com/search?content_id=457928153&content_type=Answer&match_order=1&q=%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83&zhida_source=entity)。标准的高斯分布就是各向同性的。
        
        ![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%208.png)
        
        类似的还有**whitening**操作。大概流程就是根据SVD分解的结果，旋转缩放后得到一个标准正态分布。
        
        ![](https://picx.zhimg.com/80/v2-64c44ba2cb17ec940acf5356a031e1cc_1440w.webp?source=2c26e567)
        
        **2. 消除主成分**
        
        **3. 正则化**
        
    
    回到论文，论文将 FFN 和 ResNet 去掉之后做了一些消融实验，证明了 **FFN 和 ResNet 是 Transformers 中的必备组件，这两个可以大大的缓解 token uniformity 或者 各向异性的问题。**
    
    论文中从数学上证明了经过 Attention 变换后的输出与 rank-1 的矩阵之间的差值存在上界，但是有点复杂，我也没仔细推导过。**简单一点的理解呢，就是上面说的 Attention 本质上是 value 的线性变换（虽然线性变换的权重是非线性的softmax）。**Every self-attention “layer” is a linear transformation of the previous layer (with non-linear weights)
    
    当然并不是说 Transformers 就已经将 token uniformity 问题解决了，这个问题依然存在，所以后续又有 Bert-flow 、whitening 等改进。
    
- **FFN 承担了记忆功能**
    
    《Transformer Feed-Forward Layers Are Key-Value Memories》这篇文章做了很多实验和统计，得出了以下结论：
    
    1. FFN 是一个 Key-Value 记忆网络，**第一层线性变换是 Key Memory，第二层线性变换是 Value Memory。**
    2. FFN 学到的记忆有一定的可解释性，比如低层的 Key 记住了一些通用 pattern (比如以某某结尾)，而高层的 Key 则记住了一些语义上的 Pattern （比如句子的分类）。
    3. Value Memory 根据 Key Memory 记住的 Pattern，来预测输出词的分布。
    4. skip connection 将每层 FFN 的结果进行细化。
    
    从这个角度来说，Attention 是对短期的信息进行提取，而 **FFN 则对整个训练样本进行信息提取和记忆**。**这也就能解释为什么一个有限的窗口甚至对语料进行了暴力截断，模型也能记住语料库中的信息。**
    
    《Knowledge Neurons in Pretrained Transformers》这一篇就更有意思，**在上一篇的基础上，对 Transformers 进行了前额叶切除手术，擎天柱瑟瑟发抖**。
    
    研究人员先是**定位到对某些事实或者知识影响较大的神经元，然后神经元内的数值进行增强或者抑制，发现 Transformers 对这些事实或者知识的回答效果也会变好或者变差**。如果将这个神经元删掉，也就是值全部置0，则 Transformers 完全忘记了这些知识，更神奇的是，对于其他的知识则影响不大。
    
    更进一步的，研究人员还对神经元的内容进行了替换操作以达到“篡改记忆”的效果。当然作者只是在 BERT 上进行了实验，随着预料和模型的增大，像定位知识的记忆也愈发的困难，但是给了人们一个**可控文本生成**的研究方向，未来可期。
    
- **FFN 是一种混合专家模型：通过MoE体现**
    
    一直以来，**神经网络就存在稀疏激活的现象，也就是在推理的时候，其实只有极小一部分参数参与了计算。**这篇论文则通过 **[MoE](MoE%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%201b8e64a566218027ad4bf217e06d1e1b.md)** 的思想来**将 FFN 层拆分成了多个专家**，并且新增了一个路由模块来确定推理的时候来挂哪个专家的门诊：）
    
    这么做完之后，在提升推理速度的同时，效果依然能保持原来的95%以上。
    
    - 神经网络的稀疏性
        
        https://www.cnblogs.com/sasasatori/p/17809829.html
        
        **神经网络的稀疏性是什么**
        
        随着现代神经网络的规模不断提升，其消耗的内存，算力，能量都不断增加，这构成了在神经网络在实际应用中的瓶颈。如何尽可能的缩小网络同时又不损失其性能成为了一大神经网络方面的研究重点，目前常用的方法包括：**剪枝**，量化，网络结构搜索，知识蒸馏等。
        
        我们现在讨论的神经网络稀疏与剪枝是紧密关联的，具体来说，**剪枝即剪去神经网络中一些冗余的连接关系。**由于神经元在神经网络中的连接在数学上表示为权重矩阵，因此**剪枝的具体实现即将权重矩阵中一部分元素变为零元素。这些剪枝后具有大量零元素的矩阵被称为稀疏矩阵，反之绝大部分元素非零的矩阵被称为稠密矩阵。**
        
        最早的关于神经网络稀疏的讨论可以追溯到1990年，LeCun和同事们在AT&T贝尔实验室工作时便发表了论文提倡这种压缩网络的方法。2015年Han等人发表了一篇论文[[1]](https://www.cnblogs.com/sasasatori/p/17809829.html#fn1)提出了剪枝网络的三步方法论：**1) 训练一个稠密网络 2) 以权重幅度代表其显著性，剪枝掉最不重要的权重 3）重新训练网络来微调这些剩下连接的权重。**这项工作不仅在当时的深度学习模型上有效，甚至可以相比原模型达到更高的精度。
        
        ![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%209.png)
        
        在ICLR2019上，Frankle和Carbin提出了彩票假设[[2]](https://www.cnblogs.com/sasasatori/p/17809829.html#fn2)，他们的论文也是ICLR2019的best paper，该假设认为：任何随机初始化的稠密的前馈网络都包含具有如下性质的子网络——在独立进行训练时，初始化后的子网络在至多经过与原始网络相同的迭代次数后，能够达到跟原始网络相近的测试准确率。因此他们认为通过剪枝方法可以消除网络中超过90%的权重，大大减少存储需求并提升推理的计算性能同时不损失精度。与Han等人得到的结论不同，在彩票假设中，剪枝后的网络不是需要进行微调，而是将“中奖”的子网络重置为网络最初的权重后重新训练，最后得到的结果可以追上甚至超过原始的稠密网络。彩票假设最开始被认为非常疯狂，但现在已经被广泛接受——过参数化的稠密网络中包含了若干个稀疏的子网络，这些子网络性能各异，其中之一就是“中奖”的子网络，其性能超越了其他子网络，因此后续研究可以更加关注于如何有效的找到这一“中奖”的子网络。
        
        **稀疏的方式**
        
        ![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%2010.png)
        
        - 非结构化稀疏：将一部分权重值小于阈值的权重置为零
            
            在介绍剪枝时我们提到一种朴素的方法就是剪去重要性比较低的权重，如果将权重的幅值视作重要性，那么就是将绝对值小于某一阈值的权重元素改为零。不管采用具体何种替换权重元素为零的方式，我们是没有先验的考虑这些权重元素的分布需要满足某种规律的，我们可能会删除权重矩阵中任意位置任意数量的元素——只要它们满足算法中的删除条件。
            
            但非结构稀疏对于硬件实现并不友好，不确定的稀疏结构提高了硬件结构处理稀疏的复杂性，导致开销的增大，效率的降低。为了能够降低加速稀疏结构的硬件结构复杂度，提升计算性能，可以采用结构化稀疏方案。
            
        - 结构化稀疏
            
            结构化稀疏方案即**人工先验的设计好稀疏结构，例如保留多少权重，保留的权重在矩阵中的空间位置等。**由于提前知道了非零权重的分布规律，硬件结构可以直接按照分布规律进行设计，简化了其设计复杂度。
            
            结构化稀疏可以进一步划分为**粗粒度结构化稀疏**方案和**细粒度结构化稀疏**方案，粗粒度结构化稀疏方案一般会以较粗的粒度进行权重元素的删除（归零），例如以通道/块/条为单位进行结构化稀疏，可以理解成都是**以某种连续的几何区域进行权重的删除**，这种删除权重的连续几何区域面积越大，我们可以认为其粒度越粗。Mao等人在论文中针对CNN的稀疏粒度和网络最终的精确性的关联性进行了研究，并在AlexNet，VGG-16，GoogLeNet，ResNet-50，DenseNet-121等网络上进行了实验，从最终的实验结果来看，**整体上网络的精度和稀疏粒度之间呈反相关性，在相等的稀疏度下，细粒度稀疏普遍能具有最佳的精度**，其次是以条为单位几何区域进行元素删除的向量级稀疏，最后是以块为单位几何区域进行元素删除的核级稀疏[[4]](https://www.cnblogs.com/sasasatori/p/17809829.html#fn4)。
            
            ![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%2011.png)
            
            由于细粒度稀疏能够使得网络具有更好的精度，后续的研究者致力于如何实现细粒度结构化稀疏，Nvidia在2020年推出安培架构的A100显卡中引入的**稀疏向量核心**[[5]](https://www.cnblogs.com/sasasatori/p/17809829.html#fn5)为细粒度结构化稀疏提供了硬件基础，在此基础上Sun等人[[3:1]](https://www.cnblogs.com/sasasatori/p/17809829.html#fn3)和ICLR2021上的Zhou等人[[6]](https://www.cnblogs.com/sasasatori/p/17809829.html#fn6)都对**N:M细粒度结构化稀疏**方法进行了研究，所谓N:M细粒度结构化稀疏即**在M个连续的权重值中固定有N个非零值，其余元素均为零值**，例如下图中N=2，M=4的N:M稀疏（2:4稀疏）。**N:M稀疏的权重矩阵可以进行压缩存储，仅保留所有的非零元素，再辅以一个indices矩阵指示非零元素在原矩阵中的空间位置。**从实验结果上来看，采用N:M细粒度结构化稀疏方案的模型在权重存储量和计算量大减的同时取得了比肩甚至超过稠密的原网络的推理精度。
            
        - Transformer的稀疏
            
            ### **Transformer的稀疏**
            
            Transformer主要由注意力层和前馈层组成，其中注意力层的计算规则为：
            
            ![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%2012.png)
            
            其中QKT/dk为注意力分数，softmax(QKT/dk)为注意力权重。OpenAI的Child等人[[11]](https://www.cnblogs.com/sasasatori/p/17809829.html#fn11)研究发现，在实际的网络中注意力权重存在明显的冗余，对其进行分解后可以将自注意力机制原有的O(n2)计算复杂度降低到O(n开方n)，从而使得硬件平台在同等存储容量和算力的情况下可以建模更长的序列，这项研究为后续探索自注意力的稀疏提供了基础。
            
            在ICLR2020上，Zhao等人[[12]](https://www.cnblogs.com/sasasatori/p/17809829.html#fn12)进一步研究发现保留少量注意力权重参与注意力运算可以在几乎不影响模型性能的同时大大增大注意力权重的稀疏度，具体来说，**在计算完注意力分数P=QKT/dk后，在送入softmax函数之前，可以对注意力分数进行排序，对于注意力权重矩阵，每行仅保留前k大的元素，剩余的元素全部取为负无穷**，即：
            
            ![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%2013.png)
            
            之后将 M(P,k)ij 再送入softmax函数进行计算，**此时负无穷的元素经过运算后全部为零，仅有前k大的注意力权重得到了保留，之后再用注意力权重与V进行运算即可**，即：
            
            ![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%2014.png)
            
            他们在机器翻译，图像分割，语言模型等任务上测试了模型的效果，结果显示在模型性能与前面的sota可以比肩的同时，模型的速度得到了显著的提升。上述计算的结构图如下图所示：
            
            ![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%2015.png)
            
            总体来说，由于注意力运算的特殊性，相比传统的静态权重剪枝，注意力权重类似于激活，也是**动态且依赖于输入的**，如何充分的利用好注意力权重稀疏仍然是一个待研究解决的问题。
            

## Attention

Wo：处理多头注意力的输出矩阵的拼接后的结果，将head_num * (hidden_dim/head_num) 维重新转换为hidden_dim维（实际上就是hidden_dim → hidden_dim，但是需要对concat之后的结果做一定的线性变换才能使多头的结果更好融合）

![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%2016.png)

![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%2017.png)

![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%2018.png)

## 残差连接的作用：防止梯度消失、使所有层都能“看”到原始输入的知识，网络可以更容易地学习到恒等映射，让模型的理解更加全面，学习到更丰富的表征，让训练过程变得更加高效稳定

### 为什么残差连接可以防止梯度消失？

残差连接通过以下机制缓解梯度消失问题：

1. **梯度路径的冗余性**：
    
    残差连接的输出为 Output = F(x) + x ，其中 F(x) 是网络层的变换。在反向传播时，梯度会从输出端分成两条路径：
    
    - 一条通过 F(x) 传递，涉及权重矩阵和非线性激活函数的梯度计算；
    - 另一条直接通过跳跃连接传递，梯度为。
        
        ![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%2019.png)
        
    - **即使 F(x) 的梯度趋近于零（例如激活函数饱和），跳跃连接的梯度仍能保持为常数，避免梯度完全消失**。
2. **打破梯度相乘的链式依赖：抑制逐级相乘带来的指数级梯度衰减或膨胀**
    
    ![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%2020.png)
    
3. **稳定信号传播**：
    
    残差连接**强制网络学习输入与输出之间的残差**（即 F(x) = Output - x ）。在深层网络中，**若直接学习完整输出，小权重的累积可能导致信号幅度剧烈变化**；而学习残差时，初始权重可以接近零（即  F(x) \approx 0 \)），使得网络更容易初始化并稳定训练。
    

---

### Transformer中如果不使用残差连接会有什么后果？为什么？

**后果**：

1. **梯度消失加剧**：
    
    Transformer通常包含数十甚至数百层（如GPT-3有96层）。若没有残差连接，梯度需通过所有层的链式乘法反向传播，导致梯度幅值随层数指数级衰减或膨胀，最终难以有效更新浅层参数。
    
2. **模型难以收敛**：
    
    深层网络的信息传递依赖每一层的精确变换。**若移除残差连接，输入信号经过多次非线性变换后可能严重失真（**例如ReLU的零区域截断），**导致深层网络无法学习有效特征。**
    
3. **性能显著下降**：
    
    实验表明，移除Transformer中的残差连接会使模型在语言建模、翻译等任务上的性能急剧下降。例如，BERT-base在移除残差连接后，下游任务准确率可能降低超过20%。
    

**原因**：

1. **深层网络的脆弱性**：
    
    Transformer的自注意力机制和前馈网络本质上是**高维非线性变换**。残差连接通过保留原始输入（\( x \)），确保**即使深层网络的变换能力有限（F(x) ~ 0），输入信息仍能无损传递，避免信号退化**。
    
2. **训练动态的优化**：
    
    残差连接的加法操作**使损失函数对参数的梯度更加平滑**（例如 \( \frac{\partial \text{Loss}}{\partial W} \) 的方差降低），从而允许使用更大的学习率并加速收敛。**若移除残差连接，梯度可能因深层非线性变换而剧烈震荡，导致优化过程不稳定。**
    
3. **与层归一化的协同作用**：
    
    Transformer中残差连接通常与层归一化（LayerNorm）配合使用（例如 LayerNorm(x + Sublayer(x)) 。**层归一化对输入分布进行标准化，而残差连接则确保标准化后的输入仍能保留原始信号强度。**若移除残差连接，**层归一化可能放大噪声信号，进一步破坏训练稳定性**。
    

---

### 总结

- **残差连接的核心作用**：通过加法操作提供梯度传播的冗余路径，打破链式相乘的依赖，稳定深层网络的训练动态。
- **Transformer对残差连接的依赖**：在深度和复杂结构下，残差连接是**维持梯度流动、信号完整性和模型性能**的关键设计。移除后，模型将面临严重的优化障碍和性能退化。

## 归一化的作用

### 为什么要进行归一化？

归一化的主要目的是**解决数据分布不一致带来的训练难题**，具体包括以下几点：

---

### **1. 数据尺度统一**

- **问题**：输入特征的量纲或范围差异大（例如特征A的范围是[0,1]，特征B的范围是[100,1000]），导致**模型对不同特征的敏感度不同**。
- **解决**：归一化（如Z-Score标准化、Min-Max缩放）**将特征映射到相似范围（如均值为0、方差为1），使优化过程更稳定**。

---

### **2. 加速模型收敛**

- **问题**：数据分布差异会导致损失函数的等高线呈“狭长形”，梯度下降方向震荡，收敛速度慢。
- **解决**：归一化使损失函数的等高线更接近“圆形”，梯度方向更一致，从而加快收敛速度。

---

### **3. 缓解梯度不稳定**

- **问题**：深层网络中，参数更新可能因输入尺度差异导致梯度爆炸或消失。
- **解决**：归一化通过稳定激活值的分布，避免梯度幅值剧烈波动。

---

### 归一化在Transformer中的作用

在Transformer中，**Layer Normalization（层归一化）** 是核心组件之一，其作用如下：

---

### **1. 稳定子层输出分布**

- **操作**：每个子层（如自注意力层、前馈网络）的输出后接LayerNorm，公式为：
    
    ![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%2021.png)
    
- **作用**：
    - **抑制因非线性变换（如自注意力计算）导致的输出分布偏移**；
    - **确保输入到下一层的数据具有稳定的均值和方差，缓解梯度传播问题。**

---

### **2. 与残差连接的协同**

- **残差连接**：提供**恒等映射**路径，保留**原始信号**；
- **层归一化**：对残差路径的输出进行标准化，避免信号幅度随网络深度累积而失控。
- **联合效果**：两者共同确保深层网络的训练稳定性（如Transformer可能包含数十层）。

---

### **3. 减少对初始化的敏感度**

- **问题**：**深度模型对参数初始化极为敏感，不当初始化易导致激活值饱和（如ReLU输出全零）。**
- **解决**：**归一化通过标准化每一层的输入，降低对初始权重的依赖**。

---

### 如果没有归一化会怎样？

### **1. 训练不稳定性加剧**

- **梯度爆炸/消失**：深层网络中，未归一化的激活值可能**导致梯度幅值随层数指数级变化。例如，梯度可能在反向传播时因连续相乘而爆炸（>1）或消失（<1）。**
- **现象**：训练初期Loss剧烈震荡或长期不下降。

---

### **2. 模型收敛困难**

- **参数更新失衡**：**不同特征或层输出的尺度差异会导致某些参数更新过快，而另一些几乎不更新。**
- **后果**：模型可能陷入局部最优，或需要极低的学习率才能稳定训练。

---

### **3. 性能显著下降**

- **实验验证**：在Transformer中移除LayerNorm后，模型在语言建模、翻译等任务上的性能可能下降20%以上。
- **原因**：未归一化的自注意力输出分布不稳定，导致注意力权重计算失效（如Softmax后某些维度接近0或1）。

---

### **4. 依赖精细调参**

- **学习率调整**：需手动调整学习率以适配不同层的梯度幅值，极大增加调参成本；
- **初始化限制**：必须使用特定初始化方法（如Xavier初始化）来补偿未归一化的输入分布，但仍难以解决深层问题。

---

### 总结

| **归一化的作用** | **无归一化的后果** |
| --- | --- |
| 统一数据尺度，加速收敛 | 训练速度慢，收敛困难 |
| 稳定梯度传播，防止爆炸/消失 | 梯度不稳定，模型易崩溃 |
| 降低对初始化和学习率的敏感度 | 需复杂调参，初始化容错性低 |
| 与残差连接协同保障深层网络稳定性 | 深层模型性能急剧下降 |

在Transformer中，**LayerNorm是维持模型深度和复杂结构稳定性的关键设计**。若移除归一化，模型将面临梯度失控、训练困难及性能退化等问题。

# LLaMA：SwiGLU、RoPE、RMSNorm

![image.png](transformer%E3%80%81LLaMA%201b8e64a56621806f9c95df904958e267/image%2022.png)

```
 # Linear 1
  self.w1 = ColumnParallelLinear(...)
  # Linear 2
  self.w2 = RowParallelLinear(...)
  # Linear 3
  self.w3 = ColumnParallelLinear(...)
  def forward(self, x):
      return self.w2(F.silu(self.w1(x)) * self.w3(x))
      # silu就是swish，层1和层3共同组成SwiGLU，因此说llama中前馈层的激活方法是SwiGLU
```
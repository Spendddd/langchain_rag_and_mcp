# 大模型发展史

State: 学习中

![image.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E5%8F%B2%201b8e64a5662180d09315ce75d073f189/image.png)

https://zhuanlan.zhihu.com/p/627923395**Transformers回顾 ：从BERT到GPT4**

## Transformer：post norm、正弦余弦位置编码、encoder-decoder、seq-2-seq

Transformer 结构提出是用于**机器翻译任务**，机器翻译是一个**序列到序列**的任务，因此 Transformer 设计了Encoder 用于提取源端语言的语义特征，而用 Decoder 提取目标端语言的语义特征，并生成相对应的译文。用post norm

### 损失函数

在 Transformer 模型的训练过程中，交叉熵损失的计算流程大致如下：

1. 输入序列通过编码器：源语言句子经过编码器，编码成上下文表示。
2. 目标序列经过解码器并生成预测：解码器在每个时间步利用编码器的输出和已经生成的词，预测下一个词。模型在输出层会生成一个向量，通过 softmax 转化为一个概率分布 ( p )。
3. 计算交叉熵损失：
4. 对于每一个时间步，将预测的概率分布 ( p ) 与真实标签 ( y ) 进行对比。
根据交叉熵公式计算损失，即将正确的标签位置的概率 ( p ) 取负对数（因为越接近 1，负对数越小），并对每个时间步的损失求和。
输出损失值：整个序列的损失值会反馈给优化器，用于更新模型参数，逐渐提高模型对目标序列的预测准确性。

## Bert系列：MLM、NSP、post norm、encoder-only、CLS、

https://www.cnblogs.com/nickchen121/p/15114385.html#%E5%8D%81bert-%E6%A8%A1%E5%9E%8B

## BERT

![image.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E5%8F%B2%201b8e64a5662180d09315ce75d073f189/image%201.png)

![image.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E5%8F%B2%201b8e64a5662180d09315ce75d073f189/image%202.png)

Bert Embedding由三部分组成，分别是

- **Token Embeddings**是词向量，第一个单词是CLS标志，可以用于之后的分类任务
- **Segment Embeddings**用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务
- **Position Embeddings**和之前文章中的Transformer不一样，不是三角函数而是学习出来的

其中[CLS]表示该特征用于分类模型，对非分类模型，该符号可以省去。[SEP]表示分句符号，用于断开输入语料中的两个句子。

BERT在第一句前会加一个[CLS]标志，最后一层该位对应向量可以作为整句话的语义表示，从而用于下游的分类任务等。因为与文本中已有的其它词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个词的语义信息，从而更好的表示整句话的语义

### **预训练**

BERT的预训练过程主要包括两个阶段：Masked Language Model（MLM）和Next Sentence Prediction（NSP）。

**1、Masked Language Model (MLM)**

Masked Language Model，即遮蔽语言模型，是BERT预训练的一个重要部分。在这一阶段，模型的任务是预测输入句子中被随机遮蔽（masked）掉的部分单词。

- 输入文本处理：首先，对于输入的句子，随机选择句子中15%的单词进行遮蔽。对于每个被选中的单词，有80%的概率直接用[MASK]标记替换，10%的概率用随机的一个单词替换（这有助于模型学习理解上下文的重要性，而不仅仅是依赖于[MASK]标记），剩下的10%则保持不变（这有助于模型在微调阶段更好地处理未遮蔽的单词）。
- 模型预测：模型的目标是根据上下文预测这些被遮蔽单词的原始值。这种机制促使BERT能够深入理解文本中的语义关系。

![](https://pic4.zhimg.com/v2-fa0f75d488a63c97ba3db4963a02ad05_1440w.jpg)

我们来看一个例子，假设有一句话：my dog is hairy

1）80%的时候是[MASK]。如，my dog is hairy——>my dog is [MASK]

2）10%的时候是随机的其他token。如，my dog is hairy——>my dog is apple

3）10%的时候是原来的token。如，my dog is hairy——>my dog is hairy

**2、Next Sentence Prediction (NSP)**

Next Sentence Prediction，即下一句预测，是BERT预训练的另一个重要部分，旨在提高模型对句子间关系的理解能力。

- 句子对生成：在预训练时，模型不仅接收单个句子作为输入，还接收句子对。这些句子对可能是连续的（即真实的下一句），也可能是随机组合的（即非连续的）。
- 模型预测：对于每个句子对，模型需要预测第二个句子是否是第一个句子的真实下一句。这是一个简单的二分类任务，输出是一个[0, 1]范围内的值，表示第二个句子是第一个句子真实下一句的概率。

注：在BERT的后续版本中，Next Sentence Prediction（NSP）任务被废弃了。因为研究人员发现这个任务对下游任务的性能提升有限，因此在BERT的一些后续变体中被弃用了。

根据自然语言处理（NLP）下游任务输入和输出形式的不同，微调任务可以分为四类，分别是句对分类、单句分类、文本问答和单句标注，如下图中abcd所示。

![](https://pic4.zhimg.com/v2-35e21b931f583732a965817848dad35b_1440w.jpg)

### **微调**

**1、句对分类**

- 任务描述：句对分类任务涉及两个句子的输入，并需要模型判断这两个句子之间的关系或情感倾向等。
- 应用场景：例如，自然语言推断（NLI）任务，需要判断一个句子是否可以从另一个句子推断出来；或者语义文本相似性（STS）任务，需要评估两个句子的语义相似度。

**2、单句分类**

- 任务描述：单句分类任务是将单个句子作为输入，并输出该句子的类别或情感倾向。
- 应用场景：如情感分析（判断文本是正面、负面还是中性）、垃圾邮件检测（判断邮件是否为垃圾邮件）等。

**3、文本问答**

- 任务描述：文本问答任务涉及一个问题和一段文本（如文章或段落），模型需要从文本中找出问题的答案。

**4、单句标注**

- 任务描述：单句标注任务是对句子中的每个词或子词进行标注，如命名实体识别（NER）、词性标注（POS Tagging）等。
- 应用场景：在信息抽取、文本分析等领域有广泛应用。

## RoBerta

改进点

![image.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E5%8F%B2%201b8e64a5662180d09315ce75d073f189/image%203.png)

只做MLM、动态MLM

### **1、Dynamic Masking**

BERT中有MLM的预训练任务，在预训练任务重需要将一部分的token进行MASK，针对MASK的策略分为两种

- **Static Mask**
    - 在数据准备阶段，将Mask后，训练数据将不再变化
- **Dynamic Masking**
    - 期望在每轮数据中，Mask的位置也是相应的变化

RoBERTa使用的就是**Dynamic Masking**

### **2、其他改进**

- Full-Sentences without NSP
    - RoBERTa通过实验去掉NSP任务，会提升在下游任务上的指标
- Larger Batch Size
    - RoBERTa通过实验发现，训练过程中增加batch size的大小，由于降低训练数据的Perplexity，提高在下游任务上的指标
- More Data and More Training Steps
    - 相比于BERT，RoBERTa使用了更多的训练数据，随着数据量的增加，模型的在下游任务上的表现也是不断提升

## T5：预测完整文本而不是只看masked token部分，对所有任务都通过加引导词的方式实现，区间mask，用<X>, <Y>, <Z>的顺序符号保持mask部分的顺序信息，encoder-decoder

https://zhuanlan.zhihu.com/p/178432137**刷榜标配系列！NLP预训练模型前沿技术解析 (二)：T5**

T5 借用**Seq2Seq** [2]的思想**将预训练模型不同阶段（Pretrain，Fine-tune，Predict）的任务统一到Text-to-Text**（即模型输入是文本，输出也是文本）的任务框架内；T5同时使用了Transformer Encoder和Transformer Decoder。

对于原始文本（Original Text），T5首先会随机Mask掉某个区间的词并用特殊的标识符（如<X>，<Y>，<Z>等）来替换被Mask掉的区间（Span）, 来作为模型的输入（Inputs）。之后在预训练阶段（Pretrain）阶段，类似于BERT的MLM，T5会预测被Mask的区间是哪些词，但与BERT不同的是，**T5的预训练标签（Targets）会通过<X>，<Y>，<Z>等带有顺序信息的特殊字符来与输入中的<X>, <Y>, <Z>等特殊标识符做对齐（Align）。**这是因为T5采用的**Text-to-Text**框架，需要通过这样的标识符来对齐被Mask的区间（Span）和相应的预训练标签（Targets）。

![](https://pic1.zhimg.com/v2-650e18fc92f65d065ef697dc48f95304_1440w.jpg)

在模型微调阶段（fine-tune），T5为了将不同的下游任务（如文本分类，翻译，文本摘要等）在Text-to-Text的框架内统一建模，针对不同任务，设计了不同的任务引导符。具体来说，如图2，当处理翻译任务时，会通过增加任务引导符“translate English to German”来标识从英语翻译到德语这个任务，这时对于英语句子“That is good”, 完整的模型输入就会变成“translate English to German: That is good“，模型的预测目标就是对应的德语文本序列”Das ist gut.“（图2 绿色部分）。

同样的，对于CoLA（Corpus of Linguistic Acceptability）这样的文本分类任务，我们可以通过”cola sentence“这个任务引导符，来对“The course is jumping well。“做语法是否合理的二分类，这里二分类标签可以用”not acceptable / acceptable”这样的文本来表达（图2 红色部分）。对于需要输出连续值的文本相似度任务STSB（图2 黄色部分），可以通过“stsb sentence1: xxx. sentence2:xxx.”这样的格式构造模型输入来预测文本相似度，相似程度的表示也可以通过文本表达（图 2黄色部分模型的输出“3.8”）。

![image.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E5%8F%B2%201b8e64a5662180d09315ce75d073f189/image%204.png)

![image.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E5%8F%B2%201b8e64a5662180d09315ce75d073f189/image%205.png)

## BART **Facebook / 2019：文档去噪/破坏文档恢复，交叉熵损失**

**在预训练阶段**，BART将预训练的目标定义为**破坏文档恢复**——在编码器-解码器架构中，编码器的输入即为被破坏的文档序列，而解码器输出的序列即为恢复后的文档序列。**BART在恢复文档与未经破坏文档之间构造交叉熵损失。**“被破坏”等价于“有噪声”，因此BART对破坏文档进行恢复的过程，等价于针对带有噪声数据进行的去噪操作，BART在文章题目中明确提到了“去噪”。在架构方面，BART扮演的正是去噪自编码器的角色，故在很多文献对模型进行的细化分类中，BART也被划分入去噪自编码语言建模的范畴。事实上，关于对文档的破坏，BART大量借鉴了BERT的掩膜思路，毕竟遮掩也是破坏的一种，只不过BART眼里的破坏具有任意性，因此使用包括掩膜在内的更多破坏方法。具体来说，BART使用的破坏方法有五种，包括：符号遮掩（Token Masking）、符号删除（Token Deleting）、文本填充（Text Infilling）、语句重排（Sentence Permutation）和文档轮换（Document Rotation）。

在**下游任务微调适配训练**方面，BART给出了其针对序列分类（Sequence Classification）、符号分类（Token Classification）、序列生成（Sequence Generation）和机器翻译四类任务进行模型微调适配的可能架构和方法。

## GPT系列

https://zhuanlan.zhihu.com/p/620494604**GPT-1/GPT-2/GPT-3/GPT-3.5 语言模型详细介绍**

![image.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E5%8F%B2%201b8e64a5662180d09315ce75d073f189/image%206.png)

![image.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E5%8F%B2%201b8e64a5662180d09315ce75d073f189/image%207.png)

## GPT1：learnable position embedding、decoder-only

GPT-1 目标是服务于单序列文本的生成式任务，所以舍弃了关于 Encoder 部分以及包括 Decoder 的 Encoder-Dcoder Attention 层（也就是 Decoder中 的 Multi-Head Atteion）。

GPT-1中，采用与词向量相似的随机初始化，并在训练中进行更新，即是**把每一个位置当做一个要学习的embedding来做**。

![](https://pic4.zhimg.com/v2-76d1252840457fade3987332462b30af_1440w.jpg)

GPT-1 fine-tune框图

接下来在下游任务的fine-tune过程当中，可以看到GPT的各种不一样的玩法。其实仔细的研究一下可以发现，基本上都是一个Transformer+一个线性层来表达的，不同的地方在于模型的输入部分。而需要注意的另外一个细节，是**fine-tune的loss函数，既包含了下游任务的loss，也包含了语言模型的loss（预测下一个单词）**，这么做的目的是在做垂直领域任务的时候，保持着自己本身的这种语言模型的性质，不要把语言本身给忘掉。

### **不同下游任务的输入转换**

针对不同的下游任务，需要对输入进行转换，从而能够适应 GPT-1 模型结构，比如：

**分类任务**。只需要在输入序列前后分别加上开始（Start）和结束（Extract）标记

![](https://pic1.zhimg.com/v2-183f55210124bb4b3f8581db1c3ac17c_1440w.jpg)

- **句子关系任务**。除了开始和结束标记，在两个句子中间还需要加上分隔符（Delim）

![](https://picx.zhimg.com/v2-212c483f3e556429a88d70375b701cc1_1440w.jpg)

- **文本相似性任务**。与句子关系判断任务相似，不同的是需要生成两个文本表示
    
    hlm
    

![](https://picx.zhimg.com/v2-a5dd75a36ddfe770190ee6f596758823_1440w.jpg)

- **多项选择任务**。文本相似任务的扩展，两个文本扩展为多个文本。

![](https://pic1.zhimg.com/v2-b9ef050978be3797983cf87e92fbf448_1440w.jpg)

## GPT2：zero-shot→引入类prompt方法、pre-norm、根据残差层数量缩放参数、BPE

### **区别**

GPT-2和GPT-1的区别在于GPT-2使用了更多的网络参数和更大的数据集，以此来训练一个泛化能力更强的词向量模型。GPT-2相比于GPT-1有如下几点区别：

1. **主推zero-shot**，而GPT-1为pre-train+fine-tuning；

2. 模型更大，参数量达到了15亿个，而GPT-1只有1亿个；

3. 数据集更大，WebText数据集包含了40GB的文本数据，而GPT-1只有5GB；

4. 训练参数变化，batch_size 从 64 增加到 512，上文窗口大小从 512 增加到 1024；

所以 GPT-2 的核心思想就是，当模型的容量非常大且数据量足够丰富时，仅仅靠语言模型的学习便可以完成其他有监督学习的任务，不需要在下游任务微调。

### **模型变化**

在模型结构方面，整个 GPT-2 的模型框架与 GPT-1 相同，只是做了几个地方的调整，这些调整更多的是被当作训练时的 trick，而不作为 GPT-2 的创新，具体为以下几点：

1. 后置层归一化（ post-norm ）改为**前置层归一化（ pre-norm ）**;

2. 在模型最后一个自注意力层之后，额外增加一个层归一化;

3. **调整参数的初始化方式，按残差层个数进行缩放，缩放比例为 1 : n开方 ;**

4. 输入序列的最大长度从 512 扩充到 1024;

5. 使用的标记器是字节级BPE (50K词汇表)

随着模型层数不断增加，梯度消失和梯度爆炸的风险越来越大，这些调整能够**减少预训练过程中各层之间的方差变化，使梯度更加稳定**。

### **Zero-Shot方法**

在 GPT-1 中，模型预训练完成之后会在下游任务上微调，在构造不同任务的对应输入时，我们会引入开始符（Start）、分隔符（Delim）、结束符（Extract）。虽然模型在预训练阶段从未见过这些特殊符号，但是毕竟有微调阶段的参数调整，模型会学着慢慢理解这些符号的意思。

现在，在 GPT-2 中，要做的是 zero-shot，也就是没有任何调整的过程了。这时我们在构造输入时就不能用那些在预训练时没有出现过的特殊符号了。所幸自然语言处理的灵活性很强，我们只要把想要模型做的任务 “告诉” 模型即可，如果有足够量预训练文本支撑，模型想必是能理解我们的要求的。

举个机器翻译的例子，要用 GPT-2 做 zero-shot 的机器翻译，只要将输入给模型的文本构造成 translate english to chinese, [englist text], [chinese text] 就好了。比如：translate english to chinese, [machine learning], [机器学习] 。这种做法就是日后鼎鼎大名的 prompt。下面还有其他任务的zero-shot形式：

问答：question answering prompt+文档+问题+答案: answer the question, document, question, answer。

文档总结：summarization prompt+文档+总结：summarize the document, document, summarization。

## GPT3：稀疏注意力、In-context learning

### **预训练:**

在模型结构上，GPT-3 延续使用 GPT 模型结构，但是引入了 [Sparse Transformer](https://zhida.zhihu.com/search?content_id=226019043&content_type=Article&match_order=1&q=Sparse+Transformer&zhida_source=entity) 中的 sparse attention 模块（**稀疏注意力**）。

sparse attention 与传统 self-attention（称为 dense attention） 的区别在于：

> dense attention：每个 token 之间两两计算 attention，复杂度 O(n²)。 sparse attention：每个 token 只与其他 token 的一个子集计算 attention，复杂度 O(n*logn)。
> 

具体来说，sparse attention 除了相对距离不超过 k 以及相对距离为 k，2k，3k，... 的 token，其他所有 token 的注意力都设为 0.

使用 sparse attention 的好处主要有以下两点：

1. **减少注意力层的计算复杂度**，节约显存和耗时，从而能够处理更长的输入序列；

2. **具有“局部紧密相关和远程稀疏相关”的特性**，对于距离较近的上下文关注更多，对于距离较远的上下文关注较少；

NLP语言中内容都是有上下文关系的，如此依赖必定会对长文本建模的效果变差。

### **下游任务：zero-shot/few-shot**

GPT-3是一种语言模型，它可以通过少量的样本进行学习，因此被称为“[Few-Shot Learner](https://zhida.zhihu.com/search?content_id=226019043&content_type=Article&match_order=1&q=Few-Shot+Learner&zhida_source=entity)”。和人类一样，GPT-3不需要完全不看任何样例就能学习，只需要看一小部分样例就能学会更多的知识。GPT-3的体量非常庞大，因此在下游任务中进行fine-tune的成本很高。为了解决这个问题，GPT-3使用了“[**In-Context Learning**](https://zhida.zhihu.com/search?content_id=226019043&content_type=Article&match_order=1&q=In-Context+Learning&zhida_source=entity)”的方式，在不进行梯度更新或fine-tune的情况下，直接在上下文中进行学习。

**Zero-shot**：仅使用当前任务的自然语言描述，不进行任何梯度更新； 
**One-shot**：当前任务的自然语言描述，加上一个简单的输入输出样例，不进行任何梯度更新； 
**Few-shot**：当前任务的自然语言描述，加上几个简单的输入输出样例，不进行任何梯度更新；

其中 Few-shot 也被称为 in-context learning，虽然它与 fine-tuning 一样都需要一些有监督标注数据，但是两者的区别是：

1. fine-tuning 基于标注数据对模型参数进行更新，而 in-context learning 使用标注数据时不做任何的梯度回传，模型参数不更新；
2. in-context learning 依赖的数据量（10～100）远远小于 fine-tuning 一般的数据量；

## **GPT3.5 (Instruct GPT)：RLHF（PPO）**

在做下游的任务时，我们发现GPT-3有很强大的能力，但是只要人类说的话不属于GPT-3的范式，他几乎无法理解。

ChatGPT 是基于 GPT3.5 的基础模型框架，核心变化在于**通过真实的调用数据以及人类反馈的强化学习进行训练**。ChatGPT3.5 主要用于**自然语言处理、机器翻译**等任务，而 ChatGPT3.5-Turbo 拥有更强大的强度，可用于更复杂的语言分析，比如**情感分析、语法结构分析**。所以，ChatGPT 和 GPT3.5 是同一系列的产品，但 ChatGPT 是在 GPT3.5 的基础上进行了改进和优化。

**InstructGPT采用基于人类反馈的强化学习（RLHF）来不断微调预训练语言模型（LLM），**旨在让模型能够更好地理解人类的命令和指令含义，如生成小作文、回答知识问题和进行头脑风暴等。该方法不仅让模型学会判断哪些答案是优质的，而且可以确保生成的答案富含信息、内容丰富、对用户有帮助、无害和不包含歧视信息等多种标准。因此，RLHF是一种有效的方法，可以帮助LLM不断提升性能和适应各种用户需求。

关于 InstructGPT 的技术方案，原文分为了三个步骤：有监督微调，奖励模型训练，强化学习训练；实际上可以把它拆分成两种技术方案，一个是有监督微调（SFT），一个是基于人类反馈的强化学习（RLHF），下面我们简单介绍下。

**Step1 监督策略模型 (SFT supervised fine-tuning)**

尽管GPT-3具有强大的语言处理能力，但它很难理解人类不同类型指令中蕴含的不同意图，并且很难判断生成内容是否是高质量的结果。为了解决这个问题，采取了以下步骤：首先，**从测试用户提交的 prompt 中随机抽取一批，然后请专业的标注人员为这些 prompt 给出高质量答案。接下来，我们使用这些<prompt,answer>数据来Fine-tune GPT-3模型，以使其初步具备理解人类prompt中所包含意图，并根据这个意图给出相对高质量回答的能力**。这一步骤中包含了1.2万条训练数据。虽然这个过程是有效的，但显然这还不足以解决所有问题。

**Step2 训练回报模型（Reward Model,RM）**

在这个阶段，论文中的研究者的主要目的是通过人工标注训练数据来训练回报模型。对用户提交的prompt进行随机抽样，并使用第一阶段Fine-tune好的冷启动模型，生成K个不同的回答，形成<prompt,answer1>,<prompt,answer2>….<prompt,answerK>数据。然后，标注人员根据相关性、信息性和有害信息等标准，**对K个结果进行排序，生成排序结果数据**。接下来，研究者使用这个排序结果数据进行**pair-wise learning to rank**训练模式，训练回报模型。RM模型接受一个输入<prompt,answer>，给出评价回答质量高低的回报分数Score。对于一对训练数据<answer1,answer2>，假设人工排序中answer1排在answer2前面，那么Loss函数则鼓励RM模型对<prompt,answer1>的打分要比<prompt,answer2>的打分要高。

**Step3-强化学习来增强预训练模型**

在这个阶段，研究者不需要人工标注数据，而是**利用之前学习好的RM模型，通过PPO强化学习来更新预训练模型参数**。从用户提交的命令中，随机选择一些新的命令，让PPO模型来生成回答。接着，用RM模型对这些回答进行评估，并给出一个分数作为回报。研究者的目标是训练LLM模型生成的答案能够获得高分数，也就是符合RM标准的高质量回答。最后，根据得到的回报分数来更新PPO模型的参数，以便让LLM模型生成更好的回答。

为什么使用PPO方法来更新GPT3呢？实际上是因为有限的Prompt导致的，我们不能够训练无限多的Prompt，类似于强化学习中无限的环境，所以只能够通过新旧模型预测的差别来进行学习速度上的提升。

## LLaMA系列：Pre norm-RMSnorm、SwiGLU、RoPE、Decoder-only、部分GQA

https://zhuanlan.zhihu.com/p/696571171

![image.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E5%8F%B2%201b8e64a5662180d09315ce75d073f189/image%208.png)

### llama1

（1）llama将layer-norm 改成[RMSNorm](https://zhida.zhihu.com/search?content_id=242923634&content_type=Article&match_order=1&q=RMSNorm&zhida_source=entity)(Root Mean square Layer Normalization)，并将其移到input层，而不是output层。

（2）采用SwiGLU激活函数。

（3）采用RoPE位置编码。

- **分词器：分词器采用BPE算法，使用 SentencePiece 实现**，将所有数字拆分为单独的数字，并**使用字节来分解未知的 UTF-8 字符。词表大小为 32k** 。
- **优化器：AdamW**,是Adam的改进，可以有效地处理权重衰减，提供训练稳定性。
- **learning rate：**使用余弦学习率调整 cosine learning rate schedule，使得最终学习率等于最大学习率的10%，设置0.1的权重衰减和1.0的梯度裁剪。warmup的step为2000，并根据模型的大小改变学习率和批处理大小。
- **模型效果：**llama-13B(gpt-3 1/10大小)在多数benchmarks上超越gpt-3(175B)。在规模较大的端，65B参数模型也与最好的大型模型（如Chinchilla或PaLM-540B）也具有竞争力。

### llama2

**Llama2相比Llama1的升级：**

（1）LLama2训练数据相比LLaMA多出40%，上下文长度是由之前的2048升级到4096，模型理解能力得以提升可以生成更长的文本。

（2）模型训练数据集使用的相比上一代的训练数据增加了 40%，并且更加注重安全&隐私问题。

（3）发布了llama-2-chat。（在公开数据集上**预训练以后引入SFT（有监督微调）、RLHF（人类反馈强化学习）+拒绝采样+近端策略优化 (PPO)两个优化算法**）。

*由于Llama2的性能比较优秀，Meta在2023年8月发布了专注于代码生成的[Code-Llama](https://zhida.zhihu.com/search?content_id=242923634&content_type=Article&match_order=1&q=Code-Llama&zhida_source=entity),包含 7B、13B 、 34B、70B 四个版本。*

**Meta试图证明小模型在足够多的的数据上训练后，效果也能达到甚至超过大模型。**

### llama3

- **模型结构：**Llama 3 中选择了相对标准的纯解码器decoder-only transformer架构，总体上与 Llama 2 相比没有重大变化。在 Llama 2 中只有34B,70B使用了分组查询注意 (GQA)，但为了提高**模型的推理效率，Llama 3所有模型都采用了GQA**。
- **分词器：**与Llama 2不同的是，**Llama 3将tokenizer由sentencepiece换成tiktoken,词汇量从 的32K增加到 128K，增加了 4 倍**。**更大的词汇库**能够更高效地编码文本，增加编码效率，可以实现更好的下游性能。不过这也会导致嵌入层的输入和输出矩阵尺寸增大，模型参数量也会增大。
- **序列长度：输入上下文长度从 4096（Llama 2）和 2048（Llama 1）增加到 8192**。但相对于GPT-4 的 128K来说还是相当小。
- **缩放定律：**对于像 8B 参数这样“小”的模型来说，扩展法则 Chinchilla 最优训练计算量对应于 ~200B Tokens，但是Meta使用到了 15T Tokens。从目前模型效果来看，Meta使用的Scaling Law法则是非常有效的，Meta得到了一个非常强大的模型，它非常小，易于使用和推理,而且mate表示，即使这样，该模型似乎也没有在标准意义上“收敛”,性能还能改善。这就意味着，一直以来我们使用的 LLM 训练是不足的，远远没有达到使模型收敛的那个点。**较大的模型在训练计算较少的情况下可以与较小模型的性能相匹配，但考虑到推理过程中使效率更高，还是会选择小模型。**如此说来训练和发布更多经过长期训练的甚至更小的模型，会不会是以后大模型发展的一个方向？
- **系统：**为了训练最大的 Llama 3 模型，Meta结合了三种类型的并行化：数据并行化、模型并行化和管道并行化。最高效的实现是在 16K GPU 上同时训练时，每个 GPU 的计算利用率超过 400 TFLOPS。在两个定制的 24K GPU 集群上进行了训练。
- **指令微调**：为了在聊天用例中充分释放预训练模型的潜力，Meta对指令调整方法进行了创新。**训练方法结合了监督微调 （SFT）、拒绝采样、近端策略优化 （PPO） 和直接策略优化 （DPO） 的组合。**这种组合训练，提高了模型在复杂推理任务中的表现。
- **模型效果：**LLaMA 3有基础版，和 instruct两个版本。每个版本拥有 8B 和 70B 两种参数规模的模型，它们在多项行业基准测试中展示了最先进的性能，而且 instruct效果相当炸裂。

### llama3.1

- **多语言支持：**支持8种语言翻译等能力，扩展了上下文长度至128K。
- **工具使用：支持调用外部工具、代码生成**等功能。
- **量化技术：**将模型从16位量化到8位，降低推理计算需求，使405B模型能够在单节点服务器上运行。
- **指令和聊天微调：**通过多轮对齐和合成数据生成，提升模型对用户指令的响应能力、质量和详细程度。

### **llama3.2**

Llama 3.2系列涵盖了小型和中型**视觉LLM**（参数分别为11B和90B）以及适用于边缘和端侧的轻量级纯文本模型（1B和3B），包含了预训练和指令调优版本。

视觉LLM的训练过程始于Llama 3.1文本模型的预训练，并经历多个阶段：

1. 首先集成图像适配器和编码器，并在大规模的噪声图像-文本对数据集上进行预训练。
2. 然后在中等规模的、高质量且知识增强的图像-文本对数据集上进行进一步训练。
3. 在Llama 3.2模型的后期训练阶段，采用了与文本模型相似的策略，进行了多轮的监督微调、拒绝采样和直接偏好优化以实现模型对齐。
4. 在训练过程中，利用Llama 3.1模型生成合成数据，这些数据基于领域内图像，通过过滤和丰富问题与答案来提升质量。此外，采用奖励模型对所有的候选答案进行排序，以确保微调数据的高质量。

1B和3B的轻量级模型具备出色的多语言文本生成和工具调用能力，便于在端侧设备如手机或PC上部署，确保了数据的隐私性，因为所有数据都保持在设备上。

对于1B和3B模型，采用了**剪枝和知识蒸馏**两种技术，以实现设备的高效适配和高性能：

4.1 剪枝技术用于缩减现有模型的大小，同时在尽可能保留知识和性能的基础上，对Llama模型进行精简。

4.2 1B和3B模型采用了从Llama 3.1 8B模型中提取的**结构化剪枝**方法。该方法涉及有系统地移除网络中的某些部分，并调整权重和梯度的大小，以构建一个更小、更有效的模型，同时保持原始模型的性能。知识蒸馏则通过将较大网络的知识传递给较小网络，使得小模型能够通过教师模型获得比从头训练更好的性能。**在剪枝之后，知识蒸馏被用来恢复模型的性能。**

## Qwen系列

https://blog.csdn.net/AIGCmagic/article/details/139692335

![image.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E5%8F%B2%201b8e64a5662180d09315ce75d073f189/image%209.png)

### **Qwen 1.0：NTP、RoPE、SFT+PPO**

在预训练方面，Qwen-1与先前的语言模型如GPT-3和Llama相似，都是基于Transformer架构，并通过预测文本序列中的下一个词来进行预训练。为了保持模型的简洁和稳定性，Qwen-1并未引入额外的预训练任务，而是**专注于增加模型规模和扩展数据集**。

后训练过程中，SFT（有监督微调）和RLHF（强化学习人类反馈）这两种技术通常被统称为“对齐”。目前普遍认为，通过相对较少的微调数据就可以显著改善聊天模型的性能。Qwen-1在这一过程中特别**注重提高SFT数据的多样性和复杂性**，例如使用instag和tulu 2等数据集，并通过人工审核和自动评估来严格控制数据质量。

在得到一个经过良好微调的SFT模型后，Qwen-1进一步研究了RLHF技术的应用效果。这主要采用了基于PPO（近端策略优化）的方法。然而，RLHF的训练过程是具有挑战性的，不仅因为PPO训练的不稳定性，还因为奖励模型的质量对训练结果至关重要。因此，Qwen-1**在构建稳定可靠的奖励模型上投入了大量工作，包括在大规模偏好数据上进行预训练，以及在细致标注的高质量偏好数据上进行微调**。通过RLHF训练后，Qwen-1发现模型在创造性和遵循指令方面有了显著提升，生成的回复也更受人类评注者的喜爱。

### **Qwen-Chat**

Qwen-Chat 变体：通过监督微调和人类反馈强化学习（RLHF）对齐的 Qwen1变体

### Qwen1.5：DPO、多语言、多种量化模型、部分GQA

在Qwen-1.5的更新中，除了继续提供**Int4和Int8的GPTQ量化模型**，还增加了**AWQ和GGUF量化模型**。为了提高开发者的使用体验，通义千问将Qwen-1.5的代码正式整合进了HuggingFace transformers代码库。这意味着开发者现在可以使用transformers版本4.37.0及以上的原生代码，而无需设置`trust_remote_code`选项即可进行开发工作。

Qwen-1.5已经与多个框架进行了整合，包括vLLM、SGLang（用于部署）、AutoAWQ、AutoGPTQ（用于量化）、Axolotl、LLaMA-Factory（用于微调）以及llama.cpp（用于本地LLM推理），这些框架现在都支持Qwen-1.5。Qwen-1.5系列模型可以在Ollama和LMStudio等平台上使用。此外，API服务不仅通过DashScope提供，还通过together.ai提供，实现了全球范围内的访问能力。
Qwen-1.5模型**重点提升了Chat模型与人类偏好的对齐程度**，并且显著增强了模型的**多语言处理**能力。

### **人类偏好对齐**

**对齐的主要目标是提升模型对指令的遵循能力，以及生成与人类偏好更为一致的回复**。Qwen-1.5模型在最新的系列中认识到将人类偏好整合到学习过程中的重要性，因此在对齐过程中有效地采用了**直接策略优化（DPO）和近端策略优化（PPO）**等技术。

然而，评估这类聊天模型的质量是一个重大挑战。虽然全面的人工评估是理想的，但它面临着可扩展性和可重复性的难题。因此，Qwen-1.5**利用更先进的大型模型作为评委。**

![image.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E5%8F%B2%201b8e64a5662180d09315ce75d073f189/image%2010.png)

### Qwen2：GQA、拒绝采样、反馈模型训练和在线DPO

- 推出了五种不同尺寸的预训练和指令微调模型，分别是Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B和Qwen2-72B。
- 训练数据集得到了扩展，不仅包括中文和英语，还新增了27种语言的高质量数据。
- 在多个评测基准上，Qwen系列模型展现出了领先的性能。
- 模型在处理代码和数学相关任务的能力有了显著提升。
- 上下文长度的支持得到了增强，其中Qwen2-72B-Instruct模型能够处理高达128K tokens的上下文。

在Qwen-1.5系列中，只有32B和110B的模型采用了GQA技术。在Qwen-2系列中，为了使所有规模的模型都能享受到GQA带来的推理速度提升和显存占用降低的好处，所有的模型都集成了GQA。

在上下文长度方面，所有预训练模型都在32K tokens的数据集上进行训练，并且在128K tokens的PPL评测中仍然展现出良好的性能。然而，对于指令微调模型来说，除了PPL评测，还需要进行如大海捞针这类长序列理解实验。在表格中，根据大海捞针的实际测试结果，列出了各个指令微调模型支持的最大上下文长度。当使用YARN等优化方法时，Qwen2-7B-Instruct和Qwen2-72B-Instruct都能够支持128K tokens的上下文长度。

此外，Qwen-2系列模型特别针对多语言场景中常见的语言转换（code switch）问题进行了优化。在处理多语言输入时，模型发生语言转换的概率有了显著降低。通过使用易于触发语言转换现象的提示词进行测试，可以观察到Qwen-2系列模型在这方面能力的明显提升。这表明Qwen-2在处理多语言文本时，能够更有效地维持语言的稳定性，减少不必要的语言切换，从而提高理解和生成多语言内容的准确性。

在完成了大规模预训练之后，Qwen-2模型通过精细的微调进一步提升了其智能水平，使其表现更加贴近人类。这一过程特别注重于提升模型在**代码处理、数学问题**解决、逻辑推理、指令遵循和多语言理解等方面的能力。此外，模型还学会了**更好地对齐人类价值观**，从而变得更加有用、诚实和可靠。

Qwen-2的微调过程遵循了一个关键原则，即在尽可能扩大训练规模的同时，减少对人工标注的依赖。Qwen-2**探索了多种自动化的方法来获取高质量、可靠且具有创造性的指令和偏好数据**。这些方法包括针对数学问题的**拒绝采样**、针对代码和指令遵循的**代码执行反馈**、针对创意写作的**回译技术**、以及针对角色扮演的**规模化监督**等。在训练方面，Qwen-2结合了有监督微调、反馈模型训练和**在线DPO**（数据集偏好优化）等多种策略。Qwen-2还采用了**在线模型合并**技术来减少对齐过程中的性能损失。这些技术和策略的综合应用显著提升了模型的基础能力和智能水平。
Qwen-2系列中的所有Instruct模型都经过了在32k上下文长度上的训练，并且通过使用**YARN**或Dual Chunk Attention等技术，这些模型能够处理更长的上下文长度。

### Qwen2.5：MoE(Max)

Qwen2.5-VL 在数字环境中充当**视觉**代理

Qwen 团队同期开发的 Qwen2.5-Max 是一个大规模混合专家（**MoE**）模型，在超过 20 万亿个 token 上训练，并通过监督微调（SFT）和人类反馈强化学习（RLHF）进一步完善

### QwQ：reasoning

QwQ-32B 推理模型：2024 年 11 月首次亮相的 QwQ-32B 作为**增强逻辑推理**的实验预览模型，2025 年 3 月初开始发挥重要作用。**得益于对强化学习（RL）的有效扩展**，仅拥有 320 亿参数的 QwQ-32B 性能可比肩规模大得多（671B 参数，37B 活动参数）的 DeepSeek-R1，且优于较小的 o1-mini，为具备强大推理能力的 AI 代理开辟了新的可能性

### Qwen-Agent：Agent

https://blog.csdn.net/shangguanliubei/article/details/145931328

Qwen-Agent 框架旨在为使用 Qwen 模型开发应用程序提供支持，使模型能够在实际环境中充当**智能代理**。它依托 Qwen 模型在**指令遵循、工具集成、多步骤规划和长期记忆处理**等方面的优势，采用**模块化设计**，**将具备内置函数调用支持的 LLM 与外部工具组合成更高级的代理系统**，为构建复杂的人工智能应用提供了灵活且强大的基础架构。

### **关键特性**

- 工具集成与功能调用：该框架极大地简化了定义 Qwen 模型可调用工具（如函数、API）的过程，**采用类似 OpenAI 函数调用规范的 JSON 类语法，使模型能够输出调用并接收工具执行结果**。Qwen-Agent 配备了现成的工具插件，涵盖网页浏览、代码执行、数据库查询等多个领域，允许 Qwen 模型在需要时调用计算器等工具或获取网页内容，极大地拓展了模型的功能边界。
- 计划与记忆：**代理框架为模型配备了工作记忆和规划器，使其能够处理多步骤任务**。Qwen-Agent 能够让模型内部规划一系列操作，而无需用户逐一提示每个步骤。例如，在处理复杂查询时，模型可自行规划搜索网络、汇总结果、起草答案等环节。同时，Qwen-Agent 可保留过去步骤的记忆，使模型能够记住工具返回的结果，并将其反馈到下一步的提示中，实现连贯且高效的任务执行。

### 应用实例

- **代码解释器集成**：Qwen-Agent 内置的代码解释器使模型能够执行 Python 代码，完成数据分析、计算和可视化等任务。用户可上传文件或提供数据，Qwen 将自动编写和运行 Python 代码进行分析或生成图表，为用户提供强大的数据处理能力，类似于 OpenAI 的代码解释器功能，但目前该功能并非沙盒化，代码直接在主机环境中运行。
- **浏览器助手（BrowserQwen Chrome 扩展程序）**：作为 Chrome 扩展程序的 BrowserQwen，**利用 Qwen 模型浏览用户浏览器中的网页和文档，使用实时信息回答查询。**它能够讨论或回答有关当前网页 / PDF 的问题，并保留访问页面的历史记录，汇总多页面内容以协助完成写作任务。此外，BrowserQwen 还支持插件集成，例如借助代码解释器工具直接从浏览器解决数学问题并创建数据可视化。
- **通过检索处理超长文档：**Qwen-Agent 创新性地采用检索辅助方法，将标准的 8k 上下文聊天模型拓展至处理 1M 令牌文档。其具体实现分为三个层次：
    
    ![image.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E5%8F%B2%201b8e64a5662180d09315ce75d073f189/image%2011.png)
    
    - **第一级：检索增强生成（RAG）**：将长文档分割为较小块（如 512 个标记），**利用基于关键字的搜索**查找最相关部分，采用传统的 **BM25** 检索提高效率。
    - BM25
        
        首先，简单概括 BM25 究竟作何用途。BM25 算法实质上是一个用于信息检索中，对给定查询（query）和若干 “相关” 文档（document）进行相关性排序打分的排序函数。严格来讲，这不是一个打分函数，而是一个家族的一系列评分函数，因为它的提出并非一蹴而就的事情，它的发明经过了若干试验迭代演进。一般情况下，这个相关性打分是一个类似 TF-IDF 的基于统计计数的无监督学习过程。
        
        BM25 算法其主要思想可简述如下：对 query 进行特征提取分解，生成若干特征项（词）qi；然后对于每个搜索结果 D，计算每个特征qi与 D 的相关性得分，最后，将 qi相对于 D 的相关性得分进行加权求和，从而得到query与D的相关性得分。
        
        ![image.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E5%8F%B2%201b8e64a5662180d09315ce75d073f189/image%2012.png)
        
    - **第二级：逐块阅读**：对每个块分别进行扫描，若某块相关则提取关键句子并优化搜索，避免遗漏重要细节。
    - **第三级：逐步推理**：将复杂查询分解为更小的子问题，逐步回答。例如，回答“哪种车辆是在贝多芬第五交响曲的同一世纪发明的？”时，系统先确定交响曲创作于 19 世纪，再搜索该时期发明的车辆。

## Baichuan系列

### **类似bert的文本模型**

第一部分是基于Transformer编码器的模型，用于向量化、分类、序列标记、QA(问答)、NER([命名实体识别](https://zhida.zhihu.com/search?content_id=227670447&content_type=Article&match_order=1&q=%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB&zhida_source=entity))等。

**1、BERT Google / 2018**

Transformer 编码器，wordpiece tokenization（30K 词汇量）。 输入嵌入由三个向量组成：标记向量、可训练位置向量和片段向量（第一个文本或第二个文本）。 **模型输入是 CLS 标记嵌入、第一个文本的嵌入和第二个文本的嵌入。**

BERT 有两个训练任务：Masked Language Modeling (MLM) 和 Next Sentence Prediction (NSP)。 在 MLM 中，15% 的令牌被屏蔽，80% 被 MASK 令牌替换，10% 被随机令牌替换，10% 保持不变。 模型会预测正确的 令牌，而损失仅在这 15% 的屏蔽令牌上计算。 在 NSP 中，模型预测第二个文本是否跟在第一个文本之后。 预测是在 CLS 令牌的输出向量上进行的。

为了加快训练速度，首先90%的训练在序列长度为 128 个标记上进行，然后剩下的10% 的时间在 512 个标记上训练模型以获得更有效的位置嵌入。

**2、RoBERTa Facebook / 2019**

BERT的改进版本，它只在MLM上训练(因为NSP被认为不太有用)，训练序列更长(512个令牌)。使用动态屏蔽(当再次处理相同的数据时，不同的令牌被屏蔽)，训练超参数是精心选择的。

### **类似GPT 和T5的模型**

基于完整Transformers的模型。 它的应用范围非常广泛：除了上一节的任务外，它还包括会话代理、机器翻译、逻辑和数学推理、代码分析和生成，以及基本上文本生成。 **最大和“最智能”的模型通常基于解码器架构**。 此类模型**通常在 few-shot 和 zero-shot 模式下无需微调即可表现良好**。

**1、GPT-2 OpenAI / 2018**

解码器在因果LM的任务上进行训练(根据左侧上下文预测下一个令牌)。从体系结构的角度来看，有一些小的变化：**从每个解码器块中移除交叉注意层，并使用了LayerNorm，使用的tokenizer是字节级BPE** (50K词汇表)，没有使用类似的子字符串例如（“dog”、“dog!”、“dog.”）。 最大序列长度为 1024。**层输出缓存所有先前生成的标记。**

**2、T5 Google / 2019**

**在MLM上进行完整的预训练**(15%的令牌被屏蔽)，跨度由代码掩码(， ，…)屏蔽。输出预测序列<span><\span>…**LayerNorm在自注意力层和全连接层输入之前应用pre Norm**。使用相对位置编码:位置由可学习的嵌入编码，其中每个“嵌入”只是在计算注意力权重时添加相应logit的，标量矩阵B是跨层共享的，但对于不同的自注意力注头是不同的。每一层考虑令牌之间的128个距离，其余的归零，这样可以对比训练期间看到的序列更长的序列进行推理。标记化使用sentencepece (32K词汇表)完成，在预训练期间最大序列长度为512。

## BERT和GPT相比有什么优缺点？分别更适合做什么任务？【面试真题】
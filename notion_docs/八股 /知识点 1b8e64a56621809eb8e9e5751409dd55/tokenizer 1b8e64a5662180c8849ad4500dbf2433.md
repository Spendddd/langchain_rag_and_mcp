# tokenizer

State: 已完成

https://zhuanlan.zhihu.com/p/770595538

Tokenizer是一个用于向量化文本，将文本转换为序列的类。计算机在处理语言文字时，是无法理解文字含义的，通常会把一个词（中文单个字或者词）转化为一个正整数，将一个文本就变成了一个序列，然后再对序列进行向量化，向量化后的数据送入模型处理。

![image.png](tokenizer%201b8e64a5662180c8849ad4500dbf2433/image.png)

Tokenizer 允许使用两种方法向量化一个文本语料库： 将每个文本转化为一个整数序列（每个整数都是词典中标记的索引）； 或者将其转化为一个向量，其中每个标记的系数可以是二进制值、词频、TF-IDF权重等。

https://zhuanlan.zhihu.com/p/770595538**分词器（Tokenizer）详解**

由于神经网络模型不能直接处理文本，因此我们需要先用分词器将文本转换为数字，这个过程被称为**编码 (Encoding)**，包含两个步骤：

1. 使用分词器 (tokenizer) 将文本按词、子词、字符切分为 tokens；
2. 将所有的 token 映射到对应的 token ID。

## **2 三种分词粒度**

通常情况下，Tokenizer有三种粒度：word(词)/char(字符)/subword(子词)

## **2.3 subword**

subword：按照词的subword进行分词。

如：Today is sunday. 则会分割成[to， day，is ， s，un，day， .]

为了平衡以上两种方法， 又提出了基于 subword 进行分词：它可以较好的平衡词表大小与语义表达能力；常见的子词算法有Byte-Pair Encoding (BPE) / Byte-level BPE（BBPE）、[Unigram LM](https://zhida.zhihu.com/search?content_id=248780166&content_type=Article&match_order=1&q=Unigram+LM&zhida_source=entity)、WordPiece、SentencePiece等。

![](https://pic3.zhimg.com/v2-bb610f68927f187626cb3f3578399566_1440w.jpg)

### **2.3.1 Byte-Pair Encoding (BPE)**

Byte-Pair Encoding (BPE)，字节对编码，来自于[****Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015)**](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1508.07909)。**其核心思想是**从字母开始，不断找词频最高、且连续的两个token合并，直到达到目标词数**。

BPE算法执行过程：

1. BPE**将训练数据分割成单词。 预分词可以是简单的空格分词，**像[**GPT-2**](https://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.36.1/zh/model_doc/gpt2)，[***RoBERTa**](https://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.36.1/zh/model_doc/roberta)。**更加先进的预分词方式包括了基于规则的分词，像[**XLM**](https://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.36.1/zh/model_doc/xlm)，[**FlauBERT**](https://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.36.1/zh/model_doc/flaubert)，FlauBERT在大多数语言使用了Moses，或者[**GPT**](https://link.zhihu.com/?target=https%3A//huggingface.co/docs/transformers/v4.36.1/zh/model_doc/gpt)，GPT 使用了Spacy和ftfy，统计了训练语料库中每个单词的频次。
2. 生成了单词的集合vocab，也确定了训练数据中每个单词出现的频次。
3. 根据单词集合vocab产生了一个基础词典，包含了集合中所有的符号（下图tokens）。
4. BPE学习融合的规则-组合基础词典中的两个符号来形成一个新的符号（频率最高的pair），即寻找出现频率最高的字符对，并将新的字符对合并到vocab中。
5. 循环3和4，直到词典的大小满足了期望的词典大小的要求。

注意到期望的词典大小是一个超参数，在训练这个分词器以前就需要人为指定。

![image.png](tokenizer%201b8e64a5662180c8849ad4500dbf2433/image%201.png)

### **2.3.2 Byte-level BPE (BBPE)**

2019年12月，论文：[**Neural Machine Translation with Byte-Level Subwords**](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1909.03341) 在**基于BPE基础上提出以Byte-level为粒度的分词算法Byte-level BPE**，即BBPE，主要用于**将文本数据压缩或编码成更紧凑的表示形式。**BBPE核心思想**将BPE的从字符级别扩展到字节（Byte）级别**。它的原理如下：

1. 初始化词汇表：开始时，**BBPE 将每个字符都视为一个词汇**。
2. 字节对频率统计：对输入文本进行扫描，**统计所有相邻字节对的出现频率。**
3. **合并频率最高的字节对**：找到出现频率最高的字节对，然后将它们合并成一个新的字节对，并将这个新的字节对添加到词汇表中。
4. 重复步骤 2 和步骤 3：**不断重复步骤 2 和步骤 3，直到达到指定的词汇表大小或者达到预设的迭代次数。**
5. **编码文本：最终的词汇表包含了所有频率较高的字节对，然后将输入文本中的字节对替换为词汇表中对应的新词汇**，从而完成编码。

BPE的一个问题是如果遇到了unicode编码，基本字符集可能会很大。**BBPE就是以一个字节为一种“字符”，不管实际字符集用了几个字节来表示一个字符。这样的话，基础字符集的大小就锁定在了256（2^8）。**采用BBPE的好处是可以**跨语言共用词表**，显著压缩词表的大小。而坏处就是，对于类似中文这样的语言，一段文字的序列长度会显著增长。因此，BBPE based模型可能比BPE based模型表现的更好。然而，**BBPE sequence比起BPE来说略长，这也导致了更长的训练/推理时间。**BBPE其实与BPE在实现上并无大的不同，只不过基础词表使用256的字节集。

### **2.3.3 WordPiece**

WordPiece算法也可以看作是BPE的变种。

像 BPE 一样,WordPiece 学习合并规则。主要区别在于选择要合并的对的方式。**WordPiece 不是选择最频繁的字符对，而是使用以下公式计算每对的分数:**

**score = (freq_of_pair)/(freq_of_first_element * freq_of_second_elements)**

通过将配对的频率除以其每个部分的频率的乘积, **该算法优先合并单个部分在词汇表中频率较低的对。**例如,它不一定会合并 `("un", "##able")` 即使这对在词汇表中出现的频率很高,因 `"un"` 和 `"##able"` 很可能每个词都出现在很多其他词中并且出现频率很高。相比之下,像 `("hu", "##gging")` 可能会更快地合并 (假设 “hugging” 经常出现在词汇表中),因为 `"hu"` 和 `"##gging"` 这两个词**单独出现地频率可能较低**。

### **2.3.4 Unigram**

和 BPE 以及 WordPiece 从表面上看一个大的不同是，前两者都是**初始化一个小词表，然后一个个增加到限定的词汇量**，而 **Unigram Language Model 却是先初始一个大词表，接着通过语言模型评估不断减少词表，直到限定词汇量。**

### **2.3.5 SentencePiece**

SentencePiece它是谷歌推出的子词开源工具包，它是**把一个句子看作一个整体，再拆成片段，而没有保留天然的词语的概念。**一般地，它**把空格也当作一种特殊字符来处理**，**再用BPE或者Unigram算法来构造词汇表。**SentencePiece除了集成了BPE、ULM子词算法之外，SentencePiece还能支持字符和词级别的分词。

## **2.4 典型大模型分词器类型**

| **分词方法** | **典型模型** |
| --- | --- |
| **BPE** | GPT, GPT-2, GPT-J, GPT-Neo, RoBERTa, BART, LLaMA, ChatGLM-6B, Baichuan |
| WordPiece | BERT, DistilBERT，MobileBERT |
| Unigram | AlBERT, T5, mBART, XLNet |
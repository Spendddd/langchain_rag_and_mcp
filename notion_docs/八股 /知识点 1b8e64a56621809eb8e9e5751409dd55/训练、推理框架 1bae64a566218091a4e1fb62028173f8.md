# 训练、推理框架

State: 学习中

https://blog.csdn.net/weixin_43440181/article/details/145600189

https://blog.csdn.net/sunyuhua_keyboard/article/details/145461105

## **01 大[模型训练](https://so.csdn.net/so/search?q=%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83&spm=1001.2101.3001.7020)总体架构**

分为三个方面来回顾现阶段使用多AI加速芯片训练大模型的主流方法。

1、分布式并行加速： 并行训练主要分为数据并行、模型并行、流水线并行、张量并行四种并行方式，通过上述四种主要的分布式并行策略来作为大模型训练并行的主要策略。
2、算法模型架构： 大模型训练离不开Transformer网络模型结构的提出，后来到了万亿级稀疏场景中经常遇到专家混合模型MoE都是大模型离不开的新算法模型结构。
3、内存和计算优化： 关于内存优化技术主要由激活Activation重计算、内存高效的优化器、模型压缩，而计算优化则集中体现在**混合精度训练、算子融合、梯度累加**等技术上。

## **02 大模型训练的目标公式**

超大模型训练的总体目标就是**提升总的训练速度，减少大模型的训练时间**，你知道啦，毕竟训练一个大模型基本上从按下回车的那一刻开始要1到2个月，是很蛋疼的。下面主要看一下在大模型训练中的总训练速度的公式：

![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image.png)

上面公式当中，单卡速度主要由单块AI加速芯片的运算速度、数据IO来决定；而加速芯片数量这个很清楚，数量越多增加训练速度；而多卡加速比则是有计算和通讯效率决定。

1. 单卡速度： 单卡速度既然是运算速度和数据IO的快慢来决定，那么就需要对单卡训练进行优化，于是主要的技术手段有精度训练、算子融合、梯度累加来加快单卡的训练性能。
2. 加速芯片数量： **理论上，AI芯片数量越多，模型训练越快。**但是，随着训练数据集规模的进一步增长，加速比的增长并不明显。如数据并行就会出现局限性，当训练资源扩大到一定规模时，**由于通信瓶颈的存在，增加计算资源的边际效应并明显，甚至增加资源也没办法进行加速。**这时候需要**对通讯拓扑进行优化**，例如通过**ring-all-reduce**的通讯方式来优化训练模式。
3. 多卡加速比： 多卡加速比既然由计算、通讯效率决定，那么就需要结合算法和集群中的网络拓扑一起优化，于是有了**数据并行DP、模型并行MP、流水线并行PP**相互结合的多维度混合并行策略，来增加多卡训练的效率。

## 03 大模型训练的集群架构

这里的集群架构是为了机器学习模型的分布式训练问题。深度学习的大模型目前主要是在集群中才能训练出来啦，而集群的架构也需要根据分布式并行、深度学习、大模型训练的技术来进行合理安排。

在2012年左右**Spark采取了简单直观的数据并行的方法解决模型并行训练的问题**，但由于Spark的并行梯度下降方法是同步阻断式的，且**模型参数需通过全局广播的形式发送到各节点，因此Spark的并行梯度下降是相对低效的**。

2014年李沐提出了分布式可扩展的**Parameter Server**架构，很好地解决了机器学习模型的分布式训练问题。Parameter Server不仅被直接应用在各大公司的机器学习平台上，而且也被集成在**TensorFlow，Pytroch、MindSpore、PaddlePaddle**等主流的深度框架中，作为机器学习分布式训练最重要的解决方案之一。

目前最流行的模式有两种：

- **参数服务器模式（Parameter Server，PS）**
- **集合通讯模式（Collective Communication，CC）**

其中**参数服务器主要是有一个或者多个中心节点，这些节点称为PS节点，用于聚合参数和管理模型参数。**
而**集合通信则没有管理模型参数的中心节点，每个节点都是 Worker**，每个Worker负责模型训练的同时，还需要掌握当前最新的全局梯度信息。

## 04 分布式并行策略相关

**数据并行（Data Parallel，DP）**： 数据并行训练加速比最高，但要求每个设备上都备份一份模型，显存占用比较高。

**模型并行（Model Parallel，MP)**： 模型并行，通信占比高，适合在机器内做模型并行，且支持的模型类型有限。

**流水线并行（Pipeline Parallel，PP)**： 流水线并行，训练设备容易出现空闲状态，加速效率没有数据并行高；但能减少通信边界支持更多的层数，适合在机器间使用。

**混合并行（Hybrid parallel，HP）**： 混合并行策略的思想，集三种策略的优势于一身，实现取长补短。具体来说，**先在单机内使用模型并行和分组参数切片组合的策略，**这么选择的原因是这两个策略通信量较大，适合使用机器内的卡间通信。接着，**为了承载千亿规模大模型，叠加流水线并行策略，使用多台机器共同分担计算。**最后，**为了计算和通讯高效，在外层又叠加了数据并行来增加并发数量，提升整体训练速度。**这就是我们目前在AI框架中添加的并行策略，业界基本上都是使用这种方式。

- 模型并行和流水线并行的区别
    
    *模型并行（Model Parallelism）**和**流水线并行（Pipeline Parallelism）**是两种常见的并行化策略，用于在分布式计算环境中训练大型深度学习模型。它们都旨在解决单个计算节点无法容纳整个模型或处理整个计算任务的问题，但它们在实现方式、数据流动和性能特点上有所不同。以下是这两种并行化方法的详细比较：
    
    ### 1. 模型并行（Model Parallelism）
    
    ### 定义：
    
    模型并行是指将模型的各个部分（如不同的层、子网络或参数）分配到不同的计算节点（workers或devices）上。每个节点负责处理模型的一部分，所有节点协同工作以完成整个模型的计算。
    
    ### 主要特点：
    
    - **模型分割**：将模型的不同部分分配到不同的节点上。例如，可以将一个深度神经网络的不同层分配到不同的GPU上。
    - **数据流动**：输入数据首先进入第一个节点，处理完后传递到下一个节点，依此类推，直到所有节点都处理完毕。
    - **同步操作**：在每个前向和反向传播步骤中，不同节点之间需要进行同步，以确保数据的一致性和梯度的正确传播。
    
    ### 优点：
    
    - **处理超大模型**：能够处理单个节点无法容纳的模型。
    - **资源利用**：可以充分利用不同节点的计算资源，如GPU的显存和计算能力。
    
    ### 缺点：
    
    - **通信开销大**：不同节点之间需要频繁进行数据交换，导致通信开销较大。
    - **计算效率低**：由于数据需要按顺序通过各个节点，计算效率可能受到限制。
    
    ### 应用场景：
    
    - **超大模型训练**：如GPT-3、Transformer等大型语言模型。
    - **复杂神经网络**：如深度卷积神经网络、循环神经网络等。
    
    ### 2. 流水线并行（Pipeline Parallelism）
    
    ### 定义：
    
    流水线并行是指将模型的计算过程分解为多个阶段（stages），每个阶段对应模型的一部分（如一层或多层），并将这些阶段分配到不同的计算节点上。数据以流水线的方式流经各个阶段，不同阶段可以并行处理不同的数据批次。
    
    ### 主要特点：
    
    - **阶段划分**：将模型划分为多个阶段，每个阶段对应模型的一部分。例如，可以将一个深度神经网络的前几层划分为一个阶段，后几层划分为另一个阶段。
    - **并行处理**：不同阶段可以并行处理不同的数据批次。例如，当第一个数据批次在第二个阶段处理时，第二个数据批次可以在第一个阶段处理。
    - **减少空闲时间**：通过流水线方式，减少了节点的空闲时间，提高了计算效率。
    
    ### 优点：
    
    - **提高计算效率**：不同阶段可以并行处理不同的数据批次，减少了节点的空闲时间。
    - **降低通信开销**：相比于模型并行，流水线并行减少了节点之间的通信频率，因为数据以流水线的方式流动。
    - **更好的资源利用**：充分利用了每个节点的计算资源，提高了整体性能。
    
    ### 缺点：
    
    - **延迟增加**：由于数据需要按顺序流经各个阶段，可能会引入一定的延迟。
    - **复杂性高**：实现和管理流水线并行比模型并行更复杂，需要更精细的调度和同步机制。
    
    ### 应用场景：
    
    - **大规模模型训练**：如BERT、GPT等需要大量计算资源的模型。
    - **分布式系统**：在多个计算节点之间分配计算任务，以提高整体性能。
    
    ### 3. 模型并行与流水线并行的比较
    
    | 特点 | 模型并行（Model Parallelism） | 流水线并行（Pipeline Parallelism） |
    | --- | --- | --- |
    | **模型分割** | 将模型的不同部分分配到不同节点 | 将模型划分为多个阶段，每个阶段对应模型的一部分 |
    | **数据流动** | 数据按顺序流经各个节点 | 数据以流水线方式流经各个阶段 |
    | **并行处理** | 不同节点可以并行处理不同的数据批次 | 不同阶段可以并行处理不同的数据批次 |
    | **通信开销** | 较大，需要频繁进行数据交换 | 较小，节点之间的通信频率较低 |
    | **计算效率** | 可能较低，受限于数据流动顺序 | 较高，减少了节点空闲时间 |
    | **实现复杂度** | 较低 | 较高，需要精细的调度和同步机制 |
    | **适用场景** | 超大模型、复杂神经网络 | 大规模模型训练、分布式系统 |
    
    ### 4. 结合使用
    
    在实际应用中，模型并行和流水线并行可以结合使用，以实现更高的并行化效率和更好的资源利用。例如：
    
    - **混合并行**：将模型的不同部分进行模型并行，同时在每个部分内部使用流水线并行。
    - **分层并行**：在不同的层级（如模型层级和数据层级）上分别应用不同的并行化策略。
    
    ### 5. 总结
    
    模型并行和流水线并行都是解决大规模模型训练中计算和内存瓶颈的有效方法。模型并行通过将模型的不同部分分配到不同节点上，实现了超大模型的训练；而流水线并行通过将模型划分为多个阶段并以流水线方式处理数据，提高了计算效率和资源利用率。在实际应用中，根据具体的模型架构和计算资源，选择合适的并行化策略或结合使用多种策略，可以显著提升训练效率和模型性能。
    

## 05 大模型算法相关

**ZeRO: Memory Optimization Towards Training A Trillion Parameter Models Samyam.**

微软提出很经典很经典的一个算法了，为了这个算法还基于Pytroch开发了一个分布式并行DeepSpeed框架。现有普遍的数据并行模式下的深度学习训练，每一台机器都需要消耗固定大小的全量内存，这部分内存和并不会随着数据的并行而减小，因而，数据并行模式下机器的内存通常会成为训练的瓶颈。这篇论文开发了一个Zero Redundancy Optimizer (ZeRO)，主要用于**解决数据并行状态下内存不足的问题，使得模型的内存可以平均分配到每个gpu上**，每个gpu上的内存消耗与数据并行度成反比，而又基本不影响通信效率。

**Mixed precision training：混合精度训练**

![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%201.png)

# 从并行类型看训练框架

## 数据并行：参数服务器（Parameter Server）

### 数据并行介绍

典型的数据并行实现（PyTorch的DDP和TF的tf.distribute.MirroredStrategy）来说，每个GPU通常存储的是完整的模型，包括权重和参数。然后将不同的数据批次分配给不同的GPU来进行处理，接着在所有GPU之间共享梯度信息，以更新模型的权重和参数。因为多个GPU可以同时处理不同的数据批次，而无需等待其他GPU完成它们的工作，所以这种方法可以提高训练速度。

### 数据并行训练过程

整个数据并行训练过程可以用以下步骤来总结：

1. 初始化：在每个GPU上创建一个模型的副本（replica），每个副本都有相同的初始权重和参数。
2. 训练迭代：每个GPU接收其分配的数据批次，并在本地（每个GPU本身）计算损失函数的梯度（local gradients）。
3. 全体度求和（All-Reduce）：为了保持模型权重的一致性并允许模型更新，需要将来自各个GPU的local gradients汇总为一个global gradients，这个操作称为all-reduce，其包含两个操作，reduce-scatter和all-gather：
    
    ![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%202.png)
    
    - Reduce-Scatter：
        - 每个节点（replica）的local gradients都被分成不同的块或分片（blocks or shards，通常是均匀分配的），然后在N个 节点之进行 N-1 轮数据交换，这使得每个节点获得了其他节点的一部分梯度信息。
        - 每一轮的交换都会将部分梯度数据合并，以获得一部分全局梯度信息。最终每个 replica 都会持有来自所有其他 replica 的梯度分片的汇总（fully reduced data）
        reduced是指每一轮的合并操作，使得shards减少。fully reduced就是指最终所有的梯度shards都被合并成一个值。
    - All-gather：
        - 每个 replica 将在 Reduce-Scatter 阶段中获得的 fully reduced data再次分成多个小块，然后将其 **广播** 其它节点。同样经过N-1轮数据交换，每个节点都获得了其他节点的全部梯度信息。
        - 每个节点根据收集到的全部梯度信息汇总为global gradients。
4. 基于`global gradients`进行模型参数更新（`weight update`）
5. 重复步骤2至4，直到训练完成。

### **参数服务器模式**

参数服务器架构Parameter Server，PS架构包括两个部分，首先是**把计算资源分为两个部分**，参数服务器节点和工作节点：

- **参数服务器节点用来存储参数**
- **工作节点部分用来做算法的训练**

第二个部分就是**把机器学习算法也分成两个方面，即：参数 和 训练**。

![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%203.png)

如图所示，PS架构将计算节点分为server与worker，其中，**worker用于执行网络模型的前向与反向计算。而server则对各个worker发回的梯度进行合并并更新模型参数**，对深度学习模型参数中心化管理的方式，非常易于存储超大规模模型参数。

### 参数服务器详解

https://zhuanlan.zhihu.com/p/668755022：**一文读懂ParameterServer**

![](https://pica.zhimg.com/v2-64f8e7a378a0bc14131ca20e3660a7fe_1440w.jpg)

概括来说，参数服务器是一个为了解决分布式机器学习问题的编程框架。该框架主要包括服务器端（Server ），客户端（Client）和调度器（Scheduler）。

- 服务器端的主要功能是**存放机器学习任务的参数**，接收客户端的梯度，对本地参数进行更新。
- 客户端的主要功能有两点：一是从服务器端获取当前最新的参数；二是，使用本地或者远程节点的数据和从服务器端获取的参数，**计算得到预测值**，然后**根据设定的损失函数，计算关于训练参数的梯度**，最后将梯度发送给服务器端。
- 调度器的主要功能是**管理服务器，客户端节点**，完成节点之间数据同步，节点添加/删除等功能。

图中使用了共享的参数服务器端。这是因为**如果仅使用单一的服务器端，多个客户端不断地从此服务器获取参数，推送梯度，会导致服务器端网络拥塞，部分数据丢失。**

此外整个系统还有存在一个或若干**控制节点**，用来管理服务器端和客户端。整个参数服务器最主要的操作就是图中的**Pull（从服务器获取参数）和Push（推送梯度到服务器）**。通过不断迭代执行Pull和Push操作，最小化损失函数，就是简易的参数服务器原型。

**PS优点：**相比于传统的模式，Parameter Server将模型的参数存储在中心服务器上，而不是分发给各个计算节点。该模式的优点在于，可以**简化计算节点的负担，节省内存和网络流量**。同时，参数服务器可以提供服务发现、容错、负载均衡等机制。

### Ring-All-Reduce

但是随着模型网络越来越复杂，对算力要求越来越高，在数据量不变的情况下，**单个GPU的计算时间是有差异**的，并且**网络带宽之间并不平衡**，会存在部分GPU计算得比较快，部分GPU计算得比较慢。这个时候**如果使用异步更新网络模型的参数，会导致优化器相关的参数更新出现错乱。而使用同步更新则会出现阻塞等待网络参数同步的问题。**

GPU 强大的算力毋庸置疑可以提升集群的计算性能，但随之而来的是，不仅模型规模会受到机器显存和内存的制约，而且通信带宽也会由于集群网卡数量降低而成为瓶颈。

这个时候百度基于PS架构之上提出了Ring-All-Reduce新的通讯架构方式。

![](https://i-blog.csdnimg.cn/blog_migrate/a315af10634bd4c988bf3dfcd7ccb511.jpeg)

如图所示，通过异步流水线执行机制，隐蔽了 IO 带来的额外性能开销，在保证训练速度的同时，使训练的模型大小不再受制于显存和内存，极大提升模型的规模。而 RPC&NCCL 混合通信策略可以将部分稀疏参数采用 RPC 协议跨节点通信，其余参数采用卡间 NCCL 方式完成通信，充分利用带宽资源。

- ring-all-reduce
    
    ## **Ring All_reduce：all_reduce的一种实现算法**
    
    [ring all_reduce](https://zhida.zhihu.com/search?content_id=243647919&content_type=Article&match_order=1&q=ring+all_reduce&zhida_source=entity)可以用下面这个模型来理解
    
    ![](https://picx.zhimg.com/v2-16f92ea8b3d7677ae61a1be715632375_1440w.jpg)
    
    GPUs arranged in a logical ring
    
    ring all_reduce算法包含2个步骤：
    
    - ring scatter-reduce
        - 将数据partition成`N` 份，其中`N` 是通信节点数
        - 进行`N-1` 次send / receive数据传输
    
    ![](https://pic1.zhimg.com/v2-48d6d40347b3f3f5ceea817be72a6872_1440w.jpg)
    
    第1次data transfer
    
    ![](https://pica.zhimg.com/v2-9ab4ce5b6a52107401be126a2265443c_1440w.jpg)
    
    第2次data transfer
    
    ![](https://pic2.zhimg.com/v2-badcdcee767625ab6dedfac2e8e0b6cb_1440w.jpg)
    
    第3次data transfer
    
    ![](https://picx.zhimg.com/v2-b53b9b79dddbbde615c63ed13fa9d961_1440w.jpg)
    
    第4次data transfer
    
    - all-gather
        - 经过scatter-reduce后，各个卡的状态如下图所示
    
    ![](https://pic3.zhimg.com/v2-4e4e5e34a642139afbf688b20c7cf4dc_1440w.jpg)
    
    scatter-reduce后的结果
    
    - 下面是`N-1` 次sends and receives操作来完成ring all_gather操作
    
    ![](https://pic3.zhimg.com/v2-47a7511c87b1cf12528ea442ab62d68c_1440w.jpg)
    
    第1次data transfer
    
    ![](https://pic2.zhimg.com/v2-9efb11ddfc4b16b8d2251b00b0606097_1440w.jpg)
    
    第2次data transfer
    
    ![](https://pic2.zhimg.com/v2-7dd1774d08507e4a401747b47829ac9f_1440w.jpg)
    
    第2次data transfer
    
    ![](https://pic4.zhimg.com/v2-e39a476a8c2d899bee69814aae9215c7_1440w.jpg)
    
    第4次data transfer
    
    通过上面这些图示，可以很清楚的理解ring all_reduce算法是如何实现all_reduce通信源语的。下面分析一下通信量：假设完整的数据是K bytes，传输1 byte的耗时为b。由于一共进行了`2(N-1)` 次data transfer，然后每一次data transfer，每个GPU都是send`K/N` 个bytes，同时也recive`K/N` bytes。所以完成一次ring all_reduce的耗时为`2(N-1)*K/N* b` = `2(1-1/N)Kb` ，可以看到，ring all_reduce的通信成本对节点数有着很好的扩展性。
    
    PS: 像llama这类大模型，如果根据[Megatron](https://zhida.zhihu.com/search?content_id=243647919&content_type=Article&match_order=1&q=Megatron&zhida_source=entity)的并行方案，每层layer都需要做2次all_reduce操作。
    
- PCIe
    
    **PCIe**（Peripheral Component Interconnect Express，简称PCI Express）是一种高速**串行**计算机扩展总线标准，用于连接计算机主板与各种外设和扩展卡。PCIe 旨在取代旧的PCI、PCI-X和AGP总线标准，提供更高的带宽、更快的传输速度和更好的可扩展性。以下是对PCIe的详细介绍：
    
    ### **1. PCIe 的基本概念**
    
    - **串行总线**：与传统的并行总线（如PCI）不同，PCIe 使用**串行通信**方式，通过**差分信号**传输数据。这种方式减少了信号干扰，提高了传输速度和可靠性。
    - **点对点连接**：每个PCIe设备通过独立的链路与根复合体（Root Complex，通常是CPU或芯片组）直接连接，而不是像PCI那样共享总线带宽。这种点对点连接方式提高了数据传输的效率和稳定性。

## **流水线并行：GPipe**

为了解决大模型无法在单张显卡上运行的问题，GPipe提出了模型并行的方法，这篇工作来自于google团队，并中稿于NeurIPS2019。一篇系统的文章中稿于机器学习三大顶会之一的NeurIPS实属罕见，也恰恰证明这篇工作在深度学习领域的影响力。
**GPipe的核心是模型并行，或者说是流水线并行。**现有的模型模型并行方法在结构上有限制或要求，或者只能针对特定的任务。然而模型并行是大势所趋，下面是现有的模型在CV和NLP任务上的表现：

可以看到模型的参数越多，模型的性能就越好，而模型太大现有的硬件难以支持，因此一个通用的模型并行的算法可以解燃眉之急。

![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%204.png)

上图是GPipe的解决方案，其核心思想可以分为三点：

- **模型按层进行分组**，每个组是连续的层序列。
- 提出microbatch的概念。
- re-materialization方法。

具体来说，（a）中将整个模型分成四个组，分别存储在四个不同的加速器中，这样虽然能够将模型运行起来，但是效率低下，如（b）所示，在训练阶段，每次只有一个加速器在执行计算，其它机器都在等待。为此，作者提出了**microbatch**，将输入的mini-batch进一步切分，达到类似于操作系统中流水线并行的效果，如（c）所示，**除了Bubble的部分没有利用起来，其它时间段所有机器都在执行训练任务，大大提高了训练效率。**
尽管如此，**在训练过程中，由于梯度反向传播阶段需要保留每个microbatch的激活才能计算，因此占用了大量内存，**如下图所示：

![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%205.png)

为了进一步减少内存的开销，作者提出**re-materialization重计算技术**，**训练时将中间的激活丢弃，在反向传播时重新计算**。这样虽然会带来额外的20%—30%计算开销，但也显著降低了加速器的内存占用。
最后总结一下GPipe这篇工作，GPipe具有高效性、灵活性和可靠性的特点，方法简单，效果显著，但是**GPipe的缺点在于很难均匀地切分模型，导致每一块加速器运行时间很难同步，就会造成等待的现象。**此外，额外的计算开销降低了模型的效率。因此，既然横向切分还是存在一定的问题，纵向切分可不可以解决这些问题呢？这就是接下来提到的张量并行，也是很多工作认为的真正的模型并行。

## **张量并行**

来自NVIDIA的[Megatron LM](https://arxiv.org/pdf/1909.08053.pdf)是典型的张量并行方法，它的通用性不如GPipe，只针对Transformer模型。现有的模型并行方法主要分为两种：流水线并行和张量并行。

- 流水线并行把模型的不同层放在不同的设备上。
- 张量并行则是层内纵向切分，将权重参数放到不同的设备中。

![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%206.png)

从另个角度来看，两种切分同时存在，是正交和互补的。

![](https://i-blog.csdnimg.cn/blog_migrate/ca5385e8565f32ddfa3a094e94513b89.png)

Megatron LM对Transformer的张量切片在每一层要进行两次，**一次是对自注意力层进行切分，另一次是对FFN层进行切分**。具体的切分过程如下图所示：

![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%207.png)

对于切割的部分，每次经过一层就要进行**all-reduce**，来保证参数的传递，这就导致了两个问题：

- all-reduce通信通过服务器之间的链接，这比服务器内多卡带宽NVLink要慢的多。
- 高度的模型并行产生很多小矩阵乘法，会降低GPU的利用率。

![](https://i-blog.csdnimg.cn/blog_migrate/d0ef9dc579e4c7f7ece8b40cbd87d989.png)

- all-reduce
    
    All-Reduce 可以通过 reduce-scatter 和 all-gather 这两个更基本的集群通信操作来实现。基于 ring 状通信可以高效的实现 reduce-scatter 和 all-gather，下面我们分别用示意图展示其过程。
    
    ![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%208.png)
    
- 每层Transformer模块就需要进行四次的操作（前向两次，反向两次），也就是说，随着模型的层数加深，Megatron LM所需的通信成本就要线性增大。
    
    attention层前向一次，后向一次；FFN层前向一次后向一次，图中为一个transformer块，而且用的是多头注意力，所以attention层要加一个linear层
    
    ![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%209.png)
    
    ![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%2010.png)
    

上图展示了Megatron LM的性能，**随着单组GPU的增加，模型的性能稳定下降**，在多服务器单卡时，模型仍有96%的性能，随着每个服务器中GPU的增加，模型的性能相对于单服务器相同卡的情况性能要略差些。
最后总结一下，**Megatron实现了张量的并行，在训练性能上要优于GPipe**，可以接近真正意义上的单卡训练，并且切分起来更加方便，无需考虑均匀分配问题。此外，它的局限性也很明显，首先它只对Transformer进行切分，第二它的通信量很大（大概是GPipe的10倍），不能做异步，等于用时间换空间。最后，这里的Megatron LM最多只能做到八卡并行，因为每张卡都存储了相同的输入和输出，占用了大量的显存空间，这在切片并行时可以得到解决。

## Megatron-LM

https://zhuanlan.zhihu.com/p/676116062：**Megatron-LM基础知识**

### 技术原理介绍

**Megatron-LM**结合了数据并行、张量并行和流水线并行等技术，能够高效地训练包含数十亿参数的模型。

常见的大模型训练技术包括：**数据并行技术、模型并行技术（包括张量并行技术和流水并行技术）、优化器状态并行技术、序列并行技术、激活重算技术**等。 Megatron-LM结合上述这些技术，提出了一种名为[PTD-P并行](https://zhida.zhihu.com/search?content_id=238378071&content_type=Article&match_order=1&q=PTD-P%E5%B9%B6%E8%A1%8C&zhida_source=entity)的技术，从而支持在千卡规模上以较好的计算性能来训练大模型。

- 数据并行技术（DATA PARALLEL）：在多个GPU组上有相同的模型参数副本。在参数更新前使用allreduce来全局平均梯度，从而加速模型训练。
    - 优化器状态并行技术（Distributed Optimizer/[ZeRO-DP](https://zhida.zhihu.com/search?content_id=238378071&content_type=Article&match_order=1&q=ZeRO-DP&zhida_source=entity)): 在使用数据并行技术的同时，将模型参数对应的优化器状态且分到不同的GPU上，从而支持训练更大的模型。
- 模型并行技术(Model Parallel)：在多个GPU上存放一套模型参数的不同分片，从而支持训练更大的模型。
    - 流水并行技术（Pipeline Parallel）：模型内不同Transformer层切分到不同的GPU上。
        - 技术实现
            
            以下示意图batch size = 8，可以看到图中所有的1连在一起是一个micro batch的完整前向以及后向运算。
            
            ![](https://picx.zhimg.com/v2-9d0a11deb5c3341e040cf8c6f3f82373_1440w.jpg)
            
            ### **非交错式调度**
            
            非交错式调度可分为三个阶段。第一阶段是热身阶段，处理器进行不同数量的前向计算。在接下来的阶段，处理器进行一次前向计算，然后是一次后向计算。最后一个阶段处理器完成后向计算。
            
            **微软的 [PipeDream](https://zhida.zhihu.com/search?content_id=238378071&content_type=Article&match_order=1&q=PipeDream&zhida_source=entity) 就是使用非交错式 1F1B 调度（上面三个图中中间图所示）**。虽然，这种调度模式比 GPipe 更节省内存。然而，它需要和 GPipe 一样的时间来完成一轮计算。
            
            ### **交错式调度**
            
            在交错式调度中，每个设备可以对多个层的子集（称为模型块）进行计算，而不是一个连续层的集合。
            
            具体来看，**在之前非交错式调度中，设备1拥有层1-4，设备2拥有层5-8，**以此类推；但**在交错式调度中，设备1有层1,2,9,10，设备2有层3,4,11,12，以此类推。**在交错式调度模式下，流水线上的每个设备都被分配到多个流水线阶段（虚拟阶段，virtual stages），每个流水线阶段的计算量较少。
            
            即microbatch【1】在device0的运算被切成两阶段。device0中第一个细1跑1、2层，然后等device1,2,3运行完后面的3-8层后，device0中的细2再跑9、10层的运算。
            
            这种模式既节省内存又节省时间。但这个调度模式要求 micro-batch 的数量是流水线阶段（Stage）的整数倍。
            
            ### **PipeDream（非交错式1F1B）—> Megatron-LM（交错式1F1B）**
            
            英伟达 Megatron-LM 的流水线并行相关的论文（Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM）中采用了**交错式 1F1B 调度**。
            
            上图中最下的图所示。
            
            Megatron-LM 基于 PipeDream-Flush 提出了一个小的Trick：交错式 1F1B 调度，而交错式 1F1B 调度也是 Megatron-LM 论文（Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM），virtual pipeline）中最主要的一个创新点。
            
            传统的流水线并行通常会在一个设备（Device）上放置几个连续的模型层（如：Transformer层）。但 Megatron 这篇论文采用**虚拟流水线（virtual pipeline）**，进行交错式1F1B并行。**在设备数量不变的情况下，分出更多的流水线阶段（pipeline stage），以更多的通信量，换取流水线Bubble比率降低**。
            
            例如，之前如果每个设备有 4 层（即设备 1 有 1 – 4 层，设备 2 有 5 – 8 层，依此类推），现在我们可以让每个设备对两个模型块执行计算（每个模型块有 2 层） ，即设备 1 有第 1、2、9、10 层； 设备 2 有第 3、4、11、12 层，依此类推。 通过这种方案，流水线中的每个设备都被分配多个流水线阶段（与以前相比，每个流水线阶段的计算量更少）。
            
            ![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%2011.png)
            
    - 张量并行技术（Tensor Parallel）:模型一个Transformer层内的Attention 和 Linear部分参数切分到不同的GPU上。
        - 技术实现
            
            ### MLP层
            
            MLP的计算主要包括：一个GEMM(General Matrix Multiply) + GeLU + 第二个GEMM(General Matrix Multiply) + Dropout
            
            ![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%2012.png)
            
            **最后在Dropout之前，我们在 g 处做一个all-reduce把所有结果都聚合在一起（这一步聚合是必须的，无法省去）。**
            
            ![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%2013.png)
            
            ### self-attention层
            
            和MLP类似，不同点在于列并行的方式是根据多头注意力的切分来进行的，比如下图有两个注意里头，他们各自的矩阵乘会在不同的gpu计算，第二个GEMM是output linear layer，他直接拿到第一个GEMM的输出然后做行并行，然后**在最后的 g 做all-reduce**。
            
            ![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%2014.png)
            
            ### **Output parallelize & Input Parallelize（输出和输出的embedding层并行）**
            
            ![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%2015.png)
            
            https://zhuanlan.zhihu.com/p/497672789：**Cross Entropy Loss 的并行化方案**
            
    - 序列并行技术（Sequence Parallel）:模型一个Transformer层内的dropout和layer-norm等部分切分到不同GPU上，从而支持训练更大的模型。
- 激活重算技术 (Activation Recompuation)：在反向时重新计算部分激活，避免占用显存存储这部分激活，从而支持训练更大的模型。
    - 技术实现
        
        ![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%2016.png)
        

### 实践：Megatron-LM的并行实现【看不懂】

![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%2017.png)

![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%2018.png)

- **MP：模型并行组（Model Parallism）**。假设一个完整的模型需要布在8块GPU上，则如图所示，我们共布了2个model replica（2个MP）。MP组为：[[g0, g1, g4, g5, g8, g9, g12, g13], [g2, g3, g6, g7, g10, g11, g14, g15]]
- **TP：张量并行组（Tensor Parallism）**。对于一个模型的每一层，我们将其参数纵向切开，分别置于不同的GPU上，则图中一共有8个TP组。TP组为：[[g0, g1], [g4, g5],[g8, g9], [g12, g13], [g2, g3], [g6, g7], [g10, g11], [g14, g15]]
- **PP：流水线并行组（Pipeline Parallism）**。对于一个模型，我们将其每一层都放置于不同的GPU上，则图中一共有4个PP组。PP组为：[[g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]]
- **DP：数据并行组（Data Parallism）**。经过上述切割，对维护有相同模型部分的GPU，我们就可以做数据并行，则图中共有8个DP组。DP组为[[g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]]

## **切片并行：DeepSpeed（ZeRO）**

https://blog.csdn.net/qq_56591814/article/details/133189752

PyTorch提出了一个新的分布式API名叫FSDP，即**完全分片的数据并行**，其背后的思想就来自于ZeRO。ZeRO考虑的是如何将数据并行用到超大规模的神经网络训练上，并且**在模型过大的时候如何将不需要的模块放到一个地方，在需要的时候再拿过来用。**所以，也可以说**ZeRO的核心思想是数据并行和模型并行。**

ZeRO的作者分析，现有模型内存消耗主要分为两个部分：

对于大型模型，**大部分内存被模型参数占用**，**包括Adam动量和方差、梯度和参数**等。
**激活值**，**临时缓存**和碎片。

这里举个例子来说，对于1.5B的GPT-2模型，训练时采用半精度fp16，在Adam更新模型时需要采用fp32，这一块占用24GB的空间，然而真正算梯度的时候只需要3GB的显存，其余的21GB大部分时间都不会用上。此外在长度为1000、batch为32时，中间结果需要60GB的显存来存储。

针对上面问题，作者首先提出**对模型的优化算法ZeRO-DP，即将模型切块放在不同地方，用的时候再拿来**。**其次是如何优化中间状态，称作ZeRO-R。**

### ZeRO-DP（ZeRO-1～3）：优化模型状态内存【一种改进的数据并行】

具体来说，对于ZeRO-DP，有如下的insight：

- **数据并行比模型并行效率高**。
- 当前的并行方法都需要维护中间值。

因此**ZeRO-DP采用数据并行的方法**，并且**对于某个中间值，只存储在一个GPU上，当别的GPU需要的时候再通过通信的方式从该GPU上接收数据**，其思想类似于参数服务器。

数据并行和模型并行都保持了整个训练过程中所需的所有模型状态，但并不是所有时候这都是必需的。例如，**仅在某个层的正向传播和反向传播期间才需要用到每个层对应的参数。**

ZeRO-DP是一种改进的数据并行性方法，它通过对参数（包括优化器状态、梯度和参数）进行分区来消除内存冗余，使得每个GPU仅保存部分参数及相关状态，提高了内存效率；同时还通过在训练过程中使用**动态通信**来保持计算和通信效率。

![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%2019.png)

![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%2020.png)

> 参数解释：
> 
> 
> Baseline：未优化的基线
> Ψ：模型大小，上图假设模型参数为Ψ=75亿（7.5B）
> K：存储优化器状态要消耗的内存倍数，上一节讲过，**对于混合精度的Adam优化器而言，K=12（动量、动量二阶矩、模型参数各4）**
> 
> - 解释
>     
>     Adam需要存储动量和方差两种优化器状态，以计算参数更新；此外，还需要存储梯度和模型权重本身的信息。所以对一个具有Ψ个参数的模型进行混合精度训练时，其内存需求为：
>     
>     fp16格式的参数和梯度，都是2Ψ字节
>     
>     fp32格式的优化器的状态信息，包括参数、动量和方差，都是4Ψ字节
>     
>     我们用KΨ来表示存储优化器状态信息所需要的内存数，则对于混合精度Adam优化器而言，总的内存需求为(2+2+k)Ψ=(2+2+4+4+4)Ψ=16Ψ字节，所以对于一个拥有15亿参数的模型（GPT-2），其内存需求至少为24GB，远远超过只保存fp16参数所需的3GB内存。
>     
> 
> Nd：数据并行度。基于Adam优化器的混合精度训练，数据并行度为Nd=64（即64个GPU）
> 

上图展示了`ZeRO-DP`对数据并行优化的三个阶段：

- 优化器状态分割（Pos）：
在每个gpu中保存全部的参数和梯度，但是只保存1/Nd的优化器变量。通过将优化器状态进行分割，实现4倍的内存减少，同时保持与DP相同的通信量。
- 梯度分割（Pos+g）：
每个gpu中只保存1/Nd的梯度，实现8倍的内存减少，并保持与DP相同的通信量。
- 参数分割（Pos+g+p）：
每个gpu中只保存1/Nd的参数 ，实现64倍的内存减少，通信量会略微增加50%。作者通过用少量的计算的成本和通信成本换来了大幅的内存节省。

**优化器状态 → 梯度 → 模型参数：**zero1对应着优化器参数的切片，zero2对应着zero1和梯度的切片，而zero3对应着zero2和模型参数的切片，可以看到当三种切片都采用的时候，单张卡上的显存占用明显减少。当模型越来越大的时候，比如1T的模型，也能在zero3、1024张GPU下运行。

### 为什么说ZeRO-DP是一种数据并行？

ZeRO-DP（Zero Redundancy Optimizer Data Parallelism）虽然通过优化内存使用显著区别于传统数据并行（Data Parallelism, DP），但其核心仍属于**数据并行**的范畴。以下是详细解释：

---

### **1. 数据并行的核心特征**

数据并行的定义是：

- **数据分割**：将训练数据划分为多个批次（batch），分配到不同设备（GPU）上；
- **模型复制**：每个设备持有完整的模型副本，独立计算局部梯度；
- **梯度聚合**：通过全局通信（如AllReduce）同步梯度，更新模型参数。

**传统DP的局限性**：

- 每个GPU需存储完整的模型参数、优化器状态和梯度，显存占用高，限制模型规模。

---

### **2. ZeRO-DP 的数据并行本质**

ZeRO-DP 通过**分区优化器状态、梯度和参数**消除冗余存储，但其并行逻辑仍符合数据并行的核心特征：

### **(1) 数据分割与独立计算**

- **数据分配**：每个GPU处理不同的数据子集（如Batch 1 → GPU 1, Batch 2 → GPU 2）；
- **前向/反向传播**：每个GPU使用完整的模型计算局部梯度（**尽管参数可能动态加载**）。

### **(2) 梯度聚合与参数更新**

- **梯度同步**：通过AllReduce或AllGather通信聚合梯度（与传统DP一致）；
- **优化器分区**：每个GPU仅更新其负责的参数分区，其余参数通过通信按需获取。

### **(3) 关键区别：冗余消除**

- **传统DP**：所有GPU存储全部参数、梯度、优化器状态（100%冗余）；
- **ZeRO-DP**：
    - **ZeRO-Stage 1**：分区优化器状态（减少4倍显存）；
    - **ZeRO-Stage 2**：分区梯度（减少8倍显存）；
    - **ZeRO-Stage 3**：分区参数（减少显存与模型大小线性相关）。

尽管存储策略不同，但**计算与通信逻辑仍遵循数据并行**。

---

### **3. 为什么不是模型并行？**

模型并行（如Tensor Parallelism, Pipeline Parallelism）的特征是：

- **模型分割**：将模型层或参数拆分到不同设备，每个设备仅计算部分网络；
- **数据流动**：需设备间频繁通信传递中间结果（如激活值）。

**ZeRO-DP的区别**：

- 每个GPU仍独立计算完整模型的前向/反向传播（**参数可能动态加载，但计算逻辑未拆分**）；
- 通信仅发生在梯度同步阶段，而非逐层传递中间结果。

---

### **4. 类比：图书馆与书籍管理**

| **方法** | **类比** | **ZeRO-DP的优化** |
| --- | --- | --- |
| **传统DP** | 每个图书馆（GPU）存放所有书籍（模型参数），读者（数据）分到不同图书馆。 | 冗余存储，空间浪费。 |
| **ZeRO-DP** | 图书馆仅存放部分书籍，读者需借阅其他馆藏书（通信获取参数），但所有读者仍可阅读任何书籍（完整模型计算）。 | 按需加载，消除冗余存储。 |
| **模型并行** | 每本书的章节拆分到不同图书馆，读者需跨馆阅读（中间结果传递）。 | 计算逻辑被拆分，非ZeRO-DP模式。 |

---

### **5. 代码示例（ZeRO-DP vs 传统DP）**

### **传统DP（PyTorch）**

```python
# 每个GPU存储完整模型副本
model = nn.Linear(100, 100).to(device)
dp_model = torch.nn.DataParallel(model)  # 自动分配数据，AllReduce梯度
```

### **ZeRO-DP（DeepSpeed）**

```python
# 模型参数分区存储，按需通信
model = nn.Linear(100, 100)
ds_engine = deepspeed.initialize(
    model=model,
    config={"zero_optimization": {"stage": 3}}  # 启用ZeRO-Stage 3
)
# 数据仍按batch分配到各GPU，但参数动态加载
```

---

### **6. 总结**

| **维度** | **传统DP** | **ZeRO-DP** | **模型并行** |
| --- | --- | --- | --- |
| **数据分配** | 数据分片 | 数据分片 | 数据完整传递 |
| **模型存储** | 完整副本 | 分区存储（无冗余） | 参数拆分到不同设备 |
| **计算逻辑** | 完整模型计算 | 完整模型计算（参数动态加载） | 部分模型计算 |
| **通信内容** | 梯度聚合 | **梯度/参数按需同步** | 中间激活值或层输出 |

**结论**：

ZeRO-DP通过内存优化（分区存储）扩展了数据并行的能力，但其**数据分配方式**和**计算逻辑**仍符合数据并行的定义，因此属于数据并行的一种高级实现形式。

### ZeRO-R：**优化残余状态内存**

https://blog.csdn.net/qq_56591814/article/details/133189752

对于ZeRO-R，它的思想是说**在Megatron中，每一层的参数虽然是并行的，但是输入却是相同的，即复制的，这个值可能会很大**，因此ZeRO-R将这些输入也分成块，**当一个GPU需要的时候，向其他拥有该分块的节点发送请求**，这是一种**带宽换空间**的方法。**对于缓存，给定固定的大小，当数据不用时就删除。对于碎片则采用内存整理。**

在ZeRO-DP提高了模型状态内存效率之后，主要消耗在**激活值、临时缓冲区**以及无**法使用的内存碎片**这三个方面的剩余内存成为次要的**内存瓶颈**。为了解决这个问题，我们开发了ZeRO-R来进行优化：

- 通过**激活值分区**来优化激活值内存
    
    Metagron-LM使用**Activation checkpointing技术**来优化激活值内存，但这对于大模型仍旧是不足的。**ZeRO-R通过激活值分割来识别和删除现有MP方法中的激活值复制，从而优化激活值内存**。同时，它还会在适当的情况下将激活值卸载到CPU上，以释放GPU内存（CPU-offload）：
    
    在训练过程中，激活值可能占用大量内存 。例如GPT-2有15亿参数，在序列长度为1K和批量大小为32的情况下，大约需要60GB的内存 。**Activation checkpointing通过牺牲33%的重新计算开销来减少激活值内存占用（N → sqrt{N}），这将把该模型的激活内存消耗降低到约8GB左右。**
    
    尽管Activation checkpointing显著减少了内存占用，但对于更大的模型，其内存开销依旧很大。例如，一个类似GPT的1000亿参数的模型，在批量大小为32的情况下，即使使用了Activation checkpointing，也需要约60GB的内存。
    
- **恒定临时缓冲区大小**，以在内存和计算效率之间取得平衡
    
    缓冲区的大小通常会随着模型的规模变化而变化，但**ZeRO-R采用了固定大小的缓冲区，这样可以防止随着模型规模的增加而导致缓冲区过大**。同时ZeRO-R会保持缓冲区足够大，以保证计算效率。
    
- **根据张量的不同生命周期来管理内存，以防止内存碎片化**
    
    训练时产生的内存碎片化，是由于不同张量的生命周期差异引起的。内存碎片化可能导致内存分配失败，即使总内存足够也可能出现问题，因为无法获得足够的连续内存。
    

ZeRO-R对中间变量进行处理降低其占用的内存。**Pa对输入（激活）做切分，和张量并行结合使用。CB对模型的通信参数做了一个固定的缓存**，在同时并行GPU数量很多的情况下，每次向单个GPU发送的参数数量可能很小，因此**通过设计一个缓存来尽可能发送更多的数据，提高模型的带宽通信效率**，同时通过一个计时器防止较大的通信延时。但是过多的缓存会带来碎片问题，**MD进行了碎片化管理，它将需要维护的内存开在了预选设定好的位置，其他位置都是可以随时析构的。**

![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%2021.png)

实验见上图所示，ZeRO和基线Megatron LM进行了对比，其中Megatron LM在超过8卡上由于需要不同机器之间的通信因此带来巨大的通信开销，导致性能大幅下降，而ZeRO却能一直保持比较稳定的计算峰值，当然也随着机器的增多逐渐下滑。

![image.png](%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6%201bae64a566218091a4e1fb62028173f8/image%2022.png)

上图更能直接展示ZeRO超线性增长的趋势，其中**随着GPU数量翻倍，模型的整体性能翻了不止一倍，这是因为随着单张卡上模型的参数变小，可以输入更大批量的数据，每块卡上矩阵变大，可以更高效利用GPU的多核，此外计算和通信比也变得更好了。**
总结一下，ZeRO的算法相对来说还是比较简单实用的，其核心思想是all reduce时不需要在所有节点上重复，只需要在各自节点上all reduce就好。在不用完整数据的时候，维护好自己那一块就好，当需要的时候，再通过通信得到完整的数据。

### DeepSpeed

Deepspeed是由微软开发的一款开源深度学习优化库，旨在提高大规模模型训练的效率和可扩展性。该框架采用多种技术手段来加速训练，其中包括**模型并行化、梯度累积、动态精度缩放、本地模式混合精度**等。

Deepspeed还提供了一些辅助工具，如**分布式训练管理、内存优化和模型压缩**等，以帮助开发者更好地管理和优化大规模深度学习训练任务。该框架是基于PyTorch构建的，因此可以简单修改以便进行迁移使用。Deepspeed已经在许多大规模深度学习项目中得到了应用，包括语言模型、图像分类、目标检测等领域。在深度学习模型软件体系架构中，Deepspeed扮演着重要的角色。

Deepspeed是一款高效、可扩展的深度学习优化库，其中一些核心技术如下：

- ZeRO（Zero Redundancy Optimizer）
    
    ZeRO是一种内存优化技术，用于大规模分布式深度学习。该技术可以消除数据并行进程中的内存冗余，通过在数据并行进程之间划分模型状态参数、梯度和优化器状态，而不是复制它们。此外，ZeRO还使用动态通信调度在分布式设备之间共享必要的状态，以保持数据并行的计算粒度和通信量。基于ZeRO，DeepSpeed实现了数据并行、流水线并行和张量切片模型并行等方式的训练，以提高显存和计算效率，并能够训练具有万亿个参数的模型。
    
- 3D并行
    
    数据并行和模型并行可能会导致内存冗余，因此DeepSpeed采用基于ZeRO的3D并行来优化显存利用和计算效率。该技术将模型状态参数、梯度和优化器状态按照3D方式划分，并使用动态物理内存分配来减少内存占用。
    
- 梯度累积
    
    DeepSpeed还使用梯度累积来提高批量大小，从而提高训练效率。梯度累积可以通过多次前向传递和反向传递来累积梯度，从而实现较大的批量大小。
    

https://blog.csdn.net/m0_65555479/article/details/144471963?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7Ebaidujs_baidulandingword%7ECtr-3-144471963-blog-145600189.235%5Ev43%5Epc_blog_bottom_relevance_base5&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7Ebaidujs_baidulandingword%7ECtr-3-144471963-blog-145600189.235%5Ev43%5Epc_blog_bottom_relevance_base5&utm_relevant_index=5

# 部署框架：ollama

1）、主要功能与特点

- 简化部署：Ollama使用**Docker**容器技术来简化LLM的部署过程，使得用户无需深入了解底层复杂性即可快速启动和运行模型。用户只需执行简单的命令即可在本地计算机上部署和管理LLM，降低了技术门槛。
- 捆绑模型组件：Ollama将模型权重、配置和数据捆绑到一个包中，称为**Modelfile**，这有助于优化设置和配置细节，包括GPU使用情况。这种捆绑方式使得用户能够更轻松地管理和切换不同的模型。
- 支持多种模型：Ollama支持多种大型语言模型，如Llama 2、Code Llama、Mistral、Gemma等，并允许用户根据特定需求定制和创建自己的模型。
- 命令行操作：安装完成后，用户可以通过简单的命令行操作启动和运行大型语言模型。例如，要运行Gemma 2B模型，只需执行命令ollama run gemma:2b。

2）、使用场景与优势

使用场景：

Ollama可以应用于多种场景，如聊天机器人、文本生成、问答系统等。它为研究人员、开发人员和爱好者提供了一个强大的工具来探索和使用LLM技术。

优势：

- 易用性：Ollama提供了简洁的API和类似ChatGPT的聊天界面，使得用户无需开发即可直接与模型进行交互。
- 轻量级：Ollama的**代码简洁明了，运行时占用资源少**，适合在本地计算机上运行。
- 可扩展性：Ollama**支持多种模型架构，并可以扩展以支持新的模型**。它还支持热加载模型文件，无需重新启动即可切换不同的模型。
- 预构建模型库：Ollama提供了一个预构建模型库，涵盖了各种自然语言处理任务，如文本生成、翻译、问答等。

# 推理框架：vLLM

VLLM（超大型语言模型）是SKYPILOT开发的推理优化框架，主要用于提升大语言模型在GPU上的运行效率。它的优势体现在以下几个方面：

- 快速令牌生成：采用连续批处理技术，让令牌生成速度大幅提升。
- 高效内存利用：借助PagedAttention技术，在处理大上下文窗口时，能有效控制GPU内存消耗。
- 无缝集成：与PyTorch、TensorFlow等主流深度学习平台兼容，可轻松融入AI工作流程。

大模型框架vLLM是一个高效的大模型推理与服务引擎，专为大型语言模型（LLM）打造，旨在提升推理速度、降低显存占用，并更好地满足实际应用需求。以下是对vLLM的详细介绍：

1）、概述

vLLM是一个基于Python的LLM推理和服务框架，由伯克利大学LMSYS组织开源。它通过创新的**PagedAttention**技术、**连续批处理**、**CUDA核心优化**以及**分布式推理**支持，显著提高了LLM的推理性能。vLLM不仅简单易用，而且性能高效，广泛应用于各种NLP任务中。

2）、核心优势

PagedAttention技术：

通过内存管理技术，PagedAttention能够将注意力机制中的键（keys）和值（values）存储在不连续的显存空间中，从而减少显存碎片，提高显存利用率。

吞吐量最多可以达到Hugging Face实现的24倍，文本生成推理（TGI）高出3.5倍，且无需修改模型结构。

**连续批处理：**

**vLLM能够连续批处理接入的请求，充分利用GPU资源，提高吞吐量。**

CUDA核心优化：

针对CUDA核心进行了优化，确保速度与效率。

分布式推理支持：

支持分布式推理，能够在多台GPU上并行运行模型，进一步提高推理速度。

## **PagedAttention**：充分利用显存空间

https://zhuanlan.zhihu.com/p/691038809：**图解大模型计算加速系列之：vLLM核心技术PagedAttention原理**

- vLLM是通过什么技术，动态地为请求分配KV cache显存，提升显存利用率的？
- 当采用动态分配显存的办法时，虽然明面上同一时刻能处理更多的prompt了，但因为没有为每个prompt预留充足的显存空间，如果在某一时刻整个显存被打满了，而此时所有的prompt都没做完推理，那该怎么办？

### **虚拟内存的分业管理**

**虚拟内存的分业管理技术总结起来就是：**

- **将物理内存划分为固定大小的块，我们称每一块为页（page）**。从物理内存中模拟出来的虚拟内存也按相同的方式做划分
- 对于1个进程，我们不需要静态加载它的全部代码、数据等内容。**我们想用哪部分，或者它当前跑到哪部分，我们就动态加载这部分到虚拟内存上，然后由虚拟内存帮我们做物理内存的映射**。
- **对于1个进程，虽然它在物理内存上的存储不连续（可能分布在不同的page中），但它在自己的虚拟内存上是连续**的。**通过模拟连续内存的方式，既解决了物理内存上的碎片问题，也方便了进程的开发和运行。**

### **PagedAttention实现**

**1）处理单个请求**

现在，你已经知道虚拟内存分页管理的基本原理和优势，趁热打铁，我们马上来看以其为灵感的PagedAttention技术是如何操作的。我们还是从具体的例子讲起。

假设现在你向模型server发送一条请求，prompt为`Four score and seven years ago our`，你希望模型能做续写。PagedAttention的运作流程如下图：

![](https://pic1.zhimg.com/v2-e0209cc4bb1fb2b7bd5b3b88dae70476_1440w.jpg)

在图中：

- **请求（request）可理解为操作系统中的一个进程**
- **逻辑内存（logical KV blocks）可理解为操作系统中的虚拟内存，每个block类比于虚拟内存中的一个page。每个block的大小是固定的，在vLLM中默认大小为16，即可装16个token的K/V值**
- **块表（block table）可理解为操作系统中的虚拟内存到物理内存的映射表**
- **物理内存（physical KV blocks）可理解为操作系统中的物理内存，物理块在gpu显存上，每个block类比于虚拟内存中的一个page**

**(i) Prefill阶段**

- **划分逻辑块**：vLLM拿到这条prompt，先按照设定好的block大小B（本例中B=4），为prompt划分逻辑块（Logical KV blocks）。由于prompt中有7个token，所以vLLM用2个逻辑块（block 0， block 1）来装它们的KV值。其中，在逻辑块1中目前只装了"years", "ago", "hour"这3个token的KV值，有1个位置是空余的。这个位置就被称为保留位（reservation）
- **划分物理块**：划分好逻辑块后，我们就可以将其映射到物理块中去了。物理块是实际存放KV值的地方。我们通过一张block table来记录逻辑块和物理块的映射关系，block table的主要内容包括：
    - **逻辑块和物理块的映射关系（physical block number）**：例如逻辑块0对应物理块7
    - **每个物理块上被填满的槽位（# filled）**：例如在prefill阶段，对物理块7，其4个槽位都被填满；对物理块1，其3个槽位被填满。
- **正常计算prompt的KV值，并通过划分好的关系填入物理块中。**

**(ii) Decode阶段-生成第1个词。**

- **使用KV cache计算attention，生成第1个词fathers**。不难发现，当我们计算时，我们使用的是逻辑块，即形式上这些token都是连续的。与此同时，vLLM后台会通过block table这个映射关系，帮我们从物理块上获取数据做实际计算。**通过这种方式，每个request都会认为自己在一个连续且充足的存储空间上操作，尽管物理上这些数据的存储并不是连续的。**
- **基于新生成的词，更新逻辑块、物理块和block table**。对于block table，vLLM将它filled字段由3更新至4。
- **分配新的逻辑块和物理块**。当fathers更新进去后，逻辑块已装满。所以vLLM将开辟新的逻辑块2，并同时更新对应的block table和物理块。

**(ii)** **Deocde阶段-生成第2个词**

类比步骤（2）来进行。

**（2）处理多个请求**

![](https://pica.zhimg.com/v2-eb6a0e56a2d4815cb7c942e63e3ae07c_1440w.jpg)

# 加速计算：**FlashAttention**

https://zhuanlan.zhihu.com/p/669926191：**图解大模型计算加速系列：FlashAttention V1，从硬件到计算逻辑**

https://zhuanlan.zhihu.com/p/691067658：**图解大模型计算加速系列：Flash Attention V2，从原理到并行计算**
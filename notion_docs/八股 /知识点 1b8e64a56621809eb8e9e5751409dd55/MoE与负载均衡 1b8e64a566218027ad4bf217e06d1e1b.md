# MoE与负载均衡

State: 已完成

https://www.cnblogs.com/ExMan/p/18709928

https://zhuanlan.zhihu.com/p/18565423596：**deepseek技术解读(3)-MoE的演进之路**

https://baijiahao.baidu.com/s?id=1823370044595613944&wfr=spider&for=pc

## MoE介绍

**MoE(Mixture of Experts)是一种网络层结构， 网络层主要包括三部分：**

- **专家网络(Expert Network)**：是一个前馈网络，逻辑上一个专家网络擅长处理一类专项的子任务，所有专家都接受相同的输入，来做特定计算处理，产出不同的输出
- **门控网络(Gating Network)**：跟专家网络接收一样的输入，负责产出专家偏好的权重。来指示对于一个输入，不同专家的重要程度。
- **选择器(selector)**：是一种根据专家权重来做专家选择的策略。可以选择权重最高的Top1专家或选择TopK专家来融合得到最终的结果。

## GShard：**Transformer MoE层，Top-2**

**Transformer MoE层**：MoE层替换Transformer的FFN层，计算逻辑：对于一个token 分别通过门控网络和专家网络计算门控值和专家输出，然后用门控值加权多个专家输出来产出最终结果。具体如下：

![image.png](MoE%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%201b8e64a566218027ad4bf217e06d1e1b/image.png)

> 注：这里的专家是**token级专家**，而不是样本粒度，每个token都会做专家路由。此外专家是**稀疏激活的**，是根据门控值取topK个专家来融合计算最终的结果。GShard最多激活权重最高的2个专家。
> 

**负载均衡-辅助损失**：引入负载均衡损失，目的是解决多专家token分布不均的问题。因为如果完全按门控权重选取topk专家，容易导致训练过程出现负载不均衡的问题。比如：大多数token被分配到少数几个专家，导致只有少数专家数据通信繁忙造成拥堵，从而减缓训练速度；也会导致其他专家得不到充分训练。为了解决这个问题，定义了一个辅助损失（aux_loss）来降低负载不均衡问题。

![image.png](MoE%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%201b8e64a566218027ad4bf217e06d1e1b/image%201.png)

- 为什么me可以看作是ce/S的近似
    
    ![image.png](MoE%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%201b8e64a566218027ad4bf217e06d1e1b/image%202.png)
    
    ![image.png](MoE%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%201b8e64a566218027ad4bf217e06d1e1b/image%203.png)
    
- 这样近似计算有什么好处？
    
    ![image.png](MoE%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%201b8e64a566218027ad4bf217e06d1e1b/image%204.png)
    

模型的总损失是原始训练目标损失+k*laux，其中k是一个常数系数

![image.png](MoE%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%201b8e64a566218027ad4bf217e06d1e1b/image%205.png)

## **Switch Transformer：当“少就是多”**

Switch Transformer 的思路很直接：“我们干脆只给每个 token 选一个专家得了。” 这样做一来简化了 gating 的逻辑（直接挑最高 logit 的专家），二来也极大降低了计算和通信负担。具体做法是：

**𝑔𝑖(𝑥)=softmax(𝑊router⋅𝑥)𝑖**

然后我们选

**expert_index(𝑥)=argmax𝑖𝑔𝑖(𝑥).**

Switch Transformer 最重要的创新点在于它的 **单专家路由**：每个 token 只走一个专家，代码好写，训练速度也快得多。为了保持负载均衡，还是会用类似 GShard 的辅助损失，并且提出了一个 capacity factor（CF） 的概念：

**𝐶 = CF × (tokens per batch/number of experts)**

这就**告诉模型每个专家能接收多少 token，多的就要丢（或者用 residual 旁路继续传递）。**

- residual旁路
    
    在混合专家（Mixture of Experts, MoE）架构中，**residual旁路**（residual bypass）是一种用于处理超出专家容量限制的token的机制。具体来说，当某个专家接收的token数量超过了预定的capacity factor限制时，多余的token不会被简单地丢弃，而是通过residual旁路继续传递到后续的模型层中。以下是对residual旁路的详细解释：
    
    ### 1. 什么是Residual旁路？
    
    Residual旁路是一种**旁路路径**，它**允许某些token绕过当前专家，直接传递到下一个模块或层**。这种机制确保了即使某个专家已经达到其处理能力的上限，输入数据仍然能够被有效地处理，而不会被丢弃或丢失。
    
    ### 2. 为什么需要Residual旁路？
    
    在MoE架构中，每个专家都有其处理能力的上限，这个上限由capacity factor决定。当一个专家接收的token数量超过了这个上限时，多余的token如果被丢弃，可能会导致信息丢失，影响模型的性能。因此，residual旁路提供了一种解决方案，使得这些多余的token仍然能够被处理，而不会对模型的整体性能产生负面影响。
    
    ### 3. Residual旁路的工作原理
    
    当一个专家接收的token数量超过其capacity factor限制时，以下是residual旁路的工作流程：
    
    1. **分配token**：门控网络首先将token分配给各个专家。对于每个专家，计算其接收的token数量。
    2. **检查容量**：如果某个专家接收的token数量超过了其capacity factor限制，多余的token会被标记。
    3. **通过旁路传递**：这些多余的token不会被丢弃，而是通过residual旁路直接传递到下一个模块或层。
    4. **继续处理**：在下一个模块或层中，这些通过旁路传递的token会与其他正常处理的token一起进行后续的计算和处理。
    
    ### 4. Residual旁路的优势
    
    - **信息保留**：通过residual旁路，多余的token不会被丢弃，从而保留了更多的信息。
    - **模型稳定性**：这种机制提高了模型的稳定性，避免了由于信息丢失导致的性能下降。
    - **灵活性**：residual旁路使得MoE架构更加灵活，能够处理更复杂的输入数据。
    
    ### 5. 在GLaM中的应用
    
    在GLaM中，residual旁路被用来处理那些超出专家容量限制的token。具体来说，当某个专家接收的token数量超过了capacity factor时，多余的token会通过residual旁路继续传递到后续的模型层中。这种方法确保了GLaM在处理大规模数据时能够保持高效和稳定。
    
    ### 总结
    
    Residual旁路是一种重要的机制，它允许多余的token绕过已达到容量限制的专家，直接传递到后续的模型层中。通过这种方式，MoE架构能够有效地处理所有输入数据，避免信息丢失，并保持模型的稳定性和性能。在GLaM中，residual旁路是实现高效负载均衡和确保模型性能的关键组件之一。
    

利弊：从直觉上讲，单专家路由能带来更高的速度，因为每个 token 只过一个 FFN，少了大把计算开销。但问题也很明显：要是 capacity factor 调不好，某些专家可能被疯狂挤爆，造成大量 token overflow，或者路由又太松导致浪费。Switch Transformer 让我们看到，哪怕是只用 top-1 gating，也能成功扩展大规模模型——只要你肯下功夫调好那些超参。它也把问题抛给了业界：到底选 top-K 里的哪个 “K”才最优？overflow 又怎么处理才好？

## **GLaM：带着效率回归 Top-2**

GLaM（Generalist Language Model）在 Switch Transformer 之后又把 top-2 gating 搬了回来，但增加了对 能耗效率 的关注，并声称他们只用了大约 GPT-3 训练能耗的三分之一，却在 zero-shot 任务上表现更好。核心公式大概是：

**𝑦=∑𝑖=12𝑔𝑖⋅𝐸𝑖(𝑥),**

其中 $g_i$ 是 gating 权重，$E_i(x)$ 是被选中的两个专家输出。同样，GLaM 也采用了一个精心设计的辅助损失来鼓励专家负载更均衡，损失函数类似：

**𝐿aux=𝛼⋅∑𝑖=1𝐸𝑓𝑖⋅𝑝𝑖,**

并设置了一个容量约束：

**𝐶=tokens per batchnumber of experts⋅capacity factor.**

超出这个容量的 token 还是要被丢弃，通过 **residual** 路径让网络继续往后走。**一般会把 capacity factor 设为 1.25，**来兼顾效率和 overflow 问题。

坑与经验：GLaM 让大家看到，真正只激活一小部分参数，就能在算力和能耗上吊打类似 GPT-3 的 dense 模型——这是一次在大规模模型的“能效”上非常耀眼的案例。但仍然需要提醒的是，如果真实数据分布不平衡，专家可能还是会出现负载不均，而 GLaM 也花了不少心思去调节 gating、capacity 等超参。

**在标准交叉熵损失的基础上，我们添加了GShard (Lepikhin等人，2021)中描述的MoE辅助损失，其系数为0.01，以鼓励专家负载平衡，以便门控函数将令牌更均匀地分配给所有专家。**我们使用**sentencepece** (Kudo & Richardson, 2018)子词分词器，词汇量为256K。在训练期间，我们使**用float32作为模型权重，使用bfloat16作为激活**。最大的GLaM 64B/64E模型在1024个Cloud TPU-V4芯片上进行了训练。

- 用BF16存储激活值
    
    在深度学习模型的训练过程中，数据类型的选择对模型的性能和计算效率有着重要影响。你提到的**"bfloat16 for activations"**指的是在训练过程中，使用bfloat16数据类型来存储和计算激活值（activations）****。以下是对此的详细解释：
    
    ### 1. 什么是bfloat16？
    
    **bfloat16**（Brain Floating Point）是一种16位的浮点数格式，由Google提出并广泛应用于深度学习和人工智能领域。与标准的16位浮点数格式（如IEEE 754的半精度浮点数，fp16）相比，bfloat16具有以下特点：
    
    - **格式结构**：
        - **bfloat16**：1位符号位（sign bit），8位指数位（exponent bits），7位尾数位（mantissa bits）。
        - **fp16**：1位符号位，5位指数位，10位尾数位。
    - **优势**：
        - **动态范围广**：由于bfloat16的指数位与float32相同（都是8位），它能够表示的数值范围与float32非常接近。这使得bfloat16在处理深度学习模型中的梯度、权重和激活值时，能够更好地保留数值精度，避免因数值范围不足导致的溢出或下溢问题。
        - **内存效率高**：bfloat16仅占用16位内存，是float32的一半。这使得在相同内存带宽下，可以处理更多的数据，从而提高计算效率。
        - **计算速度快**：许多现代硬件（如Google的TPU、NVIDIA的Ampere架构GPU等）都针对bfloat16进行了优化，能够更快地执行bfloat16的运算。
    
    ### 2. 为什么使用bfloat16 for activations？
    
    在深度学习模型的训练过程中，****激活值（activations）**是指每一层的输出结果，在反向传播过程中用于计算梯度，并更新模型参数**。使用bfloat16来存储和计算激活值有以下优点：
    
    - **内存节省**：激活值在每一层计算后都需要存储，以便在反向传播时使用。使用bfloat16可以显著减少激活值占用的内存空间，这对于训练大型模型（如GPT、BERT等）尤为重要。
    - **计算效率高**：许多硬件加速器（如TPU）对bfloat16进行了优化，能够更快地执行bfloat16的运算。因此，使用bfloat16可以提高训练速度。
    - **数值稳定性**：尽管bfloat16的尾数位较少，但其广泛的动态范围使其在处理深度学习中的梯度、权重和激活值时，能够保持足够的数值精度，避免因数值范围不足导致的溢出或下溢问题。
    
    ### 3. float32与bfloat16的结合使用
    
    在训练过程中，通常会结合使用不同的数据类型：
    
    - **模型权重（weights）**：通常使用float32来存储和计算模型权重，以确保足够的数值精度。这是因为权重在训练过程中需要高精度来捕捉复杂的模式和数据特征。
    - **激活值（activations）**：使用bfloat16来存储和计算激活值，以节省内存和提高计算效率。由于bfloat16的动态范围与float32相近，它能够在保持数值稳定性的同时，提供高效的计算性能。
    
    ### 4. 实际应用中的效果
    
    结合使用float32和bfloat16可以在保证模型性能的同时，显著提高训练效率。例如：
    
    - **内存节省**：使用bfloat16可以将激活值占用的内存减少一半，这对于训练大型模型尤为重要。
    - **训练速度提升**：由于硬件加速器的优化，使用bfloat16可以加快计算速度，从而缩短训练时间。
    - **数值稳定性**：在大多数情况下，bfloat16能够提供足够的数值精度，不会对模型的最终性能产生负面影响。
    
    ### 总结
    
    - *"bfloat16 for activations"**指的是在训练过程中，使用bfloat16数据类型来存储和计算激活值。这种方法结合了float32和bfloat16的优点，既保证了模型的数值稳定性，又提高了内存使用效率和计算速度，是现代深度学习训练中的一种常见做法。

## DeepSeek：全新的MoE架构

作者指出当前方法存在两方面问题：

- **知识混合性**：现有的MoE模型通常使用数量有限的专家（如8个或16个），由于token的知识是丰富多样的，**将多样的知识分配给有限的专家，会导致特定专家的token很可能会涵盖多样化的知识，而使得专家变成一个杂糅多知识的专家**，这样不能充分发挥专家的专业效果。
- **知识冗余性**：**分配给不同专家的token可能存在共同知识。因此，多个专家可能会在其各自的参数中学习到共享知识，从而导致专家参数存在冗余。**这种问题也阻碍了现有MoE实践中专家的专业化，限制了MoE模型的理论上限性能。

### v1

针对上述问题，DeepSeek引入一种实现了专家专业化而设计的创新MoE架构。架构主要包含两方面优化：

- [**细粒度专家分割](https://zhida.zhihu.com/search?content_id=252735413&content_type=Article&match_order=1&q=%E7%BB%86%E7%B2%92%E5%BA%A6%E4%B8%93%E5%AE%B6%E5%88%86%E5%89%B2&zhida_source=entity)（Fine-Grained Expert Segmentation）**：**在保持参数数量不变的情况下，作者通过分割FFN中间隐藏维度来将专家分割成更细的粒度。**相应地，在保持计算成本不变的情况下，可激活更多细粒度的专家，以实现激活专家组合的更高灵活性。细粒度专家分割使得多样化的知识能够被更细致地分解，并更精确地学习到不同的专家中，每个专家将保持更高的专业化水平。
- [**共享专家隔离](https://zhida.zhihu.com/search?content_id=252735413&content_type=Article&match_order=1&q=%E5%85%B1%E4%BA%AB%E4%B8%93%E5%AE%B6%E9%9A%94%E7%A6%BB&zhida_source=entity)（Shared Expert Isolation）**：**将某些专家隔离出来，作为始终激活的共享专家，旨在捕获不同上下文中的共同知识。**通过将共同知识压缩到这些共享专家中，可以减轻其他路由专家之间的冗余，这可以提高参数效率，确保每个路由专家专注于不同方面而保持专业化。

如下图2所示。(b)是在(a)基础上，通过将隐层切分更细粒度，而形成细粒度专家，(c)又在(b)基础上隔离出来共享专家。DeepSeekMoE模型的演进过程，一直延续这两个创新的设置。

![image.png](MoE%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%201b8e64a566218027ad4bf217e06d1e1b/image%206.png)

DeepSeekMoE架构的公式形式：

![](https://pic4.zhimg.com/v2-e36071066e54bffbc3337599677c3fed_1440w.jpg)

![image.png](MoE%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%201b8e64a566218027ad4bf217e06d1e1b/image%207.png)

**m：将FFN中间的【隐藏维度】降为原来的1/m，然后将专家的数量变成mN个，保持总的参数量不变**

除了在模型架构上的改进，随着DeepSeek从V1 到 V3的演进，在负载均衡上，做了较多工作。首先看看 V1的负载均衡的优化，主要在计算负载均衡上做了优化，包括两个负载均衡的设置：

- **1.专家级负载loss(Expert-Level Balance Loss)**
    
    loss计算如下所示
    
    ![image.png](MoE%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%201b8e64a566218027ad4bf217e06d1e1b/image%208.png)
    
    ![image.png](MoE%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%201b8e64a566218027ad4bf217e06d1e1b/image%209.png)
    
    fi要乘以N’/K’的目的是**保持计算损失的恒定，不随专家数量的变化而变化**
    
    ![image.png](MoE%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%201b8e64a566218027ad4bf217e06d1e1b/image%2010.png)
    
    在公式中 T 表示要处理的总token量，在实际模型训练中，模型是按Batch接受输入的，那这个 T 总token量，到底是个什么口径？ 是实际样本总token量，还是随着Batch累加的量，亦或是每个Batch为一组的即时token量。
    
    我们来看看V1的[源码](https://link.zhihu.com/?target=https%3A//huggingface.co/deepseek-ai/deepseek-moe-16b-base/blob/main/modeling_deepseek.py%23L361)，**从源码中看，是以每个Batch为一组token计算负载loss的，** T **就是一个Batch的总token量**。
    
- **2.设备级负载loss(Device-Level Balance Loss)**
    
    用于调参D
    
    ![image.png](MoE%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%201b8e64a566218027ad4bf217e06d1e1b/image%2011.png)
    

### **DeepSeek V3 MoE升级**

- **1.MoE门控计算Softmax->Sigmoid**
    
    V3版MoE的计算框架如图7所示，相对于前两版的计算框架，**主要是将门控网络从Softmax 升级到了 Sigmoid。**
    
    ![](https://pic1.zhimg.com/v2-c1855b5bf223417ea58e44a7770a87b0_1440w.jpg)
    
    图7. DeepSeek V3 MoE网络层计算框架
    
    从实现门控的效果上看，Softmax和Sigmoid都能做实现筛选TopK的功能，也能做概率分布的归一化处理。
    
    **但V3版的MoE为什么要做从Softmax -> Sigmoid的升级？**
    
    要解释这个问题，我们看看V3版相对于V2版的专家设置发生了哪些变化。
    
    > V2版：路由专家数： 160， 激活专家数： 6个， 模型总参数67B，激活参数21B
    > 
    
    这里我个人理解：V3相对于V2的路由专家数增加了近100个，我们考虑在计算一个较大维度的softmax操作，softmax要在内部对所有维度的值做归一化处理，维度越大，会趋向于计算出的每个维度的值会越小，因为所有维度加和要等于1，所以维度越大，每个维度值理论上分配的值就越小。这样在选取 TopK 个最大值时，对更小的小数位会敏感，会有数据区分度不高的问题，维度越大，问题越严重。而选择Sigmoid函数，它是对每个专家分别计算一个 [0,1] 的打分，它并是不随专家维度变化而变化，理论上计算的打分值域更宽，区分度更高。所以V3版在配置更多路由专家的情况下，采用了值域更宽的Sigmoid的函数计算专家激活权重。
    
- **2.无辅助损失负载均衡（Auxiliary-Loss-Free Load Balancing）**
    
    DeepSeek在V1，V2版MoE模型中，增加了专家级，设备级和设备通信级等平衡负载辅助loss。这些辅助loss只是为了做计算、通讯的负载均衡，对模型的效果调优并没有帮助。甚至这些辅助loss增加过多，loss太大会对主模型造成影响，导致主模型的效果有损。为了减轻多辅助负载均衡的loss对主模型的影响，在V3版把多辅助loss都精简掉了，通过引入一个可动态调节的bias来做到负载均衡。
    
    具体方法：V2版选择专家是通过专家的门控权重 si,t 来取 TopK ，V3版对每个专家维护一个可动态调节的bias( bi )，现在选择专家通过si,t+bi 来选择 topK 个专家。当我们检测到专家是过载的状态时，我们减小该专家的 bi ，来降低门控权重，从而减少路由到该专家的token量；当我们检测到专家负载不足时，我们增加该专家的bias( bi )，来提升专家门控权重，从而增加路由到该专家的token量。
    
    > 这里论文中有些描述是比较含糊的，比如用什么方式检测专家过载或负载不足的，是用专家的平均分配的token数作为参考吗。我本来想通过看V3的源码理解下细节（[Model源码](https://link.zhihu.com/?target=https%3A//huggingface.co/deepseek-ai/DeepSeek-V3-Base/blob/main/modeling_deepseek.py)）但没有找到... ，只看到对于每个专家的bi设置成了一个可学习的参数。这个跟论文中描述的增加和减少固定的λ量也不一样。可能是我没找对位置，这块后面会再搜集些信息，理解下具体实现。
    > 
- **3. sequence粒度的负均衡损失（Complementary Sequence-Wise Auxiliary Loss）**
    
    DeepSeek V3也增加了一个sequence粒度的负载均衡损失，来**平衡单个sequence的token分配给每个专家**。如下图公式所示
    
    ![图8 Sequence-Wise Auxiliary Loss计算](https://pic3.zhimg.com/v2-47bc792afb21be4711ed03a18d802f72_1440w.jpg)
    
    图8 Sequence-Wise Auxiliary Loss计算
    
    相对于V1版的专家级辅助损失(Expert-Level Balance Loss)其实就是**作用粒度不一样**，Sequence-Wise的粒度是单条样本粒度的token做计算。Expert-Level Balance是一个Batch的多Sequence的token做计算。公式的计算形式并没有什么差异。
    
    最后DeepSeekV3也强调通过上面的负载均衡的策略，能达到一个非常好的平衡效果，所以**在V3版并没有Token被Drop掉。**
    

### 总结

- V1版为了兼顾对通用知识和细粒度领域知识的建模，引入了**共享专家（Shared Expert）和细粒度专家（Fine-Grained Expert）。**同时为了平衡各个专家的计算负载，引入了专家级负载loss （Expert-Level Balance Loss）和 设备级负载loss（Device-Level Balance Loss）。
- V2版主要在通信负载上做了些优化，通过引入**设备受限的专家路由机制**和**通信负载均衡loss**确保设备输入、输出的通信负载均衡。
- V3版考虑负载loss对主模型的优化会有影响，将辅助负载loss做了精简，通过在门控权重增加一个可调的bias来解决通信和计算的负载。也引入了一个更细粒度的sequence负载均衡loss。同时考虑随着路由专家增到256个，在门控权重计算上选择了值域更宽、打分差异更显著的sigmoid函数替换了原来的softmax函数。

![image.png](MoE%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%201b8e64a566218027ad4bf217e06d1e1b/image%2012.png)
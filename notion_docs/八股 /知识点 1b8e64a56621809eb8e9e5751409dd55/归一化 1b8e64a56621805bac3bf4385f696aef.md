# 归一化

State: 已完成

https://blog.csdn.net/sinat_37574187/article/details/140846061

## 归一化维度：**批归一化和层归一化**

### **Batch Norm 批归一化：**

在批次内所有样本的单个通道上进行归一化。
适用场景：适合大批量数据，适用于大多数神经网络模型，特别是在卷积神经网络（CNN）和全连接网络（FCN）中广泛使用。

### **Layer Norm 层归一化：**

**Transformer使用层归一化，目的是保留序列中的token位置信息**

在单个样本的所有通道上归一化。
适用场景：适合变长序列模型。主要用于循环神经网络（RNN）、自注意力模型（如 Transformer）等序列模型。

https://blog.csdn.net/weixin_43221845/article/details/142733230

### **batch norm 和 layer norm 的区别**

BatchNorm的思路：

![image.png](%E5%BD%92%E4%B8%80%E5%8C%96%201b8e64a56621805bac3bf4385f696aef/image.png)

LayerNorm的思路：

![image.png](%E5%BD%92%E4%B8%80%E5%8C%96%201b8e64a56621805bac3bf4385f696aef/image%201.png)

BatchNorm：对每一个batch进行操作，使得对于这一个batch中所有的输入数据，它们的每个特征都是均值为0，方差为1的分布。在BN后，需要再加一个线性变换操作，让数据恢复其表达能力(让模型学习参数γ 和 β)。

LayerNorm：整体做法类似于BN，不同的是LN不是在特征间进行标准化操作（横向操作），而是在整条数据间进行标准化操作（纵向操作）。

**BN和LN的区别**：BN和LN的作用对象不同，**BatchNorm认为相同维的特征具有相同分布，在特征维度上开展归一化操作，归一化的结果保持样本之间的可比较性**。而LayerNorm认为**每个样本内的特征具有相同分布，因此针对每一个样本进行归一化处理**，保持相同样本内部不同对象的可比较性。

### **为什么TF使用LayerNorm而不是BatchNorm？**

首先，NLP数据中由于每条样本可能不一样长，会使用padding，如果对padding部分进行normalization，对效果有负面影响。
直观来说，batchnorm会对同一个特征以batch为组进行归一化，而对于文本数据，同一个位置的token很可能是没有关联的两个token，对这样一组数据进行归一化没有什么实际意义。

《PowerNorm: Rethinking Batch Normalization in Transformers》论文的实验也表明，在NLP数据使用batchnorm，均值和方差相对layernorm会更加震荡，因此效果欠佳。

v：BN 是对样本内部某特征的缩放，LN 是样本直接之间所有特征的缩放。为啥BN不适合NLP ？是因为NLP模型训练里的每次输入的句子都是多个句子，并且长度不一，那么 针对每一句的缩放才更加合理，才能表达每个句子之间代表不同的语义表示，这样让模型更加能捕捉句子之间的上下语义关系。如果要用BN，它首先要面临的长度不一的问题。有时候batch size 越小的bn 效果更不好。

## 归一化位置：prenorm与postnorm

![image.png](%E5%BD%92%E4%B8%80%E5%8C%96%201b8e64a56621805bac3bf4385f696aef/image%202.png)

![image.png](%E5%BD%92%E4%B8%80%E5%8C%96%201b8e64a56621805bac3bf4385f696aef/image%203.png)

层前归一化(Baevski and Auli, 2018;Child等人，2019;Wang等人，2019)(PreNorm)在剩余连接和注意机制之前应用归一化层，证明了大型语言模型的稳定性和性能增强。由于**PreNorm允许梯度通过残余连接更直接地从输出流向输入，在一定程度上绕过子层，因此它降低了模型的有效深度。**相比之下，Post Layer Normalization(Wang et al.， 2019) (PostNorm)在残差连接和注意机制之后进行归一化，从而保留了模型的有效深度。然而，**PostNorm可能容易出现梯度消失和爆炸**，这在训练大型语言模型时提出了重大挑战。**大多数现有的大型语言模型主要使用PreNorm，因为在传统的Transformer架构中，更宽和更深的网络之间的性能差异通常可以忽略不计，并且训练稳定性是优先考虑的。**

**minimax使用层后归一化：**实验的模型有93亿个激活参数，共有600亿个参数，每个参数由48层组成，采用不同的归一化方法。这两个模型都在5000亿个token上进行了训练。对于PostNorm，我们使用**DeepNorm** (Wang et al.， 2024a)来确保更稳定的训练。如表5所示，PostNorm在所有评估指标上的表现始终优于PreNorm。

## 归一化函数

### **Layer Norm 层归一化：**

![image.png](%E5%BD%92%E4%B8%80%E5%8C%96%201b8e64a56621805bac3bf4385f696aef/image%204.png)

### **DeepNorm：改善 postnorm 的梯度传播**

DeepNorm在LayerNorm之前对残差链接进行up-scale(𝛼>1)，以扩大残差连接。

在Xavier参数初始化阶段对模型参数进行down-scale(𝛽<1),以减小部分参数的初始化范围。

计算公式：

![image.png](%E5%BD%92%E4%B8%80%E5%8C%96%201b8e64a56621805bac3bf4385f696aef/image%205.png)

![image.png](%E5%BD%92%E4%B8%80%E5%8C%96%201b8e64a56621805bac3bf4385f696aef/image%206.png)

alpha和beta都是由模型结构决定的常数

DeepNorm的优点：

a.DeepNorm可以**缓解爆炸式模型更新**的问题，**把模型更新限制在常数，使得模型训练过程更稳定**。具体地，Deep Norm方法在执行Layer Norm之前，**up-scale了残差连接（ 𝛼>1）；另外，在初始化阶段down-scale了模型参数( 𝛽<1 )。**

b.DeepNorm 通过引入**多层归一化**操作，可以**改善梯度传播**、解决DNN模型训练中梯度消失和梯度爆炸问题。**归一化操作可以减小数据的分布差异，也可以减少对学习率的敏感性，提高泛化能力**。

### **RMSNorm：用特征平方均值开方作为缩放因子**

计算公式：

![image.png](%E5%BD%92%E4%B8%80%E5%8C%96%201b8e64a56621805bac3bf4385f696aef/image%207.png)
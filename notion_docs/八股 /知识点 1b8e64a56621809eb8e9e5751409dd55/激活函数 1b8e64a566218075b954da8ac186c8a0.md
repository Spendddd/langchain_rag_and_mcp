# 激活函数

State: 学习中

https://blog.csdn.net/sinat_37574187/article/details/140846061

sigmoid、tanh、RELU系列、SiLU（swish）、GELU、GLU系列

- **SwiGLU 激活函数公式**
    
    首先需要明确的是SwiGLU激活函数是GLU的变体。
    
    GLU（Gated Linear Units）是一种神经网络层，其定义涉及到输入的两个线性变换的逐元素积，其中一个经过了 sigmoid 函数的处理。GLU 包含以下组件：
    
    - 神经网络的输入（x）；
    - 用于进行线性变换的权重矩阵 W 和 V；
    - 用于调整线性变换的偏置项 b 和 c；
    - 将第一个线性变换（xW + b）通过 sigmoid 函数处理。
    
    GLU的公式如下：
    
    ![](https://pic2.zhimg.com/v2-e2a2b9d3b8771ab6f5421b377d8544e1_1440w.jpg)
    
    其中：
    
    - σ 表示 sigmoid 函数；
    - 线性变换之间进行逐元素乘积（Hadamard 乘积）。
    
    这个定义的核心在于将输入通过两个线性变换，然后将其中一个变换通过 sigmoid 激活函数，最后取逐元素乘积，得到最终的输出。
    
    GLU（Gated Linear Units）变体是通过在 GLU 的定义中替换激活函数或者引入其他变化来得到的。
    
- **SwiGLU 的优势**
    
    **平滑的非线性**: Swish 函数提供了一种平滑的非线性激活，与 ReLU 相比，避免了 ReLU 的死区问题。
    
    **门控机制**: GLU 提供了一种门控机制，使得模型能够选择性地传递信息，这在处理复杂的模式时非常有用。
    
    **增强表达能力**: 结合 Swish 和 GLU 的优点，SwiGLU 能够增强模型的表达能力和性能，特别是在深度神经网络中。
    

SwiGLU激活函数通过门控机制和非线性变换的组合，提供了一种灵活的非线性激活方式，有助于改善神经网络的性能和学习能力。
# softmax注意力

State: 已完成

# softmax注意力

## 为什么要对注意力分数用dk的平方根缩放

https://blog.csdn.net/Zzzzyc_/article/details/140420229

## 为什么要用不同的矩阵计算向量作为q、k、v

https://blog.csdn.net/zwqjoy/article/details/120285123

Q就是词的查询向量，K是“被查”向量，V是内容向量。

简单来说一句话：Q是最适合查找目标的，K是最适合接收查找的，V就是内容，这三者不一定要一致，所以网络这么设置了三个向量，然后学习出最适合的Q, K, V，以此增强网络的能力。

因此肯定需要给每个input定义tensor，然后通过tensor间的乘法来得到input之间的关系。那这么说是不是给每个input定义1个tensor就够了呢？不够啊！如果每个input只有一个相应的q，那么q1和q2之间做乘法求取了a1和a2的关系之后，这个结果怎么存放怎么使用呢？而且**a1和a2之间的关系是对偶的吗？如果a1找a2和a2找a1有区别怎么办**？只定义一个这模型是不是有点太简单了。

![image.png](softmax%E6%B3%A8%E6%84%8F%E5%8A%9B%201b8e64a56621806aa3e7eb5f83fdb43d/image.png)

一个不够就定义两个，于是有了q和k。**q你可以理解为代表自己用的，用q去和别的输入找关系；k理解为给别人用的，专门对付来跟你找关系的输入。**这样子，用自己的q去和别人的k（当然和自己的k也行）做乘法，就可以得到找出的关系：权重a了。

![image.png](softmax%E6%B3%A8%E6%84%8F%E5%8A%9B%201b8e64a56621806aa3e7eb5f83fdb43d/image%201.png)

仅定义q和k两个够吗？可能也还是不够的。找出来的关系是要用的，不用等于白找。权重a是要对输入信息做加权，才能体现找到的关系的价值的。那跟输入直接加权行吗？这么做也不是不行，就是显得直接和生硬了点。所以又定义了个v。要知道，v和q、k一样，都是通过系数矩阵对输入a做乘法得到的。所以**定义了个v大概等于又对a加了一层可以学习的参数，然后对经过参数调整后的a再去做加权、把通过注意力机制学到的关系给用上**。所以，a和v的乘法进行加权操作，最终得到输出o。

![image.png](softmax%E6%B3%A8%E6%84%8F%E5%8A%9B%201b8e64a56621806aa3e7eb5f83fdb43d/image%202.png)

综上，定义这3个tensor，**一方面是为了学习输入之间的关系、找到和记录谁和谁的关系权重**，一方面也是**在合理的结构下引入了可学习的参数，使得网络具有更强的学习能力**。

假设我们想查一篇文章，我们不会直接把文章的内容打上去，而是会在搜索框输入该文章的关键字，如果我们搜不到，我们往往会再换一个关键字，直到搜到为止，那么可以让我们搜到的关键字就是最适合查找目标文章的关键字。这个最适合查找目标文章的关键字就是Q。

那么搜索引擎拿到我们输入的关键字Q之后，就会把Q和库里面的文章对比，当然搜索引擎为了节省资源加快对比速度，提前把库里面的文章进行了处理提取了关键信息，关键信息有很多，那么那个关键信息能够使得搜索命中率高，那个就是最适合接收查找的关键信息，这个最适合接收查找的关键信息就是K。

使用Q和K计算了相似度之后得到score，这就是相似度评分，之后有了相似度评分，就可以把内容V加权回去了。

由于计算Q、K、V的矩阵是可以学习的，因此网络可以自己学习出要怎么样安排Q、K、V。

既然K和Q差不多（唯一区别是W_k和W_Q权值不同），直接拿K自己点乘就行，何必再创建一个Q？

1. 先从点乘的物理意义说，两个向量的点乘表示两个向量的相似度。
2. Q，K，V物理意义上是一样的，都表示同一个句子中不同token组成的矩阵。矩阵中的每一行，是表示一个token的word embedding向量。

简单的说，K和Q的点乘是为了计算一个句子中每个token相对于句子中其他token的相似度，这个相似度可以理解为attetnion score，关注度得分。

这个attention score是一个(6, 6)的矩阵。每一行代表每个token相对于其他token的关注度。

**虽然有了attention score矩阵，但是这个矩阵是经过各种计算后得到的，已经很难表示原来的句子了。**然而V还代表着原来的句子，所以我们拿这个attention score矩阵与V相乘，得到的是一个加权后结果。也就是说，**原本V里的各个单词只用word embedding表示，各个单词相互之间没什么关系。但是经过与attention score相乘后，V中每个token的向量（即一个单词的word embedding向量），在300维的每个维度上（每一列）上，都会对其他token做出调整（关注度不同）。**与V相乘这一步，相当于提纯，让每个单词关注该关注的部分。

好了，该解释为什么不把K和Q用同一个值了。

经过上面的解释，我们知道K和Q的点乘是为了得到一个attention score 矩阵，用来对V进行提纯。**K和Q使用了不同的W_k, W_q来计算，可以理解为是在不同空间上的投影**。**正因为有了这种不同空间的投影，增加了表达能力，这样计算得到的attention score矩阵的泛化能力更高**。这里解释下我理解的泛化能力，因为K和Q使用了不同的W_k, W_q来计算，得到的也是两个完全不同的矩阵，所以表达能力更强。

但是如果不用Q，直接拿K和K点乘的话，你会发现attention score 矩阵是一个对称矩阵。**如果令Q=K，那么得到的模型大概率会得到一个类似单位矩阵的attention矩阵，这样self-attention就退化成一个point-wise线性映射。**因为是**同样一个矩阵，都投影到了同样一个空间，所以泛化能力很差**。这样的矩阵导致对V进行提纯的时候，效果也不会好。
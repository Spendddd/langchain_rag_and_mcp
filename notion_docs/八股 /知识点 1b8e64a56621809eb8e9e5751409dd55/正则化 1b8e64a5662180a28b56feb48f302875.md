# 正则化

State: 已完成

https://zhuanlan.zhihu.com/p/4536946192：**算法面经系列：L1和L2正则化**

https://zhuanlan.zhihu.com/p/664962747：**常见的正则化方法及对比分析**

# 常见正则化技术

## 1. L1正则化（[Lasso回归](https://zhida.zhihu.com/search?content_id=249929658&content_type=Article&match_order=1&q=Lasso%E5%9B%9E%E5%BD%92&zhida_source=entity)）:

正则化项是模型参数的绝对值之和。公式可以表示为：

![image.png](%E6%AD%A3%E5%88%99%E5%8C%96%201b8e64a5662180a28b56feb48f302875/image.png)

因此，L1正则化的损失函数为：

![image.png](%E6%AD%A3%E5%88%99%E5%8C%96%201b8e64a5662180a28b56feb48f302875/image%201.png)

**L1正则化会导致模型的参数趋向稀疏化**，即有些参数被压缩到零，从而有**特征选择**的效果。

## 2. L2正则化（[Ridge回归](https://zhida.zhihu.com/search?content_id=249929658&content_type=Article&match_order=1&q=Ridge%E5%9B%9E%E5%BD%92&zhida_source=entity)、岭回归）:

正则化项是模型参数的平方和。公式可以表示为：

![image.png](%E6%AD%A3%E5%88%99%E5%8C%96%201b8e64a5662180a28b56feb48f302875/image%202.png)

因此，L2正则化的损失函数为：

![image.png](%E6%AD%A3%E5%88%99%E5%8C%96%201b8e64a5662180a28b56feb48f302875/image%203.png)

**L2正则化会将模型参数压缩到接近零，但一般不为零，因此不会进行特征选择**。

## **3. Elastic Net**

- **Elastic Net 是L1和L2正则化的结合**，它同时包含L1和L2的惩罚项。
- 这种方法在处理高度相关的特征时尤其有用。
- 惩罚项为：
    
    ![image.png](%E6%AD%A3%E5%88%99%E5%8C%96%201b8e64a5662180a28b56feb48f302875/image%204.png)
    
     ，其中 α 是控制L1和L2之间平衡的参数。
    

## **4. Dropout**

Dropout是在神经网络过程中随机丢掉一部分神经元来减少神经网络的复杂度，从而防止过拟合现象，Dropout实现方法很简单，**在每次迭代训练的时候，以一种概率随机屏蔽每一层中某一个神经元**，用余下的神经元所构成的网络来继续训练，这种**随机概率根据实现的目标来调整**，这种优点可**以加强神经网络的鲁棒性，并且能有效减少神经网络的复杂性**。下图是Dropout示意图：

![image.png](%E6%AD%A3%E5%88%99%E5%8C%96%201b8e64a5662180a28b56feb48f302875/image%205.png)

## **5. Early Stopping**提前终止

提前停止(early stopping)是一种训练复杂机器学习模型时避免过拟合的方法。它通过**监测在单独的测试数据集上训练模型的表现，并且观察到一旦在固定数量的训练迭代之后测试数据集上的表现没有得到改善，就会停止训练过程**。通过尝试自动选出测试数据集上的表现开始降低而训练数据集上的表现继续提高这样的过拟合发生迹象拐点来避免过拟合。表现度量可以是通过训练模型而进行优化的损失函数（例如对数损失函数（logarithmic loss）），或者通常情况下问题所关注的外部指标（例如分类精度）。

![image.png](%E6%AD%A3%E5%88%99%E5%8C%96%201b8e64a5662180a28b56feb48f302875/image%206.png)

在上图中，可以在a点停止训练，因为迭代次数增加，训练的损失函数减少的很少，但是在a点之后测试集的损失反而增加了，因为在此处之后模型会开始在训练数据上过拟合。

## **6. Data Augmentation**

- 数据增强不是直接的正则化技术，但**它通过生成额外的训练样本（通常是通过对现有数据进行变换）来增加数据的多样性**，从而间接地帮助减少过拟合。
- 比如将一张猫的图片，水平翻转生成一张新的图片，或是随机缩放裁剪图片。这样可以扩充数据集。

## 7. Weight Decay

**Weight Decay 实际上就是L2正则化的一个别名**，因为它同样是在优化过程中逐渐减小权重的大小。

## 8. Batch Normalization

虽然Batch Normalization的主要目的是加速训练和稳定学习过程，但它也有一定的正则化效果，因为每个batch的均值和方差都是随机的，这在**一定程度上类似于Dropout的效果**。

# 为什么正则化可以预防过拟合？

正则化可以预防过拟合，主要是因为它通过引入额外的约束或惩罚机制来限制模型的复杂度。过拟合通常发生在模型过于复杂，以至于它不仅学习到了数据中的有用模式（即泛化能力），还“记住”了训练数据中的噪声和细节，这使得模型在未见过的数据上的表现变差。正则化通过以下几种方式帮助缓解这个问题：

## 简化模型

正则化通过添加一个与模型权重大小相关的惩罚项到损失函数中，迫使模型在优化过程中选择较小的权重值。**较小的权重意味着模型对输入特征的依赖性降低，从而降低了模型的复杂度**。这种简化有助于模型更好地泛化，因为简单模型更有可能捕捉到数据中的基本规律，而不是特定于训练集的噪声。

## 增加偏差，减少方差

机器学习中的偏差-方差权衡指的是模型的预测误差可以分解为两部分：偏差（bias）和方差（variance）。**高偏差意味着模型过于简单，无法很好地拟合训练数据；高方差意味着模型过于复杂，对训练数据的变化非常敏感，容易过拟合。正则化通过增加一些偏差（因为模型变得更加简单），来显著地减少方差，从而达到更好的泛化性能。**

## 避免极端权重

在没有正则化的情况下，某些权重可能会变得非常大，尤其是在网络层数较多或者特征数量庞大的情况下。**这些大的权重会导致模型对某些输入特征过于敏感，从而影响其稳定性**。L2正则化（Ridge）通过惩罚大的权重，使得所有权重都趋向于较小的数值，避免了这种情况的发生。L1正则化（Lasso）则倾向于将一些权重直接缩减为零，从而实现特征选择，进一步简化模型。

## 提高模型的鲁棒性

正则化可以帮助提高模型对不同数据分布的鲁棒性。例如，**Dropout作为一种正则化技术，在训练期间随机丢弃一部分神经元，这相当于训练多个不同的“子网络”。**当这些子网络在训练时看到不同的数据样本，它们共同学习到的特征会更加鲁棒，不容易受到个别样本的影响。**在测试阶段，所有的神经元都被保留下来，但输出通常会被平均，以反映所有子网络的决策。**

## 减少对训练数据的过度依赖

正则化通过引入额外的约束，使得模型不仅仅依赖于训练数据中的具体模式。比如，Early Stopping会在验证集性能开始下降时提前终止训练，防止模型继续学习训练数据中的噪声。Data Augmentation通过生成额外的训练样本来扩大数据集，让模型能够学习到更多关于数据分布的信息，而不仅仅是训练集本身。

## 平滑决策边界

对于分类问题，正则化可以使决策边界更加平滑，而不是过于曲折，后者往往是过拟合的表现。**平滑的决策边界意味着模型不会对输入空间中的小变化做出剧烈反应**，从而提高了对新数据点的预测准确性。
# 深度学习：CNN、RNN、lstm

# **卷积神经网络（CNN）**

通过卷积和池化操作有效地处理高维图像数据，降低计算复杂度，并提取关键特征进行识别和分类。

### **网络结构**

- **卷积层：**用来提取图像的局部特征。
- **池化层：**用来大幅降低参数量级，实现数据降维。
- **全连接层：**用来输出想要的结果。

![image.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9ACNN%E3%80%81RNN%E3%80%81lstm%201cee64a5662180a7b7f5dd9d0fe82e3e/image.png)

### **解决问题**

- **提取特征：**卷积操作提取图像特征，如边缘、纹理等，保留图像特征。
- **数据降维：**池化操作大幅降低参数量级，实现数据降维，大大减少运算量，避免过拟合。

### **工作原理**

- **卷积层：**通过卷积核的过滤提取出图片中局部的特征，类似初级视觉皮层进行初步特征提取。

![*使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值*](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9ACNN%E3%80%81RNN%E3%80%81lstm%201cee64a5662180a7b7f5dd9d0fe82e3e/image%201.png)

*使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值*

- **池化层：**下采样实现数据降维，大大减少运算量，避免过拟合。

![*原始是20×20的，进行下采样，采样为10×10，从而得到2×2大小的特征图*](https://developer.qcloudimg.com/http-save/yehe-11029171/a1e07b2a2a20eb9f9713839964229c31.png)

*原始是20×20的，进行下采样，采样为10×10，从而得到2×2大小的特征图*

- **全连接层：**经过卷积层和池化层处理过的数据输入到全连接层，得到最终想要的结果。

![](https://developer.qcloudimg.com/http-save/yehe-11029171/2644b313877c423987557b98b5df5e63.png)

## 经典网络：LeNet

**LeNet-5通过引入卷积层、池化层和全连接层等关键组件，构建了一个高效且强大的图像识别网络，为后续卷积神经网络的发展奠定了基础。**

- 输入层：INPUT
- 三个卷积层：C1、C3和C5
- 两个池化层：S2和S4
- 一个全连接层：F6
- 输出层：OUTPUT

![*输入层-卷积层-池化层-卷积层-池化层-卷积层-全连接层-输出层*](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9ACNN%E3%80%81RNN%E3%80%81lstm%201cee64a5662180a7b7f5dd9d0fe82e3e/image%202.png)

*输入层-卷积层-池化层-卷积层-池化层-卷积层-全连接层-输出层*

### **实际应用**

- **图像分类：**可以节省大量的人工成本，将图像进行有效的分类，分类的准确率可以达到95%+。**典型场景：图像搜索。**
- **目标定位：**可以在图像中定位目标，并确定目标的位置及大小。**典型场景：自动驾驶。**
- **目标分割：**简单理解就是一个像素级的分类。**典型场景：视频裁剪。**
- **人脸识别：**非常普及的应用，戴口罩都可以识别。**典型场景：身份认证。**

# **RNN**

**循环神经网络（RNN）：一种能处理序列数据并存储历史信息的神经网络，通过利用先前的预测作为上下文信号，对即将发生的事件做出更明智的决策。**

### **网络结构**

- **输入层：**接收输入数据，并将其传递给隐藏层。输入不仅仅是静态的，**还包含着序列中的历史信息。**
- **隐藏层：核心部分，捕捉时序依赖性**。隐藏层的输出不仅取决于当前的输入，还**取决于前一时刻的隐藏状态。**
- **输出层：**根据隐藏层的输出生成最终的预测结果。

![image.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9ACNN%E3%80%81RNN%E3%80%81lstm%201cee64a5662180a7b7f5dd9d0fe82e3e/image%203.png)

### **解决问题**

- **序列数据处理：**RNN能够**处理多个输入对应多个输出**的情况，尤其适用于序列数据，如时间序列、语音或文本，其中每个输出**与当前的及之前的输入都有关**。
- **循环连接：**RNN中的循环连接使得网络能够**捕捉输入之间的关联性**，从而利用先前的输入信息来影响后续的输出。

### **工作原理**

- **输入层：**先对句子“what time is it ？”进行分词，然后按照顺序输入。
- **隐藏层：**在此过程中，我们注意到前面的所有输入都对后续的输出产生了影响。圆形隐藏层不仅考虑了当前的输入，还综合了之前所有的输入信息，**能够利用历史信息来影响未来的输出**。
    
    ![image.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9ACNN%E3%80%81RNN%E3%80%81lstm%201cee64a5662180a7b7f5dd9d0fe82e3e/image%204.png)
    
- **输出层：**生成最终的预测结果：Asking for the time。

![image.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9ACNN%E3%80%81RNN%E3%80%81lstm%201cee64a5662180a7b7f5dd9d0fe82e3e/image%205.png)

![image.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9ACNN%E3%80%81RNN%E3%80%81lstm%201cee64a5662180a7b7f5dd9d0fe82e3e/image%206.png)

### **应用场景**

**（1）处理数据**

- **文本数据：**处理文本中单词或字符的时序关系，并进行文本的分类或翻译。
- **语音数据：**处理语音信号中的时许信息，并将其转换为相应的文本。
- **时间序列数据：**处理具有时间序列特征的数据，如股票价格、气候变化等。
- **视频数据：**处理视频帧序列，提取视频中的关键特征。

**（2）实际应用**

- **文本生成：**填充给定文本的空格或预测下一个单词。**典型场景：对话生成。**
- [**机器翻译](https://cloud.tencent.com/product/tmt?from_column=20065&from=20065)：**学习语言之间的转换规则，并自动翻译。**典型场景：在线翻译。**
- **语音识别：**将语音转换成文本。**典型场景：语音助手。**
- **视频标记：**将视频分解为一系列关键帧，并为每个帧生成内容匹配的文本描述。**典型场景：生成视频摘要。**

# **LSTM**

**长短期记忆网络（LSTM）：一种特殊的循环神经网络，通过引入内存块和门控机制来解决梯度消失问题，从而更有效地处理和记忆长期依赖信息。***（RNN的优化算法）*

### **网络结构**

1. **细胞状态（Cell state）：**负责保存长期依赖信息。
2. **门控结构：**每个LSTM单眼包含三个门：输入门、遗忘门和输出门。
3. **遗忘门（Forget Gate）：**决定从细胞状态中丢弃哪些信息。
4. **输入门（Input Gate）：**决定哪些新信息被加入到细胞状态中。
5. **输出门（Output Gate）：**基于细胞状态决定输出的信息。

![](https://developer.qcloudimg.com/http-save/yehe-11029171/80ee6ceb8e208302ad76ce93c3a03cc3.png)

### **解决问题**

- **短时记忆：**RNN难以捕捉和利用序列中的长期依赖关系，从而限制了其在处理复杂任务时的性能。
- **梯度消失/梯度爆炸：**在RNN的反向传播过程中，梯度会随着时间步的推移而逐渐消失（变得非常小）或爆炸（变得非常大）。

### **工作原理**

![](https://developer.qcloudimg.com/http-save/yehe-11029171/b8f1c07a6e92dd0784b24a55f87816dd.png)

RNN 存在**梯度消失和梯度爆炸**问题，导致无法学习长期依赖关系。因此，LSTM 通过**门控机制**来解决这个问题。

LSTM 通过**三个门（遗忘门、输入门、输出门）**和一个**记忆单元（Cell State）**来控制信息的流动，使得网络能够记住长期信息，同时避免梯度消失问题。

![image.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9ACNN%E3%80%81RNN%E3%80%81lstm%201cee64a5662180a7b7f5dd9d0fe82e3e/image%207.png)

- **公式汇总**（可以按照后续遗忘门、输入门、记忆单元、输出门进行记忆）

![image.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9ACNN%E3%80%81RNN%E3%80%81lstm%201cee64a5662180a7b7f5dd9d0fe82e3e/image%208.png)

- **维度变化**
    
    ![image.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9ACNN%E3%80%81RNN%E3%80%81lstm%201cee64a5662180a7b7f5dd9d0fe82e3e/image%209.png)
    
- **其他说明**
    
    ![image.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9ACNN%E3%80%81RNN%E3%80%81lstm%201cee64a5662180a7b7f5dd9d0fe82e3e/image%2010.png)
    

## 公式解释

### 输出门

- 公式
    
    ![image.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9ACNN%E3%80%81RNN%E3%80%81lstm%201cee64a5662180a7b7f5dd9d0fe82e3e/image%2011.png)
    
- **参数解释**
    
    ![image.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9ACNN%E3%80%81RNN%E3%80%81lstm%201cee64a5662180a7b7f5dd9d0fe82e3e/image%2012.png)
    
- **作用**
    
    ![image.png](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9ACNN%E3%80%81RNN%E3%80%81lstm%201cee64a5662180a7b7f5dd9d0fe82e3e/image%2013.png)
    

## LSTM优缺点与适用场景

### 优点

- 能处理长时间依赖：LSTM 通过 遗忘门、输入门、输出门 机制，可以记住重要的历史信息，忽略无关的过去数据。
- 能处理非线性时间序列：比传统统计方法（如 ARIMA）更适用于复杂模式的数据，如金融市场、语音识别等。
- 自动特征学习：不需要手动提取特征，LSTM 可以从数据中自动学习重要模式。
- 适用于高维时间序列：如多变量天气预测（温度、湿度、气压等多个变量影响）。
- 能处理不规则时间间隔数据：不像 ARIMA 需要等间隔数据，LSTM 能适应不规则时间序列。

### 缺点

- 计算成本高：LSTM 的参数较多，训练时间长，计算量比传统时间序列模型大。需要 GPU 加速才能高效训练。
- 对短期数据可能表现不佳：在数据量少或模式变化快的情况下，LSTM 可能无法比简单的 ARIMA 或回归方法表现更好。
- 超参数调整困难：需要调整隐藏层大小、时间步数、学习率等参数，调整不当可能导致模型过拟合或欠拟合。
- 黑盒特性：传统时间序列模型（如 ARIMA）有清晰的数学公式，而 LSTM 的计算过程较复杂，难以解释其内部逻辑。

### **适用场景**

LSTM **适用于 处理时间依赖性强、长时间依赖、非线性、复杂模式的数据**。典型应用场景如下

![](https://i-blog.csdnimg.cn/direct/161c0b2cc5ba489c8b6c063992dc21b9.png)

### **LSTM vs 其他时间序列模型**

![](https://i-blog.csdnimg.cn/direct/ae74c055e0af4b108faaa1aafaa6fedd.png)
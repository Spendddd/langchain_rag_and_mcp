# 位置编码

State: 已完成

### **为什么正弦-余弦位置编码是有效的？**

### **2.1 周期性提供相对位置信息**

**正弦和余弦函数具有周期性，这意味着它们可以为不同的位置提供不同的编码，同时保持一定的周期性模式。**这种周期性使得模型能够学习到相对位置关系。例如，位置 pos 和位置 pos+k 之间的关系可以通过正弦和余弦函数的性质来捕捉。

### **2.2 可学习的相对位置表示**

虽然位置编码是固定的，但 Transformer 模型可以通过训练学习到如何利用这些位置编码来理解序列的顺序。具体来说，模型可以通过注意力机制和前馈神经网络来捕捉位置编码中的信息，从而理解单词之间的相对位置关系。

### **2.3 与输入嵌入的兼容性**

位置编码被直接添加到输入嵌入中，这意味着位置信息被整合到模型的输入中。这种简单的加法操作使得位置编码与输入嵌入无缝结合，而不需要额外的复杂机制。

### **2.4 外推能力**

由于位置编码是基于周期性函数的，它们可以自然地外推到比训练时看到的序列长度更长的序列。这意味着 Transformer 模型在处理比训练时更长的序列时，仍然能够保持一定的位置感知能力。

### **2.5 计算效率**

使用正弦和余弦函数生成位置编码是计算高效的，因为这些函数可以预先计算并存储，或者在需要时动态生成。这避免了为每个位置学习独立的位置嵌入，从而节省了内存和计算资源。

### **3. 其他位置编码方法**

除了正弦和余弦函数生成的位置编码外，还有其他一些位置编码方法：

- **学习的位置嵌入（Learned Positional Embeddings）**：
    - 为每个位置学习独立的位置嵌入。
    - 优点：可以捕捉更复杂的位置关系。
    - 缺点：需要为每个位置存储独立的位置嵌入，可能导致内存消耗较大，并且难以外推到更长的序列。
- **相对位置编码（Relative Positional Encoding）**：
    - 关注的是单词之间的相对位置，而不是绝对位置。
    - 优点：更符合人类的语言理解方式，因为语言中的语法和语义关系往往是相对的。
    - 缺点：实现相对复杂，需要对注意力机制进行修改。
- **旋转位置编码（Rotary Positional Encoding, RoPE）**：
    - 通过旋转操作将位置信息整合到注意力机制中。
    - 优点：保持了旋转不变性，并且在长序列处理中表现出色。
    - 缺点：实现相对复杂。

### RoPE：用绝对位置表示相对位置

https://zhuanlan.zhihu.com/p/8306958113：**[通俗易读]无痛理解旋转位置编码RoPE（数学基础，理论(复数的指数表达，矩阵，几何意义)，代码，分析）**

RoPE分两种：**GPT-NeoX Style**：q0和qd/2一组；**GPT-J style：**和原文一致，相邻两个为一组

![image.png](%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%201b8e64a5662180388464ccb99a8227f1/image.png)

> 2指的“远程衰减”不是位置层面的，是隐藏层维度层面的
> 

基础角度计算代码：

inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float().to(device) / dim))

基数（base，上述图中默认为10000）影响衰减的范围，基数越大，衰减的越慢，因此，更长的文本，需要更大的base。

形式上定义应用了RoPE **q**和**k内积如下：**

![image.png](%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%201b8e64a5662180388464ccb99a8227f1/image%201.png)

![image.png](%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%201b8e64a5662180388464ccb99a8227f1/image%202.png)

![image.png](%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%201b8e64a5662180388464ccb99a8227f1/image%203.png)

图4：训练过程中，不同 i 的训练图示

- Rope外推
    
    是指解决在短序列上训练的模型，在在长文本上推理的问题。为了达到外推，有很多人提出很多种思路，下面一一介绍。
    
    **1、位置内插(Position Interpola)：**
    
    如果在训练过程中：
    
    训练集的长度是L:[1,2,3,4,5,6,...L]
    
    预测的时候长度是4L：[1,2,3,4,...L,L+1,....4L]
    
    那么因为L+1的位置没有被训练过，所以导致出现OOD情况，表现不好。那么我们可以将所有的位置，都缩小4倍，缩小到训练过的范围内：[1/4,2/4,3/4,1,...L/4,(L+1)/4,...L]
    
    那么其就可以避免超过L的位置编码的OOD问题。但是：尽管位置内插避免了远处的位置越界问题，**因为它会阻止神经网络区分非常接近的token的顺序和位置，严重扰乱了模型的局部分辨率,导致其效果不行。**因为这种破坏，这种方法免训练外推的效果仍然很差。但是在位置内插的基础上，在训练一段长文本，可以比什么都不做直接在长文本上训练，收敛更快。
    
    **2、NTK-Aware ：（RoPE-ABF（Adjusted Base Frequency）**
    
            **作者认为：**对位置编码的所有维度只进行简单的内插（除一个常数），丢失重要的高频细节，而网络需要这些细节来解析非常相似且非常接近的标记。因此，它提出一种非线性内插。
    
            作者根据NTK理论：需要进行“高频外推（对高频变动小，让其保持原本不变）、低频内插（对低频变动大，让其内插）”。
    
    **高频和低频的定义：**
    
    高频：是RoPe的位置向量，i比较小(也就是前面的维度)， θi 较大的时候，周期短，频率高。
    
    低频: 是RoPe的位置向量，i比较小(也就是后面的维度)，θi 较小的时候，周期长，频率低。
    
            作者对这个理论拿时钟做了一些解释：RoPE 的行为就像一个时钟。您的 12 小时挂钟基本上是一个 3 维 RoPE，基数为 60。每秒钟，每分钟，每时钟是不同的频率在旋转。（频率从高到低）。现在你的时钟一天最大能表达：60*60*12=43200s。你希望你的时钟表达的时间变长。
    
            那么对于内插PI：如果我们将每秒，分钟，时钟的频率平等的缩小n倍（周期变长），可以实现这个目标。但是这导致你自己真的很难区分每一秒，因为现在秒针几乎每秒都不动。
    
            而NTK-aware认为：我们应该对频率高的秒钟，不做缩放，而但它会将分钟减慢 1.5 倍，将小时减慢 2 倍。您可以在一小时内容纳 90 分钟，在半天内容纳 24 小时。现在时钟可以表达：60*(60*1.5)*(2*12)=129600.0。我们只关注整体的时间：那么不需要精确测量时针，所以与秒相比，将小时缩放得更多是至关重要的。你不想失去秒针的精度，但你可以承受分针甚至时针的精度损失。为了实现这个高频外推，低频内插思想，他选择修改base，他让旋转位置编码最后一个维度（i=d/2-1）的编码，等效于内插：对位置缩小 Ltrain/Ltest 倍。也就是下面的公式:
    
    ![image.png](%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%201b8e64a5662180388464ccb99a8227f1/image%204.png)
    
    解释：
    
    ![image.png](%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%201b8e64a5662180388464ccb99a8227f1/image%205.png)
    
    效果：
    
    ![image.png](%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%201b8e64a5662180388464ccb99a8227f1/image%206.png)
    
    **解释：**
    
    1.可以看出，如果什么都不操作，原本LLAMA的RoPE 2048，基本也就支持2100左右的长度。
    
    2.而线性内插，scale=4，可以支持2048*4，PPL不爆炸。
    
    3.但是ROPE-aware：可以外推到8k，另外，其相比PI方法，有更低的PPL。
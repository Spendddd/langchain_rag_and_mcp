# 实验结果‼️

State: 学习中

grpo实验设置

```python
training_config = {
        'num_iterations': 1,
        'num_steps': 4000,
        'accumulation_steps': 4,
        'batch_size': 2, # reduce if you have fewer GPUs
        'num_generations': 4, # reduce if you have GPUs with less VRAM
        'max_completion_length': 500, # reduce if you have GPUs with less VRAM
        'beta': 0.04, # KL散度缩放倍数
        'learning_rate': 5e-6, # 初始学习率
        'mu': 1,
        'epsilon': 0.2 # PPO裁剪比率
    }
```

实验时间：3天8h

数据总量：1.2w条

实验结果：

Average Reward：

only-grpo：

grpo + lora：

loss：

only-grpo：

grpo + lora：

accuracy（100条数据在推理模式下各跑5次全量取均值）：

base：3.2%

only-lora：6.4%

only-grpo：26.6%

lora + grpo：42.8%

lora：1.7w条金融qa数据，4个epoch，用deepspeed zero2

实验框架/库：transformers

model.generate()

```python
def grpo_loss(model, ref_model, rollout_data, tokenizer, reward_function, beta=0.01, epsilon=0.2):
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    input_ids = rollout_data["input_ids"]
    attention_mask = rollout_data["attention_mask"]
    completion_mask = rollout_data["completion_mask"]
    logits_to_keep = rollout_data["logits_to_keep"]
    old_log_probs = rollout_data["old_log_probs"]
    ref_log_probs = rollout_data["ref_log_probs"]
    token_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep)
    ratio = torch.exp(token_log_probs - old_log_probs)
    rewards = torch.tensor(
        reward_function(prompts=rollout_data["repeated_prompts"], completions=rollout_data["formatted_completions"], answer=rollout_data["repeated_answers"]),
        dtype=torch.float32,
        device=device
    )
    #print(f"Rewards: {rewards}")  # Debug rewards
    batch_size = rollout_data["batch_size"]
    num_generations = rollout_data["num_generations"]
    rewards = rewards.view(batch_size, num_generations)
    avg_reward = rewards.mean().item()
    print("Average Reward:", avg_reward)
    mean_rewards = rewards.mean(dim=1).repeat_interleave(num_generations)
    std_rewards = rewards.std(dim=1).repeat_interleave(num_generations)
    advantages = ((rewards.view(-1) - mean_rewards) / (std_rewards + 1e-4)).unsqueeze(1)
    surr1 = ratio * advantages
    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages
    surrogate_loss = torch.min(surr1, surr2)
    kl = torch.exp(ref_log_probs - token_log_probs) - (ref_log_probs - token_log_probs) - 1
    per_token_loss = surrogate_loss - beta * kl
    loss = -((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()
    return loss, avg_reward

```
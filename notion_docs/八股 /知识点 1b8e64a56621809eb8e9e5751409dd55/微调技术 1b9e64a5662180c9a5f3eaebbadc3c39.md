# 微调技术

State: 未开始

# LoRA

https://zhuanlan.zhihu.com/p/702629428：**大模型高效微调-LoRA原理详解和训练过程深入分析**

## **LoRA简介**

LoRA的核心思想是，在冻结预训练模型权重后，将可训练的[低秩分解矩阵](https://zhida.zhihu.com/search?content_id=244270422&content_type=Article&match_order=1&q=%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3%E7%9F%A9%E9%98%B5&zhida_source=entity)注入到的Transformer架构的每一层中，从而大大减少了在下游任务上的可训练参数量。

![](https://pica.zhimg.com/v2-10ce9e224defb3732e09a257911821aa_1440w.png)

在推理时，对于使用LoRA的模型来说，可直接将原预训练模型权重与训练好的LoRA权重合并，因此在推理时不存在额外开销。

## **LoRA实现**

LoRA就是低秩矩阵适应，在冻结原有LLM参数时，用参数量更小的矩阵进行低秩近似训练。

![image.png](%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%201b9e64a5662180c9a5f3eaebbadc3c39/image.png)

![image.png](%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%201b9e64a5662180c9a5f3eaebbadc3c39/image%201.png)

## **LoRA参数初始化**

在开始训练时：

矩阵 A 通过高斯函数初始化，ai ∼ N(0,σb2)

矩阵 B 为全零初始化，bi = 0

![image.png](%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%201b9e64a5662180c9a5f3eaebbadc3c39/image%202.png)

### **LoRA参数合并系数**

![image.png](%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%201b9e64a5662180c9a5f3eaebbadc3c39/image%203.png)

### 关于LoRA缩放因子：α

https://zhuanlan.zhihu.com/p/685589734：**大模型LoRA微调的缩放因子因小失大了吗**

α：在llamafactory中是lora_alpha

![image.png](%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%201b9e64a5662180c9a5f3eaebbadc3c39/image%204.png)

描述LoRA的输出对于原模型结果的影响程度，α越大，对原模型的影响程度越大

## **LoRA的秩如何选择**

和推荐系统中的评分矩阵分解、文本的非负矩阵分解，以及奇异值分解一样。LoRA的低秩分解近似矩阵B 和 A的秩r的大小，决定了其拟合能力。

理想的情况是找到一个秩r，使得LoRA的低秩近似结构BA能具备全参数微调的增量矩阵ΔW 的表达能力，能越接近越好。

**秩r成为了LoRA的超参数，随着秩r维度的不断增加，参与训练的参数量也随之增加**，LoRA的低秩适应能力将逐渐提高甚至过拟合。

### 一些秩r选取经验：

- **微调的下游任务**：**简单任务所需的秩r不大**，任务越难/多任务混合的情况，需要更大的秩r
- **基座能力**：**越强的基座，所需的秩应该更小。**例如Qwen2-72B-Instruct对比Qwen2-7B-Instruct。越强的基座在处理同等任务时，需要微调的样本数也通常会更少些。
- **数据规模**：**数据规模越大，需要更大的秩r**。

选择合适的rank参数需要在模型性能和资源消耗之间找到平衡。以下是一些选择rank参数的指导原则：

- 初始选择：可以从一个较小的rank值开始，例如 4 或 8，然后逐步增加，观察模型性能的变化。
- 交叉验证：使用交叉验证方法，根据验证集的性能指标选择最佳的rank值。
- 任务复杂度：对于较为复杂的任务，可能需要较高的rank值，以确保模型有足够的表示能力。
- 资源限制：在计算资源和存储资源有限的情况下，选择较低的rank值，以减少资源消耗。

### **LoRA微调的原模型参数选取**

LoRA原始论文只研究了注意力参数**Wq、Wk、Wv,和Wo**。

![论文基于GPT-3 175B，对比分析了训练预算有限时，关于LoRA的微调注意力参数的选择](https://pic2.zhimg.com/v2-2719d682e0fd741f7e80c25579afb017_1440w.png)

论文基于GPT-3 175B，对比分析了训练预算有限时，关于LoRA的微调注意力参数的选择

在训练预算为18M时 (roughly 35MB if stored in FP16) on GPT-3 175B，注意力权重全部选择时的效果最佳。这表明，即使全部的注意力参数即使秩更小时（r=2），相比秩更大的（r=8）部分注意力参数，具有更强的建模能力。**在实际中，一般会把FFN的参数也考虑进来。**

## **LoRA训练**

![](https://pic4.zhimg.com/v2-92b10bf21cb3fe6e67bbbf7be3774847_1440w.jpg)

LoRA反向传播的过程

LoRA训练时，将冻结预训练权重 W0，只优化低秩矩阵 B和A。

LoRA训练后，只需保存低秩矩阵的B和A参数。

### **LoRA在哪里减少了显存占用**

![image.png](%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%201b9e64a5662180c9a5f3eaebbadc3c39/image%205.png)

加入LoRA训练的参数，对应的模型状态中的模型参数+梯度+优化器状态，节约了训练过程中优化器状态这部分的大头，引入了2个低秩矩阵的模型状态，这就是LoRA高效微调节约显存的原因。

# DoRA

[DoRA](https://zhida.zhihu.com/search?content_id=240315419&content_type=Article&match_order=1&q=DoRA&zhida_source=entity)（Weight-Decomposed Low-Rank Adaptation）的主要思想是**将预训练权重分解为幅度（magnitude）和方向（direction），并利用LoRA来微调方向矩阵**

![image.png](%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%201b9e64a5662180c9a5f3eaebbadc3c39/image%206.png)

![image.png](%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%201b9e64a5662180c9a5f3eaebbadc3c39/image%207.png)

# BitFit：训练时只更新bias的参数或者部分bias参数

BitFit（论文：**BitFit: Simple Parameter-efficient Fine-tuning or Transformer-based Masked Language-models**）是一种稀疏的微调方法，它训练时只更新bias的参数或者部分bias参数。涉及到的bias参数有attention模块中计算query,key,value跟合并多个attention结果时涉及到的bias，MLP层中的bias，Layernormalization层的bias参数。

同时，通过对比BitFit训练前后的参数，发现很多bias参数并没有太多变化（例如：跟计算key所涉及到的bias参数）。发现**计算query和将特征维度从N放大到4N的FFN层（intermediate）的bias参数变化最为明显**，只更新这两类bias参数也能达到不错的效果，反之，固定其中任何一者，模型的效果都有较大损失。

# **Prefix Tuning：每一层都加MLP & prefix参数，encoder和decoder都加**

在Prefix Tuning之前的工作主要是人工设计离散的模版或者自动化搜索离散的模版。**对于人工设计的模版，模版的变化对模型最终的性能特别敏感，加一个词、少一个词或者变动位置都会造成比较大的变化。**而对于自动化搜索模版，成本也比较高；同时，以前这种离散化的token搜索出来的结果可能并不是最优的。

除此之外，传统的微调范式利用预训练模型去对不同的下游任务进行微调，对每个任务都要保存一份微调后的模型权重，一方面微调整个模型耗时长；另一方面也会占很多存储空间。

基于上述两点，**Prefix Tuning提出固定预训练LM，为LM添加可训练，任务特定的前缀**，这样就可以**为不同任务保存不同的前缀**，**微调成本也小**；同时，这种**Prefix实际就是连续可微的Virtual Token（Soft Prompt/Continuous Prompt），相比离散的Token，更好优化，效果更好。**

![image.png](%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%201b9e64a5662180c9a5f3eaebbadc3c39/image%208.png)

Prefix Tuning（论文：**Prefix-Tuning: Optimizing Continuous Prompts for Generation**），**在输入token之前构造一段任务相关的virtual tokens作为Prefix，然后训练的时候只更新Prefix部分的参数**，而PLM中的其他部分参数固定。

针对不同的模型结构，需要构造不同的Prefix。

- 针对**自回归架构模型**：在句子前面添加前缀，得到 `z = [PREFIX; x; y]`，合适的上文能够在固定 LM 的情况下去引导生成下文（比如：[GPT3](https://zhida.zhihu.com/search?content_id=229395200&content_type=Article&match_order=1&q=GPT3&zhida_source=entity)的上下文学习）。
- 针对**编码器-解码器架构模型**：**Encoder和Decoder都增加了前缀**，得到 `z = [PREFIX; x; PREFIX0; y]`。**Encoder端增加前缀是为了引导输入部分的编码，Decoder 端增加前缀是为了引导后续token的生成**。

该方法其实和构造Prompt类似，只是Prompt是人为构造的“显式”的提示，并且无法更新参数，而Prefix则是可以学习的“隐式”的提示。

![image.png](%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%201b9e64a5662180c9a5f3eaebbadc3c39/image%209.png)

同时，**为了防止直接更新Prefix的参数导致训练不稳定和性能下降的情况，在Prefix层前面加了MLP结构，训练完成后，只保留Prefix的参数**。

![image.png](%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%201b9e64a5662180c9a5f3eaebbadc3c39/image%2010.png)

除此之外，通过消融实验证实，**只调整embedding层的表现力不够，将导致性能显著下降，因此，在每层都加了prompt的参数，改动较大。**

![](https://pic3.zhimg.com/v2-c24840aa6ebac63b5fcc450cd8354aa0_1440w.jpg)

另外，实验还对比了位置对于生成效果的影响，Prefix-tuning也是要略优于Infix-tuning的。其中，Prefix-tuning形式为 `[PREFIX; x; y]`，Infix-tuning形式为 `[x; INFIX; y]`。

### 训练过程

在Transformer模型的输入层或各层输入前添加可学习的前缀嵌入，并通过训练这些前缀嵌入来优化模型在特定任务上的表现。

初始化前缀嵌入：在Transformer模型的输入层之前，初始化一个固定长度的前缀嵌入矩阵。

将前缀嵌入与输入序列拼接：将初始化好的前缀嵌入与原始输入序列的词嵌入进行拼接，形成新的输入表示。这个新的输入表示将作为Transformer模型各层的输入。

# **Prompt Tuning：Prefix Tuning的简化版本，只在输入层加prompt tokens，一个问题生成多prompt进行训练**

大模型全量微调对每个任务训练一个模型，开销和部署成本都比较高。同时，离散的prompts（指人工设计prompts提示语加入到模型）方法，成本比较高，并且效果不太好。

基于此，作者提出了Prompt Tuning，通**过反向传播更新参数来学习prompts，而不是人工设计prompts；**同时**冻结模型原始权重，只训练prompts参数**，训练完以后，用同一个模型可以做多任务推理。

Prompt Tuning（论文：**The Power of Scale for Parameter-Efficient Prompt Tuning**），该方法可以看作是**Prefix Tuning的简化版本**，它给每个任务定义了自己的Prompt，然后**拼接到数据上作为输入**，但**只在输入层加入prompt tokens**，并且不需要加入 MLP 进行调整来解决难训练的问题。

![image.png](%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%201b9e64a5662180c9a5f3eaebbadc3c39/image%2011.png)

同时，Prompt Tuning 还提出了 Prompt Ensembling，也就是**在一个批次（Batch）里同时训练同一个任务的不同 prompt（即采用多种不同方式询问同一个问题）**，这样相当于训练了不同模型，比模型集成的成本小多了。

![image.png](%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%201b9e64a5662180c9a5f3eaebbadc3c39/image%2012.png)

除此之外，Prompt Tuning 论文中还探讨了 Prompt token 的初始化方法和长度对于模型性能的影响。通过消融实验结果发现，与随机初始化和使用样本词汇表初始化相比，**Prompt Tuning采用类标签初始化模型的效果更好**。不过随着模型参数规模的提升，这种gap最终会消失。

# P-Tuning

https://zhuanlan.zhihu.com/p/635848732

P-Tuning（论文：**GPT Understands, Too**），该方法**将Prompt转换为可以学习的Embedding层**，并用**MLP+LSTM**的方式来对Prompt Embedding进行一层处理。

![image.png](%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%201b9e64a5662180c9a5f3eaebbadc3c39/image%2013.png)

相比Prefix Tuning，P-Tuning加入的可微的virtual token，但仅限于输入层，没有在每一层都加；另外，**virtual token的位置也不一定是前缀，插入的位置是可选的**。这里的出发点实际是把传统人工设计模版中的真实token替换成可微的virtual token。

![image.png](%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%201b9e64a5662180c9a5f3eaebbadc3c39/image%2014.png)

经过预训练的LM的词嵌入已经变得高度离散，如果随机初始化virtual token，容易优化到局部最优值，而这些virtual token理论是应该有相关关联的。因此，作者通过实验发现用一个prompt encoder来编码会收敛更快，效果更好。即**用一个LSTM+MLP去编码这些virtual token**以后，再输入到模型。

从对比实验证实看出，P-Tuning获得了与全参数一致的效果。甚至在某些任务上优于全参数微调。

# **P-Tuning v2**

之前的Prompt Tuning和P-Tuning等方法存在两个主要的问题：

第一，缺乏模型参数规模和任务通用性。

- 缺乏规模通用性：Prompt Tuning论文中表明当模型规模超过100亿个参数时，提示优化可以与全量微调相媲美。但是对于那些较小的模型（从100M到1B），提示优化和全量微调的表现有很大差异，这大大限制了提示优化的适用性。
- 缺乏任务普遍性：尽管Prompt Tuning和P-tuning在一些 NLU 基准测试中表现出优势，但提示调优对硬序列标记任务（即序列标注）的有效性尚未得到验证。

第二，缺少深度提示优化，在Prompt Tuning和P-tuning中，连续提示只被插入transformer第一层的输入embedding序列中，在接下来的transformer层中，插入连续提示的位置的embedding是由之前的transformer层计算出来的，这可能导致两个可能的优化挑战。

- **由于序列长度的限制，可调参数的数量是有限的。**
- **输入embedding对模型预测只有相对间接的影响。**

### **技术原理**

P-Tuning v2（论文： **P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks**），该方法**在每一层都加入了Prompts tokens作为输入**，而不是仅仅加在输入层，这带来两个方面的好处：

- 更多可学习的参数（从P-tuning和Prompt Tuning的0.01%增加到0.1%-3%），同时也足够参数高效。
- 加入到更深层结构中的Prompt能给模型预测带来更直接的影响。

![image.png](%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%201b9e64a5662180c9a5f3eaebbadc3c39/image%2015.png)

具体做法基本同Prefix Tuning，可以看作是将文本生成的Prefix Tuning技术适配到NLU任务中，然后做了一些改进：

- **移除重参数化的编码器**。以前的方法利用重参数化功能来提高训练速度和鲁棒性（如：Prefix Tuning中的MLP、P-Tuning中的LSTM））。在 P-tuning v2 中，作者发现重参数化的改进很小，尤其是对于较小的模型，同时还会影响模型的表现。
- **针对不同任务采用不同的提示长度**。提示长度在提示优化方法的超参数搜索中起着核心作用。在实验中，我们发现**不同的理解任务通常用不同的提示长度来实现其最佳性能**，这与Prefix-Tuning中的发现一致，不同的文本生成任务可能有不同的最佳提示长度。
- **引入多任务学习**。先在多任务的Prompt上进行预训练，然后再适配下游任务。多任务学习对我们的方法来说是可选的，但可能是相当有帮助的。一方面，连续提示的随机惯性给优化带来了困难，这可以通过更多的训练数据或与任务相关的无监督预训练来缓解；另一方面，连续提示是跨任务和数据集的特定任务知识的完美载体。我们的实验表明，在一些困难的序列任务中，多任务学习可以作为P-tuning v2的有益补充。
- **回归传统的分类标签范式，而不是映射器**。标签词映射器（Label Word Verbalizer）一直是提示优化的核心组成部分，它将one-hot类标签变成有意义的词，以利用预训练语言模型头。尽管它在few-shot设置中具有潜在的必要性，但在全数据监督设置中，Verbalizer并不是必须的。它阻碍了提示调优在我们需要无实际意义的标签和句子嵌入的场景中的应用。因此，P-Tuning v2回归传统的CLS标签分类范式，采用随机初始化的分类头（Classification Head）应用于tokens之上，以增强通用性，可以适配到序列标注任务。

# **Adapter Tunning**

2019年，Houlsby N等人将Adapter引入NLP领域，作为全模型微调的一种替代方案。Adapter主体架构下图所示。

![](https://pic2.zhimg.com/v2-affdb5030d50da087c27131d1eabce87_1440w.jpg)

**在预训练模型每一层(或某些层)中添加Adapter模块**(如上图左侧结构所示)，微调时冻结预训练模型主体，由Adapter模块学习特定下游任务的知识。**每个Adapter模块由两个前馈子层组成，第一个前馈子层将Transformer块的输出作为输入，将原始输入维度d投影到m，通过控制m的大小来限制Adapter模块的参数量，通常情况下m<<d。**在输出阶段，通过第二个前馈子层还原输入维度，将m重新投影到d，作为Adapter模块的输出(如上图右侧结构)。通过添加Adapter模块来产生一个易于扩展的下游模型，每当出现新的下游任务，通过添加Adapter模块来避免全模型微调与灾难性遗忘的问题。Adapter方法不需要微调预训练模型的全部参数，通过引入少量针对特定任务的参数，来存储有关该任务的知识，降低对模型微调的算力要求。

## **Adapter算法改进**

2020年，Pfeiffer J等人对Adapter进行改进，**「提出[AdapterFusion](https://zhida.zhihu.com/search?content_id=215730926&content_type=Article&match_order=1&q=AdapterFusion&zhida_source=entity)算法，用以实现多个Adapter模块间的最大化任务迁移」**(其模型结构如下图所示)。

![](https://pic2.zhimg.com/v2-e832e1461e26641bfac17b66ef3150d3_1440w.jpg)

AdapterFusion将学习过程分为两个阶段：

- 1.**「知识提取阶段」**：训练Adapter模块学习下游任务的特定知识，将知识封装在Adapter模块参数中。
- 2.**「知识组合阶段」**：将预训练模型参数与特定于任务的Adapter参数固定，引入新参数Ψ学习组合多个Adapter中的知识，提高模型在目标任务中的表现。

首先，**对于N的不同的下游任务训练N个Adapter模块**。然后使用AdapterFusion组合N个适配器中的知识，将预训练参数Θ和全部的Adapter参数Φ固定，**引入新的参数Ψ，使用N个下游任务的数据集训练，让AdapterFusion学习如何组合N个适配器解决特定任务。参数Ψ在每一层中包含Key、Value和Query（矩阵）（上图右侧架构所示）。在Transformer每一层中将前馈网络子层的输出作为Query，Value和Key的输入是各自适配器的输出，将Query和Key做点积传入SoftMax函数中，根据上下文学习对适配器进行加权。**在给定的上下文中，AdapterFusion学习经过训练的适配器的参数混合，根据给定的输入识别和激活最有用的适配器。**「作者通过将适配器的训练分为知识提取和知识组合两部分，解决了灾难性遗忘、任务间干扰和训练不稳定的问题。Adapter模块的添加也导致模型整体参数量的增加，降低了模型推理时的性能」**。

**「[AdapterDrop](https://zhida.zhihu.com/search?content_id=215730926&content_type=Article&match_order=1&q=AdapterDrop&zhida_source=entity)」**: RückléA等人对Adapter的计算效率进行分析，发现**与全模型微调相比适配器在训练时快60%，但是在推理时慢4%-6%，**并提出了**AdapterDrop**方法缓解该问题。**「AdapterDrop在不影响任务性能的情况下，对Adapter动态高效的移除，尽可能的减少模型的参数量，提高模型在反向传播（训练）和正向传播（推理）时的效率」**。在删除了前五层的Adapter后，在对八个任务进行推理时，效率提高了39%。
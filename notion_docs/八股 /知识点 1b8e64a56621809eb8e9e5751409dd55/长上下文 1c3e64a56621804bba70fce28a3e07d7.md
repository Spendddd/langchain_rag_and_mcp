# 长上下文

State: 已完成

https://blog.csdn.net/qq_41739364/article/details/136068939：**【大模型上下文长度扩展】YaRN：以文匹意，精细化衔接长篇**

### 困惑度

https://blog.csdn.net/u013172930/article/details/145428394：**困惑度（Perplexity）**

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image.png)

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%201.png)

# PI

https://zhuanlan.zhihu.com/p/704015317：**LLM上下文长度扩展方案：Position Interpolation**

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%202.png)

一些论文提出可以通过给模型喂一些长度大于L的输入来微调模型，进而逐步将原始窗口长度扩大。实验结果如下：

![](https://pic2.zhimg.com/v2-ad54722d786a08bd869944f7e5f76183_1440w.jpg)

实验结果证明，**即使经过10000多个step的训练后，窗口长度增加的幅度仍然特别小，有效上下文窗口的增加幅度仅从2048增加到2560，这种代价是不可接受的**。

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%203.png)

将 m 变成了m/S ，**本质上是线性缩小了每个位置每个维度上的旋转角度**。值得注意的是，重新调整位置索引的方法不会引入额外的权重参数，也不会以任何方式修改模型架构。这使得它在实际应用中具有很强的适应性，**属于即插即用的框架**。在调整位置编码方式后，PI需要进一步使用 L′ 长度内的样本对模型进行简单少量微调以达到最佳性能。

# NTK**-aware interpolation**

https://zhuanlan.zhihu.com/p/704569344：**LLM上下文长度扩展方案：NTK-aware interpolation**

## PI的问题

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%204.png)

### **高频信息损失**

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%205.png)

原本**在低维度上，旋转角度较大，意味着这些维度上的信号变化非常迅速，能够精细地区分相邻位置**，虽然不同维度间的差异都减小了4倍，**但是高频低维由于量级比较大，所以相比之前降低更多，对用低维区分不同位置间的能力影响更大，显得“更拥挤”**。而**高维度由于本来旋转角度就很小，这种变化对高维度区分不同位置的能力影响相对较小，不会立即表现为“更拥挤”。**因此，**PI会在扩展倍数特别大时显著降低位置编码区分不同位置的能力，这种现象称之为高频信息的损失。**

> **高频低维的“拥挤”现象**：
> 
> - **高频低维**：高频信号在低维度上变化非常迅速，这意味着在低维度上，位置编码能够精细地区分相邻的位置。然而，由于旋转操作导致不同维度间的差异都减小了4倍，而高频低维的信号量级本身比较大，因此其变化幅度相比之前降低得更多。
> - **结果**：这种变化使得低维空间中的不同位置之间的区分度降低，导致这些位置在低维空间中显得更加“拥挤”或“接近”。换句话说，原本可以通过高频低维信号精确区分的位置，现在变得难以区分。

## **NTK-aware Scaled RoPE：高频外推+低频内插**

为了解决PI中出现的问题，NTK-aware提出的改进策略为：**高频外推和低频内插**。即：不是将RoPE的每个维度平均缩放一个因子S，而是**通过减少高频的缩放和增加低频的缩放将插值压力分散到多个维度。**

在讲NTK-aware之前，为了将PI和其NTK-aware，或者更广义上的多种内插方法联系起来，我们定义了如下表达式：

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%206.png)

### **进制编码**

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%207.png)

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%208.png)

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%209.png)

# YaRN

其中PI线性缩放所有位置索引，并平等地对待每个维度；NTK-aware则实现了高频外推和低频内插，缓解了高频也就是低维的压力，使模型能够更好地区分不同位置间的精细区别。

为了引出上述两种方案存在的问题，这里先引入一个新的定义：给定[RoPE](https://zhida.zhihu.com/search?content_id=249036082&content_type=Article&match_order=1&q=RoPE&zhida_source=entity)中一个具体的维度 i ，则这个维度的波长被定义为：

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%2010.png)

基于波长公式，我们有如下结论：

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%2011.png)

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%2012.png)

像PI这种类型的插值方案不关心波长的维数，我们将这些方法称为“盲”插值方法(blind interpolation)，比如**像PI和“NTK-aware”插值这样的blind interpolation方法中，我们面对所有RoPE隐藏维度没有做任何针对性的处理。**而其他方法如这篇文章提出的YaRN，我们将其归类为“有针对性的”插值方法，即有**对RoPE的不同维度做出不同处理**。

## **NTK-by-parts**

为了针对不同维度做出不同的处理，YaRN中首先定义了NTK-by-parts插值方法。具体来说，关于RoPE中不同维度的波长，有如下结论：

> **存在某些维度 i ，其波长 λi 大于在预训练期间看到的最大上下文长度 L。**
> 

从理论上来讲，RoPE是一种编码绝对位置的方法。然而，我们前面说过，**波长描述了为了在维度i处嵌入的旋转位置执行全旋转(2π)所需的token的长度，如果某些维度的波长大于上下文长度L，这说明该维度无法执行全旋转。**在这种情况下，由于维度在预训练期间至少不会完全旋转一次，如果我们选择第一个令牌作为基准，那么在预训练期间每隔一个令牌到它的距离是唯一的，神经网络可以用它来确定它的绝对位置信息。相反，**如果某个维度波长小于L，那该维度就执行了至少一次全旋转，我们就无法在这个维度描述绝对距离，只能描述相对位置信息。**

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%2013.png)

为了定义上述不同内插策略的边界，首先**基于前面的NTK-aware把波长定义为**：

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%2014.png)

其中 i 从0开始计数。接着将上下文长度 L 和波长的比值定义为：

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%2015.png)

然后，定义一个分段函数如下：

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%2016.png)

基于上述分段函数，NTK-by-parts被定义为：

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%2017.png)

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%2018.png)

## YaRN

YaRN的作者注意到，无论数据样本或者扩展上下文窗口上的令牌位置怎么样，在计算自注意力公式中引入温度系数 t 对困惑度ppl有着一致的影响。为此，将该公式改为：

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%2019.png)

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%2020.png)

### **Dynamic NTK**

在真正使用模型时，我们无法保证每次输入的长度都是固定的。在这种情况下，有以下两种方法得到扩展因子 S ：

![image.png](%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%201c3e64a56621804bba70fce28a3e07d7/image%2021.png)

第一种方式的问题是，当序列长度小于L时，模型的性能可能会出现一些折扣，而当序列长度大于L时，模型的性能会退化。但通过像方式2那样进行动态缩放，它允许模型在达到训练的上下文限制时较为缓慢地退化性能，而不是骤降。